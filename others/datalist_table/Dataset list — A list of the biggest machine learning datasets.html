<!DOCTYPE html>
<!-- saved from url=(0028)https://www.datasetlist.com/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <script type="text/javascript" async="" src="./Dataset list — A list of the biggest machine learning datasets_files/analytics.js.下载"></script><script async="" src="./Dataset list — A list of the biggest machine learning datasets_files/js"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-55917238-5');
</script>

    <title>Dataset list — A list of the biggest machine learning datasets</title>
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="title" content="Dataset list — A list of the biggest machine learning datasets">
<meta name="Description" content="A list of the biggest machine learning datasets from across the web. Computer vision, natural language processing, audio and medical datasets.">

<meta property="og:type" content="website">
<meta property="og:url" content="https://www.datasetlist.com/">
<meta property="og:title" content="Dataset list — A list of the biggest datasets for machine learning">
<meta property="og:description" content="A list of the biggest datasets for machine learning from across the web. Computer vision, natural language processing, self-driving and question answering datasets.">
<meta property="og:image" content="https://www.datasetlist.com/img/list.png">

<meta property="twitter:card" content="summary_large_image">
<meta property="twitter:url" content="https://www.datasetlist.com/">
<meta property="twitter:title" content="Dataset list — A list of the biggest datasets for machine learning">
<meta property="twitter:description" content="A list of the biggest datasets for machine learning from across the web. Computer vision, natural language processing, self-driving and question answering datasets.">
<meta property="twitter:image" content="https://www.datasetlist.com/img/list.png">

<link rel="canonical" href="https://www.datasetlist.com/">

<link rel="stylesheet" href="./Dataset list — A list of the biggest machine learning datasets_files/all.css" integrity="sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf" crossorigin="anonymous">
<link href="./Dataset list — A list of the biggest machine learning datasets_files/css" rel="stylesheet"> 
<link href="./Dataset list — A list of the biggest machine learning datasets_files/tailwind.min.css" rel="stylesheet">
<link href="./Dataset list — A list of the biggest machine learning datasets_files/normalize.css" rel="stylesheet">
<link href="./Dataset list — A list of the biggest machine learning datasets_files/style.css" rel="stylesheet">

<link rel="stylesheet" type="text/css" href="./Dataset list — A list of the biggest machine learning datasets_files/cookieconsent.min.css">
<script src="./Dataset list — A list of the biggest machine learning datasets_files/cookieconsent.min.js.下载"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#ffffff",
      "text": "#191e38"
    },
    "button": {
      "background": "#6574cd"
    }
  },
  "theme": "classic",
  "position": "bottom-left",
  "content": {
    "message": "This website uses cookies.",
    "dismiss": "OK",
    "href": "/privacy"
  }
})});
</script>
  <style></style></head>
  <body class="antialiased bg-grey-lightest"><div role="dialog" aria-live="polite" aria-label="cookieconsent" aria-describedby="cookieconsent:desc" class="cc-window cc-floating cc-type-info cc-theme-classic cc-bottom cc-left cc-color-override--253271512 " style=""><!--googleoff: all--><span id="cookieconsent:desc" class="cc-message">This website uses cookies. <a aria-label="learn more about cookies" role="button" tabindex="0" class="cc-link" href="https://www.datasetlist.com/privacy" rel="noopener noreferrer nofollow" target="_blank">Learn more</a></span><div class="cc-compliance"><a aria-label="dismiss cookie message" role="button" tabindex="0" class="cc-btn cc-dismiss">OK</a></div><!--googleon: all--></div>
    




<div id="theApp" class="border-t-4 border-indigo"><div class="container mx-auto px-6 pt-12 lg:pb-12" style="max-width: 1120px;"><div><div class="flex items-center justify-between mb-8"><div class="inline-flex items-center bg-white text-indigo-darker text-sm rounded overflow-hidden"><div class="bg-indigo-light text-white p-1 px-2">Updated</div> <div class="bg-white p-1 px-2">December 2019</div></div> <a href="https://www.datasetlist.com/tools" class="HoverTooltip no-underline text-indigo hover:text-indigo-light font-semibold">Annotation tools</a></div> <h1 class="text-indigo-darkest font-bold font-heading mb-10 leading-normal">Machine learning datasets</h1> <h2 class="text-indigo-darker text-base font-normal mb-2 leading-normal">A list of the biggest machine learning datasets from across the web.</h2> <div class="text-indigo-darker mb-2 leading-normal">Email me at <a href="mailto:hello@datasetlist.com" class="text-indigo-darker hover:text-indigo no-underline font-semibold">hello@datasetlist.com</a> with questions, suggestions and ideas.</div> <div class="mb-10" style="max-width: 504px;"><div class="mb-8 text-indigo-darker leading-normal">You can subscribe to get updates when new datasets and tools are released.</div> <form action="https://buttondown.email/api/emails/embed-subscribe/datasetlist" method="post" target="popupwindow" onsubmit="window.open(&#39;https://buttondown.email/datasetlist&#39;, &#39;popupwindow&#39;)" class="embeddable-buttondown-form block md:flex items-center rounded shadow-none md:shadow bg-tranparent md:bg-white"><input type="email" name="email" id="bd-email" placeholder="your@email.com" aria-label="Email address entry" class="bg-white border-none w-full md:rounded-l text-grey-darkest py-2 px-4 leading-tight md:shadow-none shadow mb-6 md:mb-0"> <input type="hidden" value="1" name="embed"> <input type="submit" value="Subscribe" class="w-full md:w-auto md:flex-no-shrink bg-indigo hover:bg-indigo-light border-indigo hover:border-indigo-light font-bold
                tracking-wide border-4 text-white py-2 px-4 md:rounded-r text-base md:shadow-none shadow"></form></div></div></div> <div class="container mx-auto px-6 pb-20" style="max-width: 1120px;"><div class="flex items-start flex-col lg:flex-row"><div class="flex flex-col text-grey pt-2 Filters"><div class="relative mb-2"><div class="HoverTooltip font-12 cursor-default uppercase text-indigo-darker flex items-center" style="letter-spacing: 1px;"><span>Search</span> <i class="fas fa-question-circle text-indigo-lighter ml-2"></i></div> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-2">
          Search datasets by name and desription.
         </div></div> <div class="mb-8"><input type="text" placeholder="Search" class="text-sm py-2 px-4 text-indigo-darkest border border-grey-lighter rounded width-100"></div> <div class="relative mb-2"><div class="HoverTooltip font-12 cursor-default uppercase text-indigo-darker flex items-center" style="letter-spacing: 1px;"><span>Category</span> <i class="fas fa-question-circle text-indigo-lighter ml-2"></i></div> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-2">
          Filter by category: computer vision, natural language processing, self-driving, question answering, audio and medical.
         </div></div> <div class="text-sm py-2 px-4 text-indigo-darkest opacity-50 mb-2 border shadow border-grey-lighter rounded flex items-center width-100 cursor-pointer bg-white opacity-100"><i class="fas fa-globe text-sm mr-2 text-indigo-dark"></i> <span>All</span></div> <div class="text-sm py-2 px-4 text-indigo-darkest opacity-50 mb-2 border shadow border-grey-lighter rounded flex items-center width-100 cursor-pointer bg-white opacity-100"><i class="far fa-eye text-sm mr-2 text-indigo-dark"></i> <span>CV</span></div> <div class="text-sm py-2 px-4 text-indigo-darkest opacity-50 mb-2 border shadow border-grey-lighter rounded flex items-center width-100 cursor-pointer bg-white opacity-100"><i class="fas fa-align-left text-sm mr-2 text-indigo-dark"></i>
        NLP
      </div> <div class="text-sm py-2 px-4 text-indigo-darkest opacity-50 mb-2 border shadow border-grey-lighter rounded flex items-center width-100 cursor-pointer bg-white opacity-100"><i class="fas fa-car text-sm mr-2 text-indigo-dark"></i> <span>Self-driving</span></div> <div class="text-sm py-2 px-4 text-indigo-darkest opacity-50 mb-2 border shadow border-grey-lighter rounded flex items-center width-100 cursor-pointer bg-white opacity-100"><i class="far fa-question-circle text-sm mr-2 text-indigo-dark"></i>
        QA
      </div> <div class="text-sm py-2 px-4 text-indigo-darkest opacity-50 mb-2 border shadow border-grey-lighter rounded flex items-center width-100 cursor-pointer bg-white opacity-100"><i class="fas fa-music text-sm mr-2 text-indigo-dark"></i>
        Audio
      </div> <div class="text-sm py-2 px-4 text-indigo-darkest opacity-50 mb-8 border shadow border-grey-lighter rounded flex items-center width-100 cursor-pointer bg-white opacity-100"><i class="far fa-plus-square text-sm mr-2 text-indigo-dark"></i>
        Medical
      </div> <div class="relative mb-2"><div class="HoverTooltip font-12 cursor-default uppercase text-indigo-darker flex items-center" style="letter-spacing: 1px;"><span>License</span> <i class="fas fa-question-circle text-indigo-lighter ml-2"></i></div> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-2">
          Filter by type of license: non-commercial licenses allow use for research and educational purposes while a commercial license also allows using the dataset for commercial licenses. Additional restrictions may apply, check the full license text.
        </div></div> <div class="text-sm py-2 px-4 text-indigo-darkest opacity-50 mb-2 border shadow border-grey-lighter rounded flex items-center width-100 cursor-pointer bg-white opacity-100">
        All
      </div> <div class="text-sm py-2 px-4 text-indigo-darkest opacity-50 mb-2 border shadow border-grey-lighter rounded flex items-center width-100 cursor-pointer bg-white opacity-100">
        Non-commercial
      </div> <div class="text-sm py-2 px-4 text-indigo-darkest opacity-50 mb-8 border shadow border-grey-lighter rounded flex items-center width-100 cursor-pointer bg-white opacity-100">
        Commercial
      </div> <a href="https://docs.google.com/forms/d/e/1FAIpQLSdK4xeYnemLBp4zYOmnJsSCh905dpkTbXKAFFqFP7T5I4RBPA/viewform" target="_blank" class="text-sm text-indigo hover:text-indigo-lighter pl-2 no-underline font-semibold mb-8">Add a dataset</a></div> <div class="lg:pl-8"><table class="table-fixed text-left text-sm bg-white width-100"><thead class="text-indigo-darkest"><tr class="border-b border-grey-light bg-grey-lightest font-12 uppercase hidden lg:table-row" style="letter-spacing: 1px;"><th class="px-4 py-2" style="width: 32%;">Name</th> <th class="px-4 py-2" style="width: 8%;">Year</th> <th class="px-4 py-2" style="width: 34%;">Description</th> <th class="px-4 py-2" style="width: 18%;">License</th> <th class="px-4 py-2 text-right" style="width: 8%;">Paper</th></tr> <tr class="border-b border-grey-light bg-grey-lightest font-12 uppercase lg:hidden" style="letter-spacing: 1px;"><th class="px-4 py-2">Name</th> <th class="px-4 py-2 text-right" style="width: 100px;">License</th></tr></thead> <tbody><tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell border-l-4 border-green-lighter"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://objectnet.dev/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">ObjectNet</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        ObjectNet is a large real-world test set for object recognition with control where object backgrounds, rotations, and imaging viewpoints are random. Collected to intentionally show objects from new viewpoints on new backgrounds. 50,000 image test set, same as ImageNet, with controls for rotation, background, and viewpoint. 313 object classes with 113 overlapping ImageNet.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      CC BY 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution 4.0 International (CC BY 4.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://objectnet.dev/objectnet-a-large-scale-bias-controlled-dataset-for-pushing-the-limits-of-object-recognition-models.pdf" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden border-l-4 border-green-lighter" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://objectnet.dev/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">ObjectNet</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          ObjectNet is a large real-world test set for object recognition with control where object backgrounds, rotations, and imaging viewpoints are random. Collected to intentionally show objects from new viewpoints on new backgrounds. 50,000 image test set, same as ImageNet, with controls for rotation, background, and viewpoint. 313 object classes with 113 overlapping ImageNet.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          CC BY 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution 4.0 International (CC BY 4.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell border-l-4 border-green-lighter"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://jrdb.stanford.edu/dataset/about" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">JRDB</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        JRDB is the largest benchmark data for 2D-3D person tracking, including: Over 60K frames (67 minutes) sensor data captured from 5 stereo camera and two LiDAR sensors, 54 sequences from different locations, during day and night time, indoors and outdoors in a university campus environment. Around 2 milion high quality 2D bounding box annotations on 360° cylindrical video streams generated from 5 stereo cameras 
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      CC-BY-NC-SA 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution-NonCommercial-ShareAlike International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon,
Under the following terms:
Attribution - you must give approprate credit,
NonCommercial - you may not use the material for commercial purposes,
ShareAlike - if you make changes, you must distribute your contributions.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1910.11792" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden border-l-4 border-green-lighter" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://jrdb.stanford.edu/dataset/about" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">JRDB</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          JRDB is the largest benchmark data for 2D-3D person tracking, including: Over 60K frames (67 minutes) sensor data captured from 5 stereo camera and two LiDAR sensors, 54 sequences from different locations, during day and night time, indoors and outdoors in a university campus environment. Around 2 milion high quality 2D bounding box annotations on 360° cylindrical video streams generated from 5 stereo cameras 
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          CC-BY-NC-SA 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution-NonCommercial-ShareAlike International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon,
Under the following terms:
Attribution - you must give approprate credit,
NonCommercial - you may not use the material for commercial purposes,
ShareAlike - if you make changes, you must distribute your contributions.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell border-l-4 border-green-lighter"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://xview2.org/dataset" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">xBD</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        A dataset for assessing building damage from satellite imagery. With over 850,000 building polygons from six different types of natural disaster around the world, covering a total area of over 45,000 square kilometers, the xBD dataset is one of the largest and highest quality public datasets of annotated high-resolution satellite imagery.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1911.09296" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden border-l-4 border-green-lighter" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://xview2.org/dataset" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">xBD</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          A dataset for assessing building damage from satellite imagery. With over 850,000 building polygons from six different types of natural disaster around the world, covering a total area of over 45,000 square kilometers, the xBD dataset is one of the largest and highest quality public datasets of annotated high-resolution satellite imagery.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell border-l-4 border-green-lighter"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-align-left text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">NLP</div></div> <div class="truncate"><a href="https://github.com/alexwarstadt/blimp" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">BLiMP</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        The Benchmark of Linguistic Minimal Pairs. BLiMP is a challenge set for evaluating what language models (LMs) know about major grammatical phenomena in English. BLiMP consists of 67 sub-datasets, each containing 1000 minimal pairs isolating specific contrasts in syntax, morphology, or semantics. The data is automatically generated according to expert-crafted grammars.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Not found
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">License information not found</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1912.00582" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden border-l-4 border-green-lighter" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-align-left text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://github.com/alexwarstadt/blimp" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">BLiMP</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          The Benchmark of Linguistic Minimal Pairs. BLiMP is a challenge set for evaluating what language models (LMs) know about major grammatical phenomena in English. BLiMP consists of 67 sub-datasets, each containing 1000 minimal pairs isolating specific contrasts in syntax, morphology, or semantics. The data is automatically generated according to expert-crafted grammars.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Not found
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">License information not found</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell border-l-4 border-green-lighter"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://github.com/msn199959/Logo-2k-plus-Dataset" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Logo-2k+</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        A Large-Scale Logo Dataset for Scalable Logo Classiﬁcation. Our resulting logo dataset contains 167,140 images with 10 root categories and 2,341 categories.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1911.07924" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden border-l-4 border-green-lighter" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://github.com/msn199959/Logo-2k-plus-Dataset" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Logo-2k+</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          A Large-Scale Logo Dataset for Scalable Logo Classiﬁcation. Our resulting logo dataset contains 167,140 images with 10 root categories and 2,341 categories.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell border-l-4 border-green-lighter"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-car text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 100px; margin-left: -16px;">Self-driving</div></div> <div class="truncate"><a href="http://www.semantic-kitti.org/index.html" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">SemanticKITTI</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        SemanticKITTI is based on the KITTI Vision Benchmark and we provide semantic annotation for all sequences of the Odometry Benchmark. The dataset contains 28 classes including classes distinguishing non-moving and moving objects.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      CC-BY-NC-SA 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution-NonCommercial-ShareAlike International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon,
Under the following terms:
Attribution - you must give approprate credit,
NonCommercial - you may not use the material for commercial purposes,
ShareAlike - if you make changes, you must distribute your contributions.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1904.01416" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden border-l-4 border-green-lighter" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-car text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://www.semantic-kitti.org/index.html" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">SemanticKITTI</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          SemanticKITTI is based on the KITTI Vision Benchmark and we provide semantic annotation for all sequences of the Odometry Benchmark. The dataset contains 28 classes including classes distinguishing non-moving and moving objects.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          CC-BY-NC-SA 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution-NonCommercial-ShareAlike International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon,
Under the following terms:
Attribution - you must give approprate credit,
NonCommercial - you may not use the material for commercial purposes,
ShareAlike - if you make changes, you must distribute your contributions.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell border-l-4 border-green-lighter"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="http://hacs.csail.mit.edu/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">HACS</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        This project introduces a novel video dataset, named HACS (Human Action Clips and Segments). It consists of two kinds of manual annotations. HACS Clips contains 1.55M 2-second clip annotations; HACS Segments has complete action segments (from action start to end) on 50K videos. The large-scale dataset is effective for pretraining action recognition and localization models, and also serves as a new benchmark for temporal action localization.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1712.09374" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden border-l-4 border-green-lighter" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://hacs.csail.mit.edu/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">HACS</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          This project introduces a novel video dataset, named HACS (Human Action Clips and Segments). It consists of two kinds of manual annotations. HACS Clips contains 1.55M 2-second clip annotations; HACS Segments has complete action segments (from action start to end) on 50K videos. The large-scale dataset is effective for pretraining action recognition and localization models, and also serves as a new benchmark for temporal action localization.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell border-l-4 border-green-lighter"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-car text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 100px; margin-left: -16px;">Self-driving</div></div> <div class="truncate"><a href="https://www.astyx.com/development/astyx-hires2019-dataset.html" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Astyx HiRes2019</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        A   radar-centric   automotive   datasetbased   on   radar,   lidar   and   camera   data   for   the   purposeof   3D   object   detection. 
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      CC-BY-NC-SA 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution-NonCommercial-ShareAlike International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon,
Under the following terms:
Attribution - you must give approprate credit,
NonCommercial - you may not use the material for commercial purposes,
ShareAlike - if you make changes, you must distribute your contributions.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://www.astyx.com/fileadmin/redakteur/dokumente/Automotive_Radar_Dataset_for_Deep_learning_Based_3D_Object_Detection.PDF" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden border-l-4 border-green-lighter" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-car text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://www.astyx.com/development/astyx-hires2019-dataset.html" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Astyx HiRes2019</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          A   radar-centric   automotive   datasetbased   on   radar,   lidar   and   camera   data   for   the   purposeof   3D   object   detection. 
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          CC-BY-NC-SA 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution-NonCommercial-ShareAlike International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon,
Under the following terms:
Attribution - you must give approprate credit,
NonCommercial - you may not use the material for commercial purposes,
ShareAlike - if you make changes, you must distribute your contributions.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell border-l-4 border-green-lighter"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://www.isprs-ann-photogramm-remote-sens-spatial-inf-sci.net/IV-2-W7/153/2019/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">SEN12MS</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        SEN12MS is a dataset consisting of 180,748 corresponding image triplets containing Sentinel-1 dual-pol SAR data, Sentinel-2 multi-spectral imagery, and MODIS-derived land cover maps. 

      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      CC BY 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution 4.0 International (CC BY 4.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1906.07789" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden border-l-4 border-green-lighter" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://www.isprs-ann-photogramm-remote-sens-spatial-inf-sci.net/IV-2-W7/153/2019/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">SEN12MS</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          SEN12MS is a dataset consisting of 180,748 corresponding image triplets containing Sentinel-1 dual-pol SAR data, Sentinel-2 multi-spectral imagery, and MODIS-derived land cover maps. 

        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          CC BY 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution 4.0 International (CC BY 4.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell border-l-4 border-green-lighter"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="http://www.aiskyeye.com/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">VisDrone2019</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        The VisDrone2019 dataset is collected by the AISKYEYE team at Lab of Machine Learning and Data Mining , Tianjin University, China. The benchmark dataset consists of 288 video clips formed by 261,908 frames and 10,209 static images, captured by various drone-mounted cameras, covering a wide range of aspects including location (taken from 14 different cities separated by thousands of kilometers in China), environment (urban and country), objects (pedestrian, vehicles, bicycles, etc.), and density (sparse and crowded scenes). 
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="http://www.aiskyeye.com/upfile/Vision_Meets_Drones_A_Challenge.pdf" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden border-l-4 border-green-lighter" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://www.aiskyeye.com/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">VisDrone2019</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          The VisDrone2019 dataset is collected by the AISKYEYE team at Lab of Machine Learning and Data Mining , Tianjin University, China. The benchmark dataset consists of 288 video clips formed by 261,908 frames and 10,209 static images, captured by various drone-mounted cameras, covering a wide range of aspects including location (taken from 14 different cities separated by thousands of kilometers in China), environment (urban and country), objects (pedestrian, vehicles, bicycles, etc.), and density (sparse and crowded scenes). 
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell border-l-4 border-green-lighter"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-plus-square text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 60px; margin-left: -16px;">Medical</div></div> <div class="truncate"><a href="https://www.medicalimageanalysis.com/research/skinia" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Datasets for skin image analysis</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        A list of datasets for skin image analysis, from the 'Visual Diagnosis of Dermatological Disorders: Human and Machine Performance' paper.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Various
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">This is a list of several datasets, check the links on the website for individual licenses</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1906.01256" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden border-l-4 border-green-lighter" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-plus-square text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://www.medicalimageanalysis.com/research/skinia" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Datasets for skin image analysis</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          A list of datasets for skin image analysis, from the 'Visual Diagnosis of Dermatological Disorders: Human and Machine Performance' paper.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Various
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">This is a list of several datasets, check the links on the website for individual licenses</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell border-l-4 border-green-lighter"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-align-left text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">NLP</div></div> <div class="truncate"><a href="https://www.uni-mannheim.de/en/dws/research/resources/opiec/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">OPIEC</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        OPIEC is an Open Information Extraction (OIE) corpus, constructed from the entire English Wikipedia. It containing more than 341M triples. Each triple from the corpus is composed of rich meta-data: each token from the subj / obj / rel along with NLP annotations (POS tag, NER tag, ...), provenance sentence (along with its dependency parse, sentence order relative to the article), original (golden) links contained in the Wikipedia articles, space / time, etc.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      CC-BY-SA 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution-ShareAlike 4.0 International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit,
ShareAlike - if you make changes, you must distribute your contributions.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1904.12324" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden border-l-4 border-green-lighter" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-align-left text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://www.uni-mannheim.de/en/dws/research/resources/opiec/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">OPIEC</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          OPIEC is an Open Information Extraction (OIE) corpus, constructed from the entire English Wikipedia. It containing more than 341M triples. Each triple from the corpus is composed of rich meta-data: each token from the subj / obj / rel along with NLP annotations (POS tag, NER tag, ...), provenance sentence (along with its dependency parse, sentence order relative to the article), original (golden) links contained in the Wikipedia articles, space / time, etc.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          CC-BY-SA 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution-ShareAlike 4.0 International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit,
ShareAlike - if you make changes, you must distribute your contributions.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-car text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 100px; margin-left: -16px;">Self-driving</div></div> <div class="truncate"><a href="https://www.audi-electronics-venture.de/aev/web/en/driving-dataset.html" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">AEV Autonomous Driving Dataset</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        The dataset features 2D semantic segmentation, 3D point clouds, 3D bounding boxes, and vehicle bus data. Dataset includes more than 40,000 frames with semantic segmentation image and point cloud labels, of which more than 12,000 frames also have annotations for 3D bounding boxes. In addition, we provide unlabelled sensor data (approx. 390,000 frames) for sequences with several loops, recorded in three cities. A2D2 is around 2.3 TB in total.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      CC BY-ND 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution No Derivatives 4.0 International (CC BY ND 4.0) - 
You are free to:
Share - copy and redistribute,
Under the following terms:
Attribution - you must give approprate credit.,
NoDerivatives - you may not redistribute the modified material.</div></td> <td class="p-4 text-right hidden lg:table-cell"><i class="text-grey-light far fa-file"></i></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-car text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://www.audi-electronics-venture.de/aev/web/en/driving-dataset.html" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">AEV Autonomous Driving Dataset</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          The dataset features 2D semantic segmentation, 3D point clouds, 3D bounding boxes, and vehicle bus data. Dataset includes more than 40,000 frames with semantic segmentation image and point cloud labels, of which more than 12,000 frames also have annotations for 3D bounding boxes. In addition, we provide unlabelled sensor data (approx. 390,000 frames) for sequences with several loops, recorded in three cities. A2D2 is around 2.3 TB in total.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          CC BY-ND 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution No Derivatives 4.0 International (CC BY ND 4.0) - 
You are free to:
Share - copy and redistribute,
Under the following terms:
Attribution - you must give approprate credit.,
NoDerivatives - you may not redistribute the modified material.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="http://bigearth.net/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">BigEarthNet</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        The BigEarthNet is a new large-scale Sentinel-2 benchmark archive, consisting of 590,326 Sentinel-2 image patches. To construct the BigEarthNet, 125 Sentinel-2 tiles acquired between June 2017 and May 2018 over the 10 countries (Austria, Belgium, Finland, Ireland, Kosovo, Lithuania, Luxembourg, Portugal, Serbia, Switzerland) of Europe were initially selected. All the tiles were atmospherically corrected by the Sentinel-2 Level 2A product generation and formatting tool (sen2cor). Then, they were divided into 590,326 non-overlapping image patches. Each image patch was annotated by the multiple land-cover classes (i.e., multi-labels) that were provided from the CORINE Land Cover database of the year 2018.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      CDLA Permissive
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">The CDLA-Permissive agreement is similar to permissive open source licenses in that the publisher of data allows anyone to use, modify and do what they want with the data with no obligations to share any of their changes or modifications.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="http://bigearth.net/static/documents/BigEarthNet_IGARSS_2019.pdf" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://bigearth.net/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">BigEarthNet</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          The BigEarthNet is a new large-scale Sentinel-2 benchmark archive, consisting of 590,326 Sentinel-2 image patches. To construct the BigEarthNet, 125 Sentinel-2 tiles acquired between June 2017 and May 2018 over the 10 countries (Austria, Belgium, Finland, Ireland, Kosovo, Lithuania, Luxembourg, Portugal, Serbia, Switzerland) of Europe were initially selected. All the tiles were atmospherically corrected by the Sentinel-2 Level 2A product generation and formatting tool (sen2cor). Then, they were divided into 590,326 non-overlapping image patches. Each image patch was annotated by the multiple land-cover classes (i.e., multi-labels) that were provided from the CORINE Land Cover database of the year 2018.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          CDLA Permissive
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">The CDLA-Permissive agreement is similar to permissive open source licenses in that the publisher of data allows anyone to use, modify and do what they want with the data with no obligations to share any of their changes or modifications.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://deepfakedetectionchallenge.ai/dataset" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Deepfake Detection Challenge Dataset</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        Facebook, Microsoft, Amazon Web Services, and the Partnership on AI have created the Deepfake Detection Challenge to encourage research into deepfake detection. Dataset consists of around 5000 videos, both original and manipulated. To build the dataset, the researchers crowdsourced videos from people while "ensuring a variability in gender, skin tone and age".
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1910.08854" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://deepfakedetectionchallenge.ai/dataset" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Deepfake Detection Challenge Dataset</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          Facebook, Microsoft, Amazon Web Services, and the Partnership on AI have created the Deepfake Detection Challenge to encourage research into deepfake detection. Dataset consists of around 5000 videos, both original and manipulated. To build the dataset, the researchers crowdsourced videos from people while "ensuring a variability in gender, skin tone and age".
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="http://www.cbsr.ia.ac.cn/users/sfzhang/WiderPerson/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">WiderPerson</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        The WiderPerson dataset is a pedestrian detection benchmark dataset in the wild, of which images are selected from a wide range of scenarios, no longer limited to the traffic scenario. We choose 13,382 images and label about 400K annotations with various kinds of occlusions. We randomly select 8000/1000/4382 images as training, validation and testing subsets. 
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1909.12118" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://www.cbsr.ia.ac.cn/users/sfzhang/WiderPerson/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">WiderPerson</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          The WiderPerson dataset is a pedestrian detection benchmark dataset in the wild, of which images are selected from a wide range of scenarios, no longer limited to the traffic scenario. We choose 13,382 images and label about 400K annotations with various kinds of occlusions. We randomly select 8000/1000/4382 images as training, validation and testing subsets. 
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://vcl3d.github.io/3D60/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">3D60</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        3D60 is a collective dataset generated in the context of various 360 vision research works. It comprises multi-modal (i.e. color, depth and normal) omnidirectional stereo renders (i.e. horizontal and vertical) of scenes from realistic and synthetic large-scale 3D datasets (Matterport3D, Stanford2D3D, SunCG). Contains 224,406 spherical panoramas.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1909.08112" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://vcl3d.github.io/3D60/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">3D60</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          3D60 is a collective dataset generated in the context of various 360 vision research works. It comprises multi-modal (i.e. color, depth and normal) omnidirectional stereo renders (i.e. horizontal and vertical) of scenes from realistic and synthetic large-scale 3D datasets (Matterport3D, Stanford2D3D, SunCG). Contains 224,406 spherical panoramas.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-question-circle text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">QA</div></div> <div class="truncate"><a href="https://www.tau-nlp.org/commonsenseqa" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">CommonsenseQA</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        CommonsenseQA is a new multiple-choice question answering dataset that requires different types of commonsense knowledge to predict the correct answers . It contains 12,102 questions with one correct answer and four distractor answers.  The dataset is provided in two major training/validation/testing set splits: "Random split" which is the main evaluation split, and "Question token split".
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Not found
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">License information not found</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1811.00937" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-question-circle text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://www.tau-nlp.org/commonsenseqa" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">CommonsenseQA</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          CommonsenseQA is a new multiple-choice question answering dataset that requires different types of commonsense knowledge to predict the correct answers . It contains 12,102 questions with one correct answer and four distractor answers.  The dataset is provided in two major training/validation/testing set splits: "Random split" which is the main evaluation split, and "Question token split".
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Not found
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">License information not found</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-car text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 100px; margin-left: -16px;">Self-driving</div></div> <div class="truncate"><a href="https://dbarnes.github.io/radar-robotcar-dataset/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Oxford Radar RobotCar Dataset</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        The Oxford Radar RobotCar Dataset is a radar extension to The Oxford RobotCar Dataset. We provide data from a Navtech CTS350-X Millimetre-Wave FMCW radar and Dual Velodyne HDL-32E LIDARs with optimised ground truth radar odometry for 280 km of driving around Oxford, UK (in addition to all sensors in the original Oxford RobotCar Dataset).
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      CC-BY-NC-SA 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution-NonCommercial-ShareAlike International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon,
Under the following terms:
Attribution - you must give approprate credit,
NonCommercial - you may not use the material for commercial purposes,
ShareAlike - if you make changes, you must distribute your contributions.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1909.013" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-car text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://dbarnes.github.io/radar-robotcar-dataset/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Oxford Radar RobotCar Dataset</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          The Oxford Radar RobotCar Dataset is a radar extension to The Oxford RobotCar Dataset. We provide data from a Navtech CTS350-X Millimetre-Wave FMCW radar and Dual Velodyne HDL-32E LIDARs with optimised ground truth radar odometry for 280 km of driving around Oxford, UK (in addition to all sensors in the original Oxford RobotCar Dataset).
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          CC-BY-NC-SA 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution-NonCommercial-ShareAlike International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon,
Under the following terms:
Attribution - you must give approprate credit,
NonCommercial - you may not use the material for commercial purposes,
ShareAlike - if you make changes, you must distribute your contributions.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-align-left text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">NLP</div></div> <div class="truncate"><a href="https://github.com/cs-chan/Total-Text-Dataset" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Total Text</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        The Total-Text consists of 1555 images with more than 3 different text orientations: Horizontal, Multi-Oriented, and Curved, one of a kind.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      BSD
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">BSD 3-Clause "New" or "Revised" License -

A permissive license similar to the BSD 2-Clause License, but with a 3rd clause that prohibits others from using the name of the project or its contributors to promote derived products without written consent.</div></td> <td class="p-4 text-right hidden lg:table-cell"><i class="text-grey-light far fa-file"></i></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-align-left text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://github.com/cs-chan/Total-Text-Dataset" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Total Text</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          The Total-Text consists of 1555 images with more than 3 different text orientations: Horizontal, Multi-Oriented, and Curved, one of a kind.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          BSD
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">BSD 3-Clause "New" or "Revised" License -

A permissive license similar to the BSD 2-Clause License, but with a 3rd clause that prohibits others from using the name of the project or its contributors to promote derived products without written consent.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-align-left text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">NLP</div></div> <div class="truncate"><a href="https://rrc.cvc.uab.es/?ch=14" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">ArT</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        ArT is a combination of Total-Text, SCUT-CTW1500 and Baidu Curved Scene Text, which were collected with the motive of introducing the arbitrary-shaped text problem to the scene text community. There is a total of 10,166 images in the ArT dataset. The ArT dataset was collected with text shape diversity in mind, hence all existing text shapes (i.e. horizontal, multi-oriented, and curved) have high number of existence in the dataset, which makes it an unique dataset.

      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1909.07145" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-align-left text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://rrc.cvc.uab.es/?ch=14" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">ArT</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          ArT is a combination of Total-Text, SCUT-CTW1500 and Baidu Curved Scene Text, which were collected with the motive of introducing the arbitrary-shaped text problem to the scene text community. There is a total of 10,166 images in the ArT dataset. The ArT dataset was collected with text shape diversity in mind, hence all existing text shapes (i.e. horizontal, multi-oriented, and curved) have high number of existence in the dataset, which makes it an unique dataset.

        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="http://www.cs.albany.edu/~lsw/celeb-deepfakeforensics.html" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Celeb-DF</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        DeepFake Forensics (Celeb-DF) dataset contains real and DeepFake synthesized videos having similar visual quality on par with those circulated online. The Celeb-DF dataset includes 408 original videos collected from YouTube with subjects of different ages, ethic groups and genders, and 795 DeepFake videos synthesized from these real videos.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1909.12962" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://www.cs.albany.edu/~lsw/celeb-deepfakeforensics.html" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Celeb-DF</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          DeepFake Forensics (Celeb-DF) dataset contains real and DeepFake synthesized videos having similar visual quality on par with those circulated online. The Celeb-DF dataset includes 408 original videos collected from YouTube with subjects of different ages, ethic groups and genders, and 795 DeepFake videos synthesized from these real videos.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://github.com/cs-chan/Exclusively-Dark-Image-Dataset" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">ExDARK Dataset</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        The Exclusively Dark (ExDARK) dataset is a collection of 7,363 low-light images from very low-light environments to twilight (i.e 10 different conditions) with 12 object classes (similar to PASCAL VOC) annotated on both image class level and local object bounding boxes
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      BSD
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">BSD 3-Clause "New" or "Revised" License -

A permissive license similar to the BSD 2-Clause License, but with a 3rd clause that prohibits others from using the name of the project or its contributors to promote derived products without written consent.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1805.11227" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://github.com/cs-chan/Exclusively-Dark-Image-Dataset" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">ExDARK Dataset</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          The Exclusively Dark (ExDARK) dataset is a collection of 7,363 low-light images from very low-light environments to twilight (i.e 10 different conditions) with 12 object classes (similar to PASCAL VOC) annotated on both image class level and local object bounding boxes
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          BSD
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">BSD 3-Clause "New" or "Revised" License -

A permissive license similar to the BSD 2-Clause License, but with a 3rd clause that prohibits others from using the name of the project or its contributors to promote derived products without written consent.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-align-left text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">NLP</div></div> <div class="truncate"><a href="https://github.com/google-research-datasets/dstc8-schema-guided-dialogue" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Schema-Guided Dialogue</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        Schema-Guided Dialogue (SGD) dataset, containing over 16k multi-domain conversations spanning 16 domains. Our dataset exceeds the existing task-oriented dialogue corpora in scale, while also highlighting the challenges associated with building large-scale virtual assistants. It provides a challenging testbed for a number of tasks including language understanding, slot filling, dialogue state tracking and response generation.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Not found
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">License information not found</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1909.05855" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-align-left text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://github.com/google-research-datasets/dstc8-schema-guided-dialogue" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Schema-Guided Dialogue</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          Schema-Guided Dialogue (SGD) dataset, containing over 16k multi-domain conversations spanning 16 domains. Our dataset exceeds the existing task-oriented dialogue corpora in scale, while also highlighting the challenges associated with building large-scale virtual assistants. It provides a challenging testbed for a number of tasks including language understanding, slot filling, dialogue state tracking and response generation.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Not found
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">License information not found</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://www.eecs.yorku.ca/~kamel/sidd/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">SIDD</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        The Smartphone Image Denoising Dataset (SIDD), of ~30,000 noisy images from 10 scenes under different lighting conditions using five representative smartphone cameras and generated their ground truth images.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://www.eecs.yorku.ca/~kamel/sidd/files/SIDD_CVPR_2018.pdf" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://www.eecs.yorku.ca/~kamel/sidd/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">SIDD</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          The Smartphone Image Denoising Dataset (SIDD), of ~30,000 noisy images from 10 scenes under different lighting conditions using five representative smartphone cameras and generated their ground truth images.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-car text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 100px; margin-left: -16px;">Self-driving</div></div> <div class="truncate"><a href="https://github.com/Robotics-BUT/Brno-Urban-Dataset" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Brno Urban Dataset</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        A new dataset recorded in Brno, Czech Republic. It offers data from four WUXGA cameras, two 3D LiDARs, inertial measurement unit, infrared camera and especially differential RTK GNSS receiver with centimetre accuracy which, to the best knowledge of the authors, is not available from any other public dataset so far. In addition, all the data are precisely timestamped with sub-millisecond precision to allow wider range of applications. At the time of publishing of the paper, it contains recordings of more than 350 km of rides in varying environments.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      MIT
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">MIT - You are free to: use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the work. Under the following terms: the work is provided "as is", you must include copyright and the license in all copies or substantial uses of the work.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1909.06897" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-car text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://github.com/Robotics-BUT/Brno-Urban-Dataset" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Brno Urban Dataset</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          A new dataset recorded in Brno, Czech Republic. It offers data from four WUXGA cameras, two 3D LiDARs, inertial measurement unit, infrared camera and especially differential RTK GNSS receiver with centimetre accuracy which, to the best knowledge of the authors, is not available from any other public dataset so far. In addition, all the data are precisely timestamped with sub-millisecond precision to allow wider range of applications. At the time of publishing of the paper, it contains recordings of more than 350 km of rides in varying environments.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          MIT
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">MIT - You are free to: use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the work. Under the following terms: the work is provided "as is", you must include copyright and the license in all copies or substantial uses of the work.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://github.com/MemoonaTahira/CrowdFix" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">CrowdFix</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        Dataset of Human Eye Fixation over Crowd Videos. CrowdFix includes 434 videos with diverse crowd scenes, containing a total of 37,493 frames and 1,249 seconds. The diverse content refers to different crowd activities under three distinct categories - Sparse, Dense Free Flowing and Dense Congested. All videos are at 720p resolution and 30 Hz frame rate.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Not found
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">License information not found</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1910.02618" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://github.com/MemoonaTahira/CrowdFix" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">CrowdFix</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          Dataset of Human Eye Fixation over Crowd Videos. CrowdFix includes 434 videos with diverse crowd scenes, containing a total of 37,493 frames and 1,249 seconds. The diverse content refers to different crowd activities under three distinct categories - Sparse, Dense Free Flowing and Dense Congested. All videos are at 720p resolution and 30 Hz frame rate.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Not found
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">License information not found</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-car text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 100px; margin-left: -16px;">Self-driving</div></div> <div class="truncate"><a href="https://interaction-dataset.com/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">INTERACTION Dataset</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        The INTERACTION dataset contains naturalistic motions of various traffic participants in a variety of highly interactive driving scenarios. Using drones and traffic cameras, trajectories were captured from different countries, including the US, Germany, China and other countries.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      Research and commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Research and commercial licenses available.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1910.03088" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-car text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://interaction-dataset.com/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">INTERACTION Dataset</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          The INTERACTION dataset contains naturalistic motions of various traffic participants in a variety of highly interactive driving scenarios. Using drones and traffic cameras, trajectories were captured from different countries, including the US, Germany, China and other countries.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          Research and commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Research and commercial licenses available.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://diode-dataset.org/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">DIODE: A Dense Indoor and Outdoor DEpth Dataset</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        DIODE (Dense Indoor and Outdoor DEpth) is a dataset that contains diverse high-resolution color images with accurate, dense, wide-range depth measurements. It is the first public dataset to include RGBD images of indoor and outdoor scenes obtained with one sensor suite. 
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      MIT
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">MIT - You are free to: use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the work. Under the following terms: the work is provided "as is", you must include copyright and the license in all copies or substantial uses of the work.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1908.00463" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://diode-dataset.org/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">DIODE: A Dense Indoor and Outdoor DEpth Dataset</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          DIODE (Dense Indoor and Outdoor DEpth) is a dataset that contains diverse high-resolution color images with accurate, dense, wide-range depth measurements. It is the first public dataset to include RGBD images of indoor and outdoor scenes obtained with one sensor suite. 
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          MIT
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">MIT - You are free to: use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the work. Under the following terms: the work is provided "as is", you must include copyright and the license in all copies or substantial uses of the work.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://generated.photos/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">100,000 Faces</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        100,000 Faces Generated by AI. We have built an original machine learning dataset, and used StyleGAN (an amazing resource by NVIDIA) to construct a realistic set of 100,000 faces. Our dataset has been built by taking 29,000+ photos of 69 different models over the last 2 years in our studio.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><i class="text-grey-light far fa-file"></i></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://generated.photos/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">100,000 Faces</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          100,000 Faces Generated by AI. We have built an original machine learning dataset, and used StyleGAN (an amazing resource by NVIDIA) to construct a realistic set of 100,000 faces. Our dataset has been built by taking 29,000+ photos of 69 different models over the last 2 years in our studio.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://www.objects365.org/overview.html" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Objects365</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        Objects365 is a brand new dataset, designed to spur object detection research with a focus on diverse objects in the Wild:

365 categories
600k images
10 million bounding boxes
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      CC BY 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution 4.0 International (CC BY 4.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></td> <td class="p-4 text-right hidden lg:table-cell"><i class="text-grey-light far fa-file"></i></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://www.objects365.org/overview.html" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Objects365</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          Objects365 is a brand new dataset, designed to spur object detection research with a focus on diverse objects in the Wild:

365 categories
600k images
10 million bounding boxes
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          CC BY 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution 4.0 International (CC BY 4.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="http://kaldir.vc.in.tum.de/faceforensics_benchmark/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">FaceForensics Benchmark</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        FaceForensics++ is a forensics dataset consisting of 1000 original video sequences that have been manipulated with four automated face manipulation methods: Deepfakes, Face2Face, FaceSwap and NeuralTextures. The data has been sourced from 977 youtube videos and all videos contain a trackable mostly frontal face without occlusions which enables automated tampering methods to generate realistic forgeries. As we provide binary masks the data can be used for image and video classification as well as segmentation. In addition, we provide 1000 Deepfakes models to generate and augment new data.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1901.08971" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://kaldir.vc.in.tum.de/faceforensics_benchmark/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">FaceForensics Benchmark</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          FaceForensics++ is a forensics dataset consisting of 1000 original video sequences that have been manipulated with four automated face manipulation methods: Deepfakes, Face2Face, FaceSwap and NeuralTextures. The data has been sourced from 977 youtube videos and all videos contain a trackable mostly frontal face without occlusions which enables automated tampering methods to generate realistic forgeries. As we provide binary masks the data can be used for image and video classification as well as segmentation. In addition, we provide 1000 Deepfakes models to generate and augment new data.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-align-left text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">NLP</div></div> <div class="truncate"><a href="https://tabfact.github.io/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">TabFact: A Large-scale Dataset for Table-based Fact Verification</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        We introduce a large-scale dataset called TabFact(website: https://tabfact.github.io/), which consists of 117,854 manually annotated statements with regard to 16,573 Wikipedia tables, their relations are classified as ENTAILED and REFUTED.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      MIT
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">MIT - You are free to: use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the work. Under the following terms: the work is provided "as is", you must include copyright and the license in all copies or substantial uses of the work.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1909.02164" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-align-left text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://tabfact.github.io/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">TabFact: A Large-scale Dataset for Table-based Fact Verification</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          We introduce a large-scale dataset called TabFact(website: https://tabfact.github.io/), which consists of 117,854 manually annotated statements with regard to 16,573 Wikipedia tables, their relations are classified as ENTAILED and REFUTED.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          MIT
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">MIT - You are free to: use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the work. Under the following terms: the work is provided "as is", you must include copyright and the license in all copies or substantial uses of the work.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://github.com/olivesgatech/CURE-TSD" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">CURE-TSD</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        CURE-TSD: Challenging Unreal and Real Environments for Traffic Sign Detection. The video sequences in the CURE-TSD dataset are grouped into two classes: real data and unreal data. Real data correspond to processed versions of sequences acquired from real world. Unreal data corresponds to synthesized sequences generated in a virtual environment. There are 49 real sequences and 49 unreal sequences that do not include any specific challenge. We have 34 training videos and 15 test videos in both real and unreal sequences that are challenge-free. There are 300 frames in each video sequence. There are 49 challenge-free real video sequences processed with 12 different types of effects and 5 different challenge levels. Moreover, there are 49 synthesized video sequences processed with 11 different types of effects and 5 different challenge levels. In total, there are 5,733 video sequences, which include around 1.72 million frames.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Not found
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">License information not found</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1908.11262" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://github.com/olivesgatech/CURE-TSD" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">CURE-TSD</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          CURE-TSD: Challenging Unreal and Real Environments for Traffic Sign Detection. The video sequences in the CURE-TSD dataset are grouped into two classes: real data and unreal data. Real data correspond to processed versions of sequences acquired from real world. Unreal data corresponds to synthesized sequences generated in a virtual environment. There are 49 real sequences and 49 unreal sequences that do not include any specific challenge. We have 34 training videos and 15 test videos in both real and unreal sequences that are challenge-free. There are 300 frames in each video sequence. There are 49 challenge-free real video sequences processed with 12 different types of effects and 5 different challenge levels. Moreover, there are 49 synthesized video sequences processed with 11 different types of effects and 5 different challenge levels. In total, there are 5,733 video sequences, which include around 1.72 million frames.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Not found
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">License information not found</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://v-sense.scss.tcd.ie/DublinCity/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">DublinCity: Annotated LiDAR Point Cloud </a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        Urban Modelling Group at University College Dublin (UCD) captured major area of Dublin city centre (i.e. around 5.6 km^2 including partially covered areas) was scanned via an ALS device which was carried out by helicopter in 2015. However, the actual focused area was around 2 km^2 which contains the most densest LiDAR point cloud and imagery dataset. The flight altitude was mostly around 300m and the total journey was performed in 41 flight path strips. The datasets is made up of over 260 million laser scanning points labelled into 100,000 objects.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1909.03613" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://v-sense.scss.tcd.ie/DublinCity/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">DublinCity: Annotated LiDAR Point Cloud </a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          Urban Modelling Group at University College Dublin (UCD) captured major area of Dublin city centre (i.e. around 5.6 km^2 including partially covered areas) was scanned via an ALS device which was carried out by helicopter in 2015. However, the actual focused area was around 2 km^2 which contains the most densest LiDAR point cloud and imagery dataset. The flight altitude was mostly around 300m and the total journey was performed in 41 flight path strips. The datasets is made up of over 260 million laser scanning points labelled into 100,000 objects.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-car text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 100px; margin-left: -16px;">Self-driving</div></div> <div class="truncate"><a href="https://github.com/I2RDL2/ASTAR-3D" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">A*3D</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        A*3D dataset is a step forward to make autonomous driving safer for pedestrians and the public in the real world.
230K human-labeled 3D object annotations in 39,179 LiDAR point cloud frames and corresponding frontal-facing RGB images.
Captured at different times (day, night) and weathers (sun, cloud, rain).
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1909.07541" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-car text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://github.com/I2RDL2/ASTAR-3D" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">A*3D</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          A*3D dataset is a step forward to make autonomous driving safer for pedestrians and the public in the real world.
230K human-labeled 3D object annotations in 39,179 LiDAR point cloud frames and corresponding frontal-facing RGB images.
Captured at different times (day, night) and weathers (sun, cloud, rain).
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-align-left text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">NLP</div></div> <div class="truncate"><a href="https://ai.google/tools/datasets/coached-conversational-preference-elicitation" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Google Coached Conversational Preference Elicitation</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        A dataset consisting of 502 dialogs with 12,000 annotated utterances between a user and an assistant discussing movie preferences in natural language. It was collected using a Wizard-of-Oz methodology between two paid crowd-workers, where one worker plays the role of an 'assistant', while the other plays the role of a 'user'.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      CC-BY-SA 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution-ShareAlike 4.0 International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit,
ShareAlike - if you make changes, you must distribute your contributions.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://ai.google/research/pubs/pub48414.pdf" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-align-left text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://ai.google/tools/datasets/coached-conversational-preference-elicitation" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Google Coached Conversational Preference Elicitation</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          A dataset consisting of 502 dialogs with 12,000 annotated utterances between a user and an assistant discussing movie preferences in natural language. It was collected using a Wizard-of-Oz methodology between two paid crowd-workers, where one worker plays the role of an 'assistant', while the other plays the role of a 'user'.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          CC-BY-SA 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution-ShareAlike 4.0 International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit,
ShareAlike - if you make changes, you must distribute your contributions.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://qmul-openlogo.github.io/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">QMUL-OpenLogo</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        QMUL-OpenLogo contains 27,083 images from 352 logo classes, built by aggregating and refining 7 existing datasets and establishing an open logo detection evaluation protocol. 
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1807.01964" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://qmul-openlogo.github.io/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">QMUL-OpenLogo</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          QMUL-OpenLogo contains 27,083 images from 352 logo classes, built by aggregating and refining 7 existing datasets and establishing an open logo detection evaluation protocol. 
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-align-left text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">NLP</div></div> <div class="truncate"><a href="https://ai.google/tools/datasets/taskmaster-1" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Taskmaster-1 </a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        The dataset consists of 13,215 task-based dialogs, including 5,507 spoken and 7,708 written dialogs created with two distinct procedures. Each conversation falls into one of six domains: ordering pizza, creating auto repair appointments, setting up ride service, ordering movie tickets, ordering coffee drinks and making restaurant reservations.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      CC-BY-SA 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution-ShareAlike 4.0 International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit,
ShareAlike - if you make changes, you must distribute your contributions.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://ai.google/research/pubs/pub48484.pdf" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-align-left text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://ai.google/tools/datasets/taskmaster-1" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Taskmaster-1 </a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          The dataset consists of 13,215 task-based dialogs, including 5,507 spoken and 7,708 written dialogs created with two distinct procedures. Each conversation falls into one of six domains: ordering pizza, creating auto repair appointments, setting up ride service, ordering movie tickets, ordering coffee drinks and making restaurant reservations.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          CC-BY-SA 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution-ShareAlike 4.0 International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit,
ShareAlike - if you make changes, you must distribute your contributions.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-car text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 100px; margin-left: -16px;">Self-driving</div></div> <div class="truncate"><a href="https://waymo.com/open/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Waymo Open Dataset</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        The Waymo Open Dataset is comprised of high resolution sensor data collected by Waymo self-driving cars in a wide variety of conditions. We are releasing this dataset publicly to aid the research community in making advancements in machine perception and self-driving technology. The Waymo Open Dataset currently contains lidar and camera data from 1,000 segments (20s each): 1,000 segments of 20s each, collected at 10Hz (200,000 frames) in diverse geographies and conditions, Labels for 4 object classes - Vehicles, Pedestrians, Cyclists, Signs, 12M 3D bounding box labels with tracking IDs on lidar data, 1.2M 2D bounding box labels with tracking IDs on camera data...
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><i class="text-grey-light far fa-file"></i></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-car text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://waymo.com/open/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Waymo Open Dataset</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          The Waymo Open Dataset is comprised of high resolution sensor data collected by Waymo self-driving cars in a wide variety of conditions. We are releasing this dataset publicly to aid the research community in making advancements in machine perception and self-driving technology. The Waymo Open Dataset currently contains lidar and camera data from 1,000 segments (20s each): 1,000 segments of 20s each, collected at 10Hz (200,000 frames) in diverse geographies and conditions, Labels for 4 object classes - Vehicles, Pedestrians, Cyclists, Signs, 12M 3D bounding box labels with tracking IDs on lidar data, 1.2M 2D bounding box labels with tracking IDs on camera data...
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-car text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 100px; margin-left: -16px;">Self-driving</div></div> <div class="truncate"><a href="https://level5.lyft.com/dataset/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Lyft Level 5</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        A comprehensive, large-scale dataset featuring the raw sensor camera and LiDAR inputs as perceived by a fleet of multiple, high-end, autonomous vehicles in a bounded geographic area. This dataset also includes high quality, human-labelled 3D bounding boxes of traffic agents, an underlying HD spatial semantic map. Contains over 55,000 human-labeled 3D annotated frames; data from 7 cameras and up to 3 lidars; a drivable surface map; and, an underlying HD spatial semantic map. A semantic map provides context to reason about the presence and motion of the agents in the scenes. The provided map has over 4000 lane segments (2000 road segment lanes and about 2000 junction lanes) , 197 pedestrian crosswalks, 60 stop signs, 54 parking zones, 8 speed bumps, 11 speed humps.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      CC-BY-NC-SA 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution-NonCommercial-ShareAlike International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon,
Under the following terms:
Attribution - you must give approprate credit,
NonCommercial - you may not use the material for commercial purposes,
ShareAlike - if you make changes, you must distribute your contributions.</div></td> <td class="p-4 text-right hidden lg:table-cell"><i class="text-grey-light far fa-file"></i></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-car text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://level5.lyft.com/dataset/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Lyft Level 5</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          A comprehensive, large-scale dataset featuring the raw sensor camera and LiDAR inputs as perceived by a fleet of multiple, high-end, autonomous vehicles in a bounded geographic area. This dataset also includes high quality, human-labelled 3D bounding boxes of traffic agents, an underlying HD spatial semantic map. Contains over 55,000 human-labeled 3D annotated frames; data from 7 cameras and up to 3 lidars; a drivable surface map; and, an underlying HD spatial semantic map. A semantic map provides context to reason about the presence and motion of the agents in the scenes. The provided map has over 4000 lane segments (2000 road segment lanes and about 2000 junction lanes) , 197 pedestrian crosswalks, 60 stop signs, 54 parking zones, 8 speed bumps, 11 speed humps.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          CC-BY-NC-SA 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution-NonCommercial-ShareAlike International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon,
Under the following terms:
Attribution - you must give approprate credit,
NonCommercial - you may not use the material for commercial purposes,
ShareAlike - if you make changes, you must distribute your contributions.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-align-left text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">NLP</div></div> <div class="truncate"><a href="https://skylion007.github.io/OpenWebTextCorpus/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">OpenWebText</a></div> <div class="relative ml-2 hidden lg:block"><a href="https://academictorrents.com/details/36c39b25657ce1639ccec0a91cf242b42e1f01db" target="_blank" class="HoverTooltip rotate-180 fas fa-magnet text-xs text-indigo-lighter hover:text-indigo no-underline"></a> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 120px; margin-left: -16px;">Download torrent</div></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        Open WebText – an open source effort to reproduce OpenAI’s WebText dataset. This distribution was created by Aaron Gokaslan and Vanya Cohen of Brown University. Dataset was created by extracting all Reddit post urls from the Reddit submissions dataset. These links were deduplicated, filtered to exclude non-html content, and then shuffled randomly. The links were then distributed to several machines in parallel for download, and all web pages were extracted using the newspaper python package. Documents were hashed into sets of 5-grams and all documents that had a similarity threshold of greater than 0.5 were removed. The the remaining documents were tokenized, and documents with fewer than 128 tokens were removed. This left 38GB of text data (40GB using SI units) from 8,013,769 documents.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Various
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Dataset packaging is licensed under CC-0 but contains content that can have a different license, check the dataset download for more details.</div></td> <td class="p-4 text-right hidden lg:table-cell"><i class="text-grey-light far fa-file"></i></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-align-left text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://skylion007.github.io/OpenWebTextCorpus/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">OpenWebText</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          Open WebText – an open source effort to reproduce OpenAI’s WebText dataset. This distribution was created by Aaron Gokaslan and Vanya Cohen of Brown University. Dataset was created by extracting all Reddit post urls from the Reddit submissions dataset. These links were deduplicated, filtered to exclude non-html content, and then shuffled randomly. The links were then distributed to several machines in parallel for download, and all web pages were extracted using the newspaper python package. Documents were hashed into sets of 5-grams and all documents that had a similarity threshold of greater than 0.5 were removed. The the remaining documents were tokenized, and documents with fewer than 128 tokens were removed. This left 38GB of text data (40GB using SI units) from 8,013,769 documents.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Various
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Dataset packaging is licensed under CC-0 but contains content that can have a different license, check the dataset download for more details.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://www.lvisdataset.org/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">LVIS</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        LVIS is a new dataset for long tail object instance segmentation. 1000+ Categories: found by data-driven object discovery in 164k images. More than 2.2 million high quality instance segmentation masks.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      CC BY 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution 4.0 International (CC BY 4.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1908.03195" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://www.lvisdataset.org/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">LVIS</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          LVIS is a new dataset for long tail object instance segmentation. 1000+ Categories: found by data-driven object discovery in 164k images. More than 2.2 million high quality instance segmentation masks.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          CC BY 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution 4.0 International (CC BY 4.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-question-circle text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">QA</div></div> <div class="truncate"><a href="https://github.com/Websail-NU/CODAH" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">CODAH</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        CODAH is an adversarially-constructed evaluation dataset with 2.8k questions for testing common sense. CODAH forms a challenging extension to the SWAG dataset, which tests commonsense knowledge using sentence-completion questions that describe situations observed in video.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Not found
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">License information not found</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1904.04365v4" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-question-circle text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://github.com/Websail-NU/CODAH" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">CODAH</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          CODAH is an adversarially-constructed evaluation dataset with 2.8k questions for testing common sense. CODAH forms a challenging extension to the SWAG dataset, which tests commonsense knowledge using sentence-completion questions that describe situations observed in video.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Not found
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">License information not found</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="http://tacodataset.org/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">TACO (Trash Annotations in Context)</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        Taco is an open image dataset of waste in the wild. It contains photos of litter taken under diverse environments, from tropical beaches to London streets. These images are manually labeled and segmented according to a hierarchical taxonomy to train and evaluate object detection algorithms.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      CC BY 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution 4.0 International (CC BY 4.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></td> <td class="p-4 text-right hidden lg:table-cell"><i class="text-grey-light far fa-file"></i></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://tacodataset.org/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">TACO (Trash Annotations in Context)</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          Taco is an open image dataset of waste in the wild. It contains photos of litter taken under diverse environments, from tropical beaches to London streets. These images are manually labeled and segmented according to a hierarchical taxonomy to train and evaluate object detection algorithms.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          CC BY 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution 4.0 International (CC BY 4.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://www.mapillary.com/dataset/trafficsign" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Mapillary Traffic Sign Dataset</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        A diverse street-level imagery dataset with bounding box annotations for detecting and classifying traffic signs around the world. 100,000 high-resolution images from all over the world with bounding box annotations of over 300 classes of traffic signs. The fully annotated set of the Mapillary Traffic Sign Dataset (MTSD) includes a total of 52,453 images with 257,543 traffic sign bounding boxes. The additional, partially annotated dataset contains 47,547 images with more than 80,000 signs that are automatically labeled with correspondence information from 3D reconstruction.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      Research and commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Research and commercial licenses available.</div></td> <td class="p-4 text-right hidden lg:table-cell"><i class="text-grey-light far fa-file"></i></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://www.mapillary.com/dataset/trafficsign" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Mapillary Traffic Sign Dataset</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          A diverse street-level imagery dataset with bounding box annotations for detecting and classifying traffic signs around the world. 100,000 high-resolution images from all over the world with bounding box annotations of over 300 classes of traffic signs. The fully annotated set of the Mapillary Traffic Sign Dataset (MTSD) includes a total of 52,453 images with 257,543 traffic sign bounding boxes. The additional, partially annotated dataset contains 47,547 images with more than 80,000 signs that are automatically labeled with correspondence information from 3D reconstruction.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          Research and commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Research and commercial licenses available.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-car text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 100px; margin-left: -16px;">Self-driving</div></div> <div class="truncate"><a href="https://www.argoverse.org/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Argoverse</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        Argoverse is a research collection with three distinct types of data. The first is a dataset with sensor data from 113 scenes observed by our fleet, with 3D tracking annotations on all objects. The second is a dataset of 300,000-plus scenarios observed by our fleet, wherein each scenario contains motion trajectories of all observed objects. The third is a set of HD maps of several neighborhoods in Pittsburgh and Miami, to add rich context for all of the data mentioned above. 
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      CC-BY-NC-SA 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution-NonCommercial-ShareAlike International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon,
Under the following terms:
Attribution - you must give approprate credit,
NonCommercial - you may not use the material for commercial purposes,
ShareAlike - if you make changes, you must distribute your contributions.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Chang_Argoverse_3D_Tracking_and_Forecasting_With_Rich_Maps_CVPR_2019_paper.pdf" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-car text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://www.argoverse.org/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Argoverse</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          Argoverse is a research collection with three distinct types of data. The first is a dataset with sensor data from 113 scenes observed by our fleet, with 3D tracking annotations on all objects. The second is a dataset of 300,000-plus scenarios observed by our fleet, wherein each scenario contains motion trajectories of all observed objects. The third is a set of HD maps of several neighborhoods in Pittsburgh and Miami, to add rich context for all of the data mentioned above. 
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          CC-BY-NC-SA 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution-NonCommercial-ShareAlike International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon,
Under the following terms:
Attribution - you must give approprate credit,
NonCommercial - you may not use the material for commercial purposes,
ShareAlike - if you make changes, you must distribute your contributions.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-question-circle text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">QA</div></div> <div class="truncate"><a href="https://www.thesocialiq.com/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Social-IQ</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        The dataset contains rigorously annotated and validated videos, questions and answers, as well as annotations for the complexity level of each question and answer. Social-IQ brings novel challenges to the field of artificial intelligence which sparks future research in social intelligence modeling, visual reasoning, and multimodal question answering. 1,250 videos, 7,500 questions, 33,000 correct answers, 22,500 incorrect answers.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Zadeh_Social-IQ_A_Question_Answering_Benchmark_for_Artificial_Social_Intelligence_CVPR_2019_paper.pdf" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-question-circle text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://www.thesocialiq.com/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Social-IQ</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          The dataset contains rigorously annotated and validated videos, questions and answers, as well as annotations for the complexity level of each question and answer. Social-IQ brings novel challenges to the field of artificial intelligence which sparks future research in social intelligence modeling, visual reasoning, and multimodal question answering. 1,250 videos, 7,500 questions, 33,000 correct answers, 22,500 incorrect answers.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-question-circle text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">QA</div></div> <div class="truncate"><a href="https://allennlp.org/drop" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">DROP</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        DROP is a crowdsourced, adversarially-created, 96k-question benchmark, in which a system must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting). These operations require a much more comprehensive understanding of the content of paragraphs than what was necessary for prior datasets.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      CC-BY-SA 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution-ShareAlike 4.0 International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit,
ShareAlike - if you make changes, you must distribute your contributions.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://pdfs.semanticscholar.org/dda6/fb309f62e2557a071522354d8c2c897a2805.pdf" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-question-circle text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://allennlp.org/drop" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">DROP</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          DROP is a crowdsourced, adversarially-created, 96k-question benchmark, in which a system must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting). These operations require a much more comprehensive understanding of the content of paragraphs than what was necessary for prior datasets.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          CC-BY-SA 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution-ShareAlike 4.0 International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit,
ShareAlike - if you make changes, you must distribute your contributions.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-align-left text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">NLP</div></div> <div class="truncate"><a href="https://super.gluebenchmark.com/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">SuperGLUE benchmark</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        SuperGLUE, a new benchmark styled after GLUE with a new set of more difficult language understanding tasks, improved resources, and a new public leaderboard. Full citation list of the datasets contained: {The CommitmentBank}: Investigating projection in naturally occurring discourse, Choice of plausible alternatives: An evaluation of commonsense causal reasoning, Looking beyond the surface: A challenge set for reading comprehension over multiple sentences, The {PASCAL} recognising textual entailment challenge, The second {PASCAL} recognising textual entailment challenge, The third {PASCAL} recognizing textual entailment challenge, The Fifth {PASCAL} Recognizing Textual Entailment Challenge, {WiC}: The Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations, The {W}inograd schema challenge.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Various
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">The dataset contains data from several sources, check the links on the website for individual licenses</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1905.00537" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-align-left text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://super.gluebenchmark.com/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">SuperGLUE benchmark</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          SuperGLUE, a new benchmark styled after GLUE with a new set of more difficult language understanding tasks, improved resources, and a new public leaderboard. Full citation list of the datasets contained: {The CommitmentBank}: Investigating projection in naturally occurring discourse, Choice of plausible alternatives: An evaluation of commonsense causal reasoning, Looking beyond the surface: A challenge set for reading comprehension over multiple sentences, The {PASCAL} recognising textual entailment challenge, The second {PASCAL} recognising textual entailment challenge, The third {PASCAL} recognizing textual entailment challenge, The Fifth {PASCAL} Recognizing Textual Entailment Challenge, {WiC}: The Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations, The {W}inograd schema challenge.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Various
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">The dataset contains data from several sources, check the links on the website for individual licenses</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="http://hake-mvig.cn/home/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Human Activity Knowledge Engine (HAKE)</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        Human Activity Knowledge Engine (HAKE) aims at promoting the human activity/action understanding. As a large-scale knowledge base, HAKE is built upon existing activity datasets, and supplies human instance action labels and corresponding body part level atomic action labels (Part States). Dataset contains 104 K+ images, 154 activity classes, 677 K+ human instances.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Not found
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">License information not found</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1904.06539" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://hake-mvig.cn/home/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Human Activity Knowledge Engine (HAKE)</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          Human Activity Knowledge Engine (HAKE) aims at promoting the human activity/action understanding. As a large-scale knowledge base, HAKE is built upon existing activity datasets, and supplies human instance action labels and corresponding body part level atomic action labels (Part States). Dataset contains 104 K+ images, 154 activity classes, 677 K+ human instances.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Not found
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">License information not found</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="http://pedx.io/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">PedX</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        PedX is a large-scale multi-modal collection of pedestrians at complex urban intersections. The dataset provides high-resolution stereo images and LiDAR data with manual 2D and automatic 3D annotations. The data was captured using two pairs of stereo cameras and four Velodyne LiDAR sensors.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      MIT
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">MIT - You are free to: use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the work. Under the following terms: the work is provided "as is", you must include copyright and the license in all copies or substantial uses of the work.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1809.03605" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://pedx.io/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">PedX</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          PedX is a large-scale multi-modal collection of pedestrians at complex urban intersections. The dataset provides high-resolution stereo images and LiDAR data with manual 2D and automatic 3D annotations. The data was captured using two pairs of stereo cameras and four Velodyne LiDAR sensors.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          MIT
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">MIT - You are free to: use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the work. Under the following terms: the work is provided "as is", you must include copyright and the license in all copies or substantial uses of the work.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://github.com/facebookresearch/Replica-Dataset" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Replica</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        The Replica Dataset is a dataset of high quality reconstructions of a variety of indoor spaces. Each reconstruction has clean dense geometry, high resolution and high dynamic range textures, glass and mirror surface information, planar segmentation as well as semantic class and instance segmentation.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1906.05797" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://github.com/facebookresearch/Replica-Dataset" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Replica</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          The Replica Dataset is a dataset of high quality reconstructions of a variety of indoor spaces. Each reconstruction has clean dense geometry, high resolution and high dynamic range textures, glass and mirror surface information, planar segmentation as well as semantic class and instance segmentation.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://github.com/PKU-IMRE/VERI-Wild" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">VERI-Wild</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        A large-scale vehicle ReID dataset in the wild (VERI-Wild) is captured from a large CCTV surveillance system consisting of 174 cameras across one month (30× 24h) under unconstrained scenarios. The cameras are distributed in a large urban district of more than 200km2. After data cleaning and annotation, 416,314 vehicle images of 40,671 identities are collected.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Not found
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">License information not found</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Lou_VERI-Wild_A_Large_Dataset_and_a_New_Method_for_Vehicle_CVPR_2019_paper.pdf" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://github.com/PKU-IMRE/VERI-Wild" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">VERI-Wild</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          A large-scale vehicle ReID dataset in the wild (VERI-Wild) is captured from a large CCTV surveillance system consisting of 174 cameras across one month (30× 24h) under unconstrained scenarios. The cameras are distributed in a large urban district of more than 200km2. After data cleaning and annotation, 416,314 vehicle images of 40,671 identities are collected.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Not found
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">License information not found</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="http://dronedataset.icg.tugraz.at/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Semantic Drone Dataset</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        The Semantic Drone Dataset focuses on semantic understanding of urban scenes for increasing the safety of autonomous drone flight and landing procedures. The imagery depicts  more than 20 houses from nadir (bird's eye) view acquired at an altitude of 5 to 30 meters above ground. A high resolution camera was used to acquire images at a size of 6000x4000px (24Mpx). The training set contains 400 publicly available images and the test set is made up of 200 private images.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><i class="text-grey-light far fa-file"></i></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://dronedataset.icg.tugraz.at/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Semantic Drone Dataset</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          The Semantic Drone Dataset focuses on semantic understanding of urban scenes for increasing the safety of autonomous drone flight and landing procedures. The imagery depicts  more than 20 houses from nadir (bird's eye) view acquired at an altitude of 5 to 30 meters above ground. A high resolution camera was used to acquire images at a size of 6000x4000px (24Mpx). The training set contains 400 publicly available images and the test set is made up of 200 private images.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://github.com/cvdfoundation/google-landmark" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Google Landmarks V2</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        This is the second version of the Google Landmarks dataset, which contains images annotated with labels representing human-made and natural landmarks. The dataset can be used for landmark recognition and retrieval experiments. This version of the dataset contains approximately 5 million images, split into 3 sets of images: train, index and test.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      CC BY 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution 4.0 International (CC BY 4.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></td> <td class="p-4 text-right hidden lg:table-cell"><i class="text-grey-light far fa-file"></i></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://github.com/cvdfoundation/google-landmark" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Google Landmarks V2</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          This is the second version of the Google Landmarks dataset, which contains images annotated with labels representing human-made and natural landmarks. The dataset can be used for landmark recognition and retrieval experiments. This version of the dataset contains approximately 5 million images, split into 3 sets of images: train, index and test.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          CC BY 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution 4.0 International (CC BY 4.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://boxy-dataset.com/boxy/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">The Boxy Vehicles Dataset</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        A large dataset of almost two million annotated vehicles
for training and evaluating object detection methods. 200,000 images. 1,990,000 annotated vehicles. 5 Megapixel resolution.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://boxy-dataset.com/static/boxy/boxy_preview.pdf" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://boxy-dataset.com/boxy/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">The Boxy Vehicles Dataset</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          A large dataset of almost two million annotated vehicles
for training and evaluating object detection methods. 200,000 images. 1,990,000 annotated vehicles. 5 Megapixel resolution.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://unsupervised-llamas.com/llamas/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">The Unsupervised Labeled Lane Markers Dataset</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        The Unsupervised Llamas dataset was annotated by creating high definition maps for automated driving including lane markers based on Lidar. The automated vehicle can be localized against these maps and the lane markers are projected into the camera frame. The 3D projection is optimized by minimizing the difference between already detected markers in the image and projected ones. Further improvements can likely be achieved by using better detectors, optimizing difference metrics, and adding some temporal consistency. 

Over 100,000 annotated images.
Annotations of over 100 meters.
Resolution of 1276 x 717 pixels.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://unsupervised-llamas.com/static/llamas/llamas_preview.pdf" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://unsupervised-llamas.com/llamas/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">The Unsupervised Labeled Lane Markers Dataset</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          The Unsupervised Llamas dataset was annotated by creating high definition maps for automated driving including lane markers based on Lidar. The automated vehicle can be localized against these maps and the lane markers are projected into the camera frame. The 3D projection is optimized by minimizing the difference between already detected markers in the image and projected ones. Further improvements can likely be achieved by using better detectors, optimizing difference metrics, and adding some temporal consistency. 

Over 100,000 annotated images.
Annotations of over 100 meters.
Resolution of 1276 x 717 pixels.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://storage.googleapis.com/openimages/web/index.html" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Google Open Images V5</a></div> <div class="relative ml-2 hidden lg:block"><a href="https://academictorrents.com/details/9e9194e21ce045deee8d811481b4cd676b20b06b" target="_blank" class="HoverTooltip rotate-180 fas fa-magnet text-xs text-indigo-lighter hover:text-indigo no-underline"></a> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 120px; margin-left: -16px;">Download torrent</div></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        Open Images is a dataset of ~9M images annotated with image-level labels, object bounding boxes, object segmentation masks, and visual relationships. It contains a total of 16M bounding boxes for 600 object classes on 1.9M images, making it the largest existing dataset with object location annotations. Open Images V5 features segmentation masks for 2.8 million object instances in 350 categories. Unlike bounding-boxes, which only identify regions in which an object is located, segmentation masks mark the outline of objects, characterizing their spatial extent to a much higher level of detail.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      CC BY 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution 4.0 International (CC BY 4.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1811.00982" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://storage.googleapis.com/openimages/web/index.html" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Google Open Images V5</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          Open Images is a dataset of ~9M images annotated with image-level labels, object bounding boxes, object segmentation masks, and visual relationships. It contains a total of 16M bounding boxes for 600 object classes on 1.9M images, making it the largest existing dataset with object location annotations. Open Images V5 features segmentation masks for 2.8 million object instances in 350 categories. Unlike bounding-boxes, which only identify regions in which an object is located, segmentation masks mark the outline of objects, characterizing their spatial extent to a much higher level of detail.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          CC BY 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution 4.0 International (CC BY 4.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://midair.ulg.ac.be/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Mid-Air</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        Mid-Air is a multi-modal synthetic dataset for low altitude drone flights in unstructured environments. It contains synchronized data captured by multiple sensors for a total of 54 trajectories and more than 420k video frames simulated in various climate conditions.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      CC-BY-NC-SA 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution-NonCommercial-ShareAlike International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon,
Under the following terms:
Attribution - you must give approprate credit,
NonCommercial - you may not use the material for commercial purposes,
ShareAlike - if you make changes, you must distribute your contributions.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://orbi.uliege.be/bitstream/2268/234665/3/Fonder2019MidAir.pdf" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://midair.ulg.ac.be/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Mid-Air</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          Mid-Air is a multi-modal synthetic dataset for low altitude drone flights in unstructured environments. It contains synchronized data captured by multiple sensors for a total of 54 trajectories and more than 420k video frames simulated in various climate conditions.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          CC-BY-NC-SA 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution-NonCommercial-ShareAlike International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon,
Under the following terms:
Attribution - you must give approprate credit,
NonCommercial - you may not use the material for commercial purposes,
ShareAlike - if you make changes, you must distribute your contributions.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-plus-square text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 60px; margin-left: -16px;">Medical</div></div> <div class="truncate"><a href="http://headctstudy.qure.ai/dataset" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">CQ500</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        We have made the CQ500 dataset of 491 scans with 193,317 slices publicly available so that others can compare and build upon the results we have achieved in the paper. We provide anonymized dicoms for all the 491 scans and the corresponding radiologists' reads.

The scans in the CQ500 dataset were generously provided by Centre for Advanced Research in Imaging, Neurosciences and Genomics(CARING), New Delhi, IN. The reads were done by three radiologists with an experience of 8, 12 and 20 years in cranial CT interpretation respectively.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      CC-BY-NC-SA 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution-NonCommercial-ShareAlike International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon,
Under the following terms:
Attribution - you must give approprate credit,
NonCommercial - you may not use the material for commercial purposes,
ShareAlike - if you make changes, you must distribute your contributions.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1803.05854" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-plus-square text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://headctstudy.qure.ai/dataset" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">CQ500</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          We have made the CQ500 dataset of 491 scans with 193,317 slices publicly available so that others can compare and build upon the results we have achieved in the paper. We provide anonymized dicoms for all the 491 scans and the corresponding radiologists' reads.

The scans in the CQ500 dataset were generously provided by Centre for Advanced Research in Imaging, Neurosciences and Genomics(CARING), New Delhi, IN. The reads were done by three radiologists with an experience of 8, 12 and 20 years in cranial CT interpretation respectively.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          CC-BY-NC-SA 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution-NonCommercial-ShareAlike International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon,
Under the following terms:
Attribution - you must give approprate credit,
NonCommercial - you may not use the material for commercial purposes,
ShareAlike - if you make changes, you must distribute your contributions.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-question-circle text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">QA</div></div> <div class="truncate"><a href="https://textvqa.org/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">TextVQA</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        TextVQA requires models to read and reason about text in images to answer questions about them. Specifically, models need to incorporate a new modality of text present in the images and reason over it to answer TextVQA questions. Dataset contains 28,408 images from OpenImages, 45,336 questions, 453,360 ground truth answers.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      CC BY 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution 4.0 International (CC BY 4.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1904.0892" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-question-circle text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://textvqa.org/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">TextVQA</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          TextVQA requires models to read and reason about text in images to answer questions about them. Specifically, models need to incorporate a new modality of text present in the images and reason over it to answer TextVQA questions. Dataset contains 28,408 images from OpenImages, 45,336 questions, 453,360 ground truth answers.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          CC BY 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution 4.0 International (CC BY 4.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-plus-square text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 60px; margin-left: -16px;">Medical</div></div> <div class="truncate"><a href="https://stanfordmlgroup.github.io/competitions/mrnet/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">MRNet</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        
The MRNet dataset consists of 1,370 knee MRI exams performed at Stanford University Medical Center. The dataset contains 1,104 (80.6%) abnormal exams, with 319 (23.3%) ACL tears and 508 (37.1%) meniscal tears; labels were obtained through manual extraction from clinical reports.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://stanfordmlgroup.github.io/projects/mrnet/" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-plus-square text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://stanfordmlgroup.github.io/competitions/mrnet/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">MRNet</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          
The MRNet dataset consists of 1,370 knee MRI exams performed at Stanford University Medical Center. The dataset contains 1,104 (80.6%) abnormal exams, with 319 (23.3%) ACL tears and 508 (37.1%) meniscal tears; labels were obtained through manual extraction from clinical reports.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://github.com/switchablenorms/DeepFashion2" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">DeepFashion2</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        It is a versatile benchmark of four tasks including clothes detection, pose estimation, segmentation, and retrieval. It has 801K clothing items where each item has rich annotations such as style, scale, viewpoint, occlusion, bounding box, dense landmarks and masks. There are also 873K Commercial-Consumer clothes pairs.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Not found
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">License information not found</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1901.07973" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://github.com/switchablenorms/DeepFashion2" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">DeepFashion2</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          It is a versatile benchmark of four tasks including clothes detection, pose estimation, segmentation, and retrieval. It has 801K clothing items where each item has rich annotations such as style, scale, viewpoint, occlusion, bounding box, dense landmarks and masks. There are also 873K Commercial-Consumer clothes pairs.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Not found
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">License information not found</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://github.com/visipedia/imat_comp" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">iMat Fashion 2019</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        While early work in computer vision addressed related clothing recognition tasks, these are not designed with fashion insiders’ needs in mind, possibly due to the research gap in fashion design and computer vision. To address this, we first propose a fashion taxonomy built by fashion experts, informed by product description from the internet. To capture the complex structure of fashion objects and ambiguity in descriptions obtained from crawling the web, our standardized taxonomy contains 46 apparel objects (27 main apparel items and 19 apparel parts), and 92 related fine-grained attributes. Secondly, a total of around 50K clothing images (10K with both segmentation and fine-grained attributes, 40K with apparel instance segmentation) in daily-life, celebrity events, and online shopping are labeled by both domain experts and crowd workers for fine-grained segmentation.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Not found
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">License information not found</div></td> <td class="p-4 text-right hidden lg:table-cell"><i class="text-grey-light far fa-file"></i></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://github.com/visipedia/imat_comp" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">iMat Fashion 2019</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          While early work in computer vision addressed related clothing recognition tasks, these are not designed with fashion insiders’ needs in mind, possibly due to the research gap in fashion design and computer vision. To address this, we first propose a fashion taxonomy built by fashion experts, informed by product description from the internet. To capture the complex structure of fashion objects and ambiguity in descriptions obtained from crawling the web, our standardized taxonomy contains 46 apparel objects (27 main apparel items and 19 apparel parts), and 92 related fine-grained attributes. Secondly, a total of around 50K clothing images (10K with both segmentation and fine-grained attributes, 40K with apparel instance segmentation) in daily-life, celebrity events, and online shopping are labeled by both domain experts and crowd workers for fine-grained segmentation.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Not found
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">License information not found</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://eurocity-dataset.tudelft.nl/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">EuroCity Persons Dataset</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        With over 238,200 person instances manually labeled in over 47,300 images, EuroCity Persons is nearly one order of magnitude larger than person datasets used previously for benchmarking. Diversity is gained by recording this dataset throughout Europe.

All objects were annotated with tight bounding boxes delineating their full extent. If objects were partly occluded, their full extents were estimated (this is useful for later processing steps such as tracking) and the level of occlusion was annotated.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="http://intelligent-vehicles.org/wp-content/uploads/2019/04/braun2019tpami_eurocity_persons.pdf" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://eurocity-dataset.tudelft.nl/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">EuroCity Persons Dataset</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          With over 238,200 person instances manually labeled in over 47,300 images, EuroCity Persons is nearly one order of magnitude larger than person datasets used previously for benchmarking. Diversity is gained by recording this dataset throughout Europe.

All objects were annotated with tight bounding boxes delineating their full extent. If objects were partly occluded, their full extents were estimated (this is useful for later processing steps such as tracking) and the level of occlusion was annotated.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-music text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">Audio</div></div> <div class="truncate"><a href="https://voice.mozilla.org/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Mozilla Common Voice</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        Mozilla crowdsources the largest dataset of human voices available for use, including 18 different languages, adding up to almost 1,400 hours of recorded voice data from more than 42,000 contributors.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      CC-0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">CC-0 - No Copyright</div></td> <td class="p-4 text-right hidden lg:table-cell"><i class="text-grey-light far fa-file"></i></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-music text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://voice.mozilla.org/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Mozilla Common Voice</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          Mozilla crowdsources the largest dataset of human voices available for use, including 18 different languages, adding up to almost 1,400 hours of recorded voice data from more than 42,000 contributors.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          CC-0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">CC-0 - No Copyright</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://www.research.ibm.com/artificial-intelligence/trusted-ai/diversity-in-faces/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">IBM Diversity in Faces Dataset</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        The Diversity in Faces(DiF)is a large and diverse dataset that seeks to advance the study of fairness and accuracy in facial recognition technology. The first of its kind available to the global research community, DiF provides a dataset of annotations of 1 million human facial images.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1901.10436" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://www.research.ibm.com/artificial-intelligence/trusted-ai/diversity-in-faces/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">IBM Diversity in Faces Dataset</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          The Diversity in Faces(DiF)is a large and diverse dataset that seeks to advance the study of fairness and accuracy in facial recognition technology. The first of its kind available to the global research community, DiF provides a dataset of annotations of 1 million human facial images.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-question-circle text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">QA</div></div> <div class="truncate"><a href="https://ai.google.com/research/NaturalQuestions" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Google Natural Questions</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        Natural Questions (NQ), a new, large-scale corpus for training and evaluating open-domain question answering systems, and the first to replicate the end-to-end process in which people find answers to questions. NQ is large, consisting of 300,000 naturally occurring questions, along with human annotated answers from Wikipedia pages, to be used in training QA systems. We have additionally included 16,000 examples where answers (to the same questions) are provided by 5 different annotators, useful for evaluating the performance of the learned QA systems.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      CC-BY-SA 3.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution-ShareAlike International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit,
ShareAlike - if you make changes, you must distribute your contributions.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://ai.google/research/pubs/pub47761.pdf" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-question-circle text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://ai.google.com/research/NaturalQuestions" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Google Natural Questions</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          Natural Questions (NQ), a new, large-scale corpus for training and evaluating open-domain question answering systems, and the first to replicate the end-to-end process in which people find answers to questions. NQ is large, consisting of 300,000 naturally occurring questions, along with human annotated answers from Wikipedia pages, to be used in training QA systems. We have additionally included 16,000 examples where answers (to the same questions) are provided by 5 different annotators, useful for evaluating the performance of the learned QA systems.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          CC-BY-SA 3.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution-ShareAlike International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit,
ShareAlike - if you make changes, you must distribute your contributions.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-align-left text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">NLP</div></div> <div class="truncate"><a href="https://github.com/brightmart/nlp_chinese_corpus" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Large Scale Chinese Corpus for NLP</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        Dataset contents: 1. Wikipedia (wiki2019zh), 1 million well-formed Chinese entries
2. News corpus (news2016zh), 2.5 million news, including keywords, description
3. Encyclopedia question and answer (baike2018qa), 1.5 million questions and answers with question types
4. Community Q&amp;A json version (webtext2019zh), 4.1 million high quality community Q&amp;A, suitable for training oversized models
5. Translation corpus (translation2019zh), 5.2 million pairs of Chinese and English sentences
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Various
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">The dataset contains data from several sources, check the links on the website for individual licenses</div></td> <td class="p-4 text-right hidden lg:table-cell"><i class="text-grey-light far fa-file"></i></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-align-left text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://github.com/brightmart/nlp_chinese_corpus" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Large Scale Chinese Corpus for NLP</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          Dataset contents: 1. Wikipedia (wiki2019zh), 1 million well-formed Chinese entries
2. News corpus (news2016zh), 2.5 million news, including keywords, description
3. Encyclopedia question and answer (baike2018qa), 1.5 million questions and answers with question types
4. Community Q&amp;A json version (webtext2019zh), 4.1 million high quality community Q&amp;A, suitable for training oversized models
5. Translation corpus (translation2019zh), 5.2 million pairs of Chinese and English sentences
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Various
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">The dataset contains data from several sources, check the links on the website for individual licenses</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-question-circle text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">QA</div></div> <div class="truncate"><a href="https://github.com/MILVLG/activitynet-qa" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">ActivityNet-QA</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        The ActivityNet-QA dataset contains 58,000 human-annotated QA pairs on 5,800 videos derived from the popular ActivityNet dataset. The dataset provides a benckmark for testing the performance of VideoQA models on long-term spatio-temporal reasoning.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      MIT
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">MIT - You are free to: use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the work. Under the following terms: the work is provided "as is", you must include copyright and the license in all copies or substantial uses of the work.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1906.02467" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-question-circle text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://github.com/MILVLG/activitynet-qa" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">ActivityNet-QA</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          The ActivityNet-QA dataset contains 58,000 human-annotated QA pairs on 5,800 videos derived from the popular ActivityNet dataset. The dataset provides a benckmark for testing the performance of VideoQA models on long-term spatio-temporal reasoning.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          MIT
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">MIT - You are free to: use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the work. Under the following terms: the work is provided "as is", you must include copyright and the license in all copies or substantial uses of the work.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-align-left text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">NLP</div></div> <div class="truncate"><a href="https://tblock.github.io/10kGNAD/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Ten Thousand German News Articles Dataset</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        The 10kGNAD dataset is intended to solve part of this problem as the first german topic classification dataset. It consists of 10273 german language news articles from an austrian online newspaper categorized into nine topics. These articles are a till now unused part of the One Million Posts Corpus.


      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      CC-BY-NC-SA 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution-NonCommercial-ShareAlike International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon,
Under the following terms:
Attribution - you must give approprate credit,
NonCommercial - you may not use the material for commercial purposes,
ShareAlike - if you make changes, you must distribute your contributions.</div></td> <td class="p-4 text-right hidden lg:table-cell"><i class="text-grey-light far fa-file"></i></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-align-left text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://tblock.github.io/10kGNAD/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Ten Thousand German News Articles Dataset</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          The 10kGNAD dataset is intended to solve part of this problem as the first german topic classification dataset. It consists of 10273 german language news articles from an austrian online newspaper categorized into nine topics. These articles are a till now unused part of the One Million Posts Corpus.


        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          CC-BY-NC-SA 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution-NonCommercial-ShareAlike International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon,
Under the following terms:
Attribution - you must give approprate credit,
NonCommercial - you may not use the material for commercial purposes,
ShareAlike - if you make changes, you must distribute your contributions.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="http://hexianghu.com/bison/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Facebook BISON</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        Facebook BISON (Binary Image Selection) dataset complements the COCO Captions dataset. BISON-COCO is not a training dataset, but rather an evaluation dataset that can be used to test existing models’ ability for pairing visual content with appropriate text descriptions.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Not found
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">License information not found</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1901.06595" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://hexianghu.com/bison/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Facebook BISON</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          Facebook BISON (Binary Image Selection) dataset complements the COCO Captions dataset. BISON-COCO is not a training dataset, but rather an evaluation dataset that can be used to test existing models’ ability for pairing visual content with appropriate text descriptions.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Not found
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">License information not found</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-plus-square text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 60px; margin-left: -16px;">Medical</div></div> <div class="truncate"><a href="https://www.physionet.org/physiobank/database/mimiccxr/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">MIMIC-CXR</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        MIMIC-CXR is a large, publicly-available database comprising of de-identified chest radiographs from patients admitted to the Beth Israel Deaconess Medical Center between 2011 and 2016. The dataset contains 371,920 chest x-rays associated with 227,943 imaging studies. Each imaging study can pertain to one or more images, but most often are associated with two images: a frontal view and a lateral view. Images are provided with 14 labels derived from a natural language processing tool applied to the corresponding free-text radiology reports. 
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1901.07042" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-plus-square text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://www.physionet.org/physiobank/database/mimiccxr/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">MIMIC-CXR</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          MIMIC-CXR is a large, publicly-available database comprising of de-identified chest radiographs from patients admitted to the Beth Israel Deaconess Medical Center between 2011 and 2016. The dataset contains 371,920 chest x-rays associated with 227,943 imaging studies. Each imaging study can pertain to one or more images, but most often are associated with two images: a frontal view and a lateral view. Images are provided with 14 labels derived from a natural language processing tool applied to the corresponding free-text radiology reports. 
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-plus-square text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 60px; margin-left: -16px;">Medical</div></div> <div class="truncate"><a href="https://stanfordmlgroup.github.io/competitions/chexpert/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">CheXpert</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        CheXpert is a large public dataset for chest radiograph interpretation, consisting of 224,316 chest radiographs of 65,240 patients.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1901.07031" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-plus-square text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://stanfordmlgroup.github.io/competitions/chexpert/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">CheXpert</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          CheXpert is a large public dataset for chest radiograph interpretation, consisting of 224,316 chest radiographs of 65,240 patients.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-question-circle text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">QA</div></div> <div class="truncate"><a href="https://cs.stanford.edu/people/dorarad/gqa/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">GQA</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        The dataset consists of 22M questions about various day-to-day images. Each image is associated with a scene graph of the image's objects, attributes and relations, a new cleaner version based on Visual Genome.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      CC BY 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution 4.0 International (CC BY 4.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://cs.stanford.edu/people/dorarad/gqa/gqaPaper.pdf" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-question-circle text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://cs.stanford.edu/people/dorarad/gqa/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">GQA</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          The dataset consists of 22M questions about various day-to-day images. Each image is associated with a scene graph of the image's objects, attributes and relations, a new cleaner version based on Visual Genome.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          CC BY 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution 4.0 International (CC BY 4.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://kelvins.esa.int/satellite-pose-estimation-challenge/home/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Spacecraft Pose Estimation Dataset (SPEED)</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        SPEED consists of synthetic as well as actual camera images of a mock-up of the Tango spacecraft from the PRISMA mission. The synthetic images are created by fusing OpenGL-based renderings of the spacecraft’s3D model with actual images of the Earth captured by the Himawari-8 meteorolog-ical satellite. Dataset contains over 12,000 images with a resolution of 1920×1200 pixels.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      CC-BY-NC-SA 3.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution-NonCommercial-ShareAlike International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon,
Under the following terms:
Attribution - you must give approprate credit,
NonCommercial - you may not use the material for commercial purposes,
ShareAlike - if you make changes, you must distribute your contributions.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://damicos.people.stanford.edu/sites/g/files/sbiybj2226/f/asm2019_sharmadamico_final.pdf" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://kelvins.esa.int/satellite-pose-estimation-challenge/home/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Spacecraft Pose Estimation Dataset (SPEED)</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          SPEED consists of synthetic as well as actual camera images of a mock-up of the Tango spacecraft from the PRISMA mission. The synthetic images are created by fusing OpenGL-based renderings of the spacecraft’s3D model with actual images of the Earth captured by the Himawari-8 meteorolog-ical satellite. Dataset contains over 12,000 images with a resolution of 1920×1200 pixels.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          CC-BY-NC-SA 3.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution-NonCommercial-ShareAlike International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon,
Under the following terms:
Attribution - you must give approprate credit,
NonCommercial - you may not use the material for commercial purposes,
ShareAlike - if you make changes, you must distribute your contributions.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://ai.baidu.com/broad/subordinate?dataset=lsvt" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Baidu Large-scale Street View Text with Partial Labeling (LSVT)</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        A new large-scale scene text dataset, namely Large-scale Street View Text with Partial Labeling (LSVT), with 30,000 training data and 20,000 testing images in full annotations, and 400,000 training data in weak annotations, which are referred to as partial labels.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><i class="text-grey-light far fa-file"></i></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://ai.baidu.com/broad/subordinate?dataset=lsvt" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Baidu Large-scale Street View Text with Partial Labeling (LSVT)</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          A new large-scale scene text dataset, namely Large-scale Street View Text with Partial Labeling (LSVT), with 30,000 training data and 20,000 testing images in full annotations, and 400,000 training data in weak annotations, which are referred to as partial labels.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://github.com/NVlabs/ffhq-dataset" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">NVIDIA Flickr-Faces-HQ Dataset</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        Flickr-Faces-HQ (FFHQ) is a high-quality image dataset of human faces, originally created as a benchmark for generative adversarial networks (GAN). The dataset consists of 70,000 high-quality PNG images at 1024×1024 resolution and contains considerable variation in terms of age, ethnicity and image background. It also has good coverage of accessories such as eyeglasses, sunglasses, hats, etc.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      CC-BY-NC-SA 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution-NonCommercial-ShareAlike International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon,
Under the following terms:
Attribution - you must give approprate credit,
NonCommercial - you may not use the material for commercial purposes,
ShareAlike - if you make changes, you must distribute your contributions.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1812.04948" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://github.com/NVlabs/ffhq-dataset" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">NVIDIA Flickr-Faces-HQ Dataset</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          Flickr-Faces-HQ (FFHQ) is a high-quality image dataset of human faces, originally created as a benchmark for generative adversarial networks (GAN). The dataset consists of 70,000 high-quality PNG images at 1024×1024 resolution and contains considerable variation in terms of age, ethnicity and image background. It also has good coverage of accessories such as eyeglasses, sunglasses, hats, etc.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          CC-BY-NC-SA 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution-NonCommercial-ShareAlike International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon,
Under the following terms:
Attribution - you must give approprate credit,
NonCommercial - you may not use the material for commercial purposes,
ShareAlike - if you make changes, you must distribute your contributions.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://www.gwern.net/Danbooru2018" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Danbooru2018</a></div> <div class="relative ml-2 hidden lg:block"><a href="https://www.gwern.net/Danbooru2018#torrent" target="_blank" class="HoverTooltip rotate-180 fas fa-magnet text-xs text-indigo-lighter hover:text-indigo no-underline"></a> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 120px; margin-left: -16px;">Download torrent</div></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        Danbooru2018 is a large-scale anime image database with 3.33m+ images annotated with 99.7m+ tags; It can be useful for machine learning purposes such as image recognition and generation.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Not found
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">License information not found</div></td> <td class="p-4 text-right hidden lg:table-cell"><i class="text-grey-light far fa-file"></i></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://www.gwern.net/Danbooru2018" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Danbooru2018</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          Danbooru2018 is a large-scale anime image database with 3.33m+ images annotated with 99.7m+ tags; It can be useful for machine learning purposes such as image recognition and generation.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Not found
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">License information not found</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://yingqianwang.github.io/Flickr1024/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Flickr1024</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2019</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        Flickr1024 is a large stereo dataset, which consists of 1024 high-quality images pairs and covers diverse senarios. This dataset can be employed for stereo image super-resolution (SR). 
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1903.06332" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://yingqianwang.github.io/Flickr1024/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Flickr1024</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          Flickr1024 is a large stereo dataset, which consists of 1024 high-quality images pairs and covers diverse senarios. This dataset can be employed for stereo image super-resolution (SR). 
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2019</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-question-circle text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">QA</div></div> <div class="truncate"><a href="https://quac.ai/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Question Answering in Context (QuAC)</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2018</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        QuAC, a dataset for Question Answering in Context that contains 14K information-seeking QA dialogs (100K questions in total). Question Answering in Context is a dataset for modeling, understanding, and participating in information seeking dialog. Data instances consist of an interactive dialog between two crowd workers: (1) a student who poses a sequence of freeform questions to learn as much as possible about a hidden Wikipedia text, and (2) a teacher who answers the questions by providing short excerpts (spans) from the text. QuAC introduces challenges not found in existing machine comprehension datasets: its questions are often more open-ended, unanswerable, or only meaningful within the dialog context.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      CC-BY-SA 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution-ShareAlike 4.0 International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit,
ShareAlike - if you make changes, you must distribute your contributions.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1808.07036" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-question-circle text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://quac.ai/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Question Answering in Context (QuAC)</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          QuAC, a dataset for Question Answering in Context that contains 14K information-seeking QA dialogs (100K questions in total). Question Answering in Context is a dataset for modeling, understanding, and participating in information seeking dialog. Data instances consist of an interactive dialog between two crowd workers: (1) a student who poses a sequence of freeform questions to learn as much as possible about a hidden Wikipedia text, and (2) a teacher who answers the questions by providing short excerpts (spans) from the text. QuAC introduces challenges not found in existing machine comprehension datasets: its questions are often more open-ended, unanswerable, or only meaningful within the dialog context.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          CC-BY-SA 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution-ShareAlike 4.0 International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit,
ShareAlike - if you make changes, you must distribute your contributions.</div></div> <div class="text-indigo-darker px-4 pb-4">2018</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="http://www.nlpr.ia.ac.cn/iva/homepage/jqwang/Vehicle1M.htm" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Vehicle-1M</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2018</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        The Vehicle-1M dataset is constructed by National Laboratory of Pattern Recognition, Institute of Automation, University of Chinese Academy of Sciences (NLPR, CASIA). This dataset involves vehicle images captured across day and night, from head or rear, by multiple surveillance cameras installed in several cities in China. There are totally 936,051 images from 55,527 vehicles and 400 vehicle models in the dataset. Each image is attached with a vehicle ID label denoting its identity in real world as well as a vehicle model label indicating the make, model and year of the vehicle(i.e. "Audi-A6-2013").
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16206/16270" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://www.nlpr.ia.ac.cn/iva/homepage/jqwang/Vehicle1M.htm" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Vehicle-1M</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          The Vehicle-1M dataset is constructed by National Laboratory of Pattern Recognition, Institute of Automation, University of Chinese Academy of Sciences (NLPR, CASIA). This dataset involves vehicle images captured across day and night, from head or rear, by multiple surveillance cameras installed in several cities in China. There are totally 936,051 images from 55,527 vehicles and 400 vehicle models in the dataset. Each image is attached with a vehicle ID label denoting its identity in real world as well as a vehicle model label indicating the make, model and year of the vehicle(i.e. "Audi-A6-2013").
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2018</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-align-left text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">NLP</div></div> <div class="truncate"><a href="https://ai.tencent.com/ailab/nlp/embedding.html" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Tencent AI Lab Embedding Corpus for Chinese Words and Phrases</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2018</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        This corpus provides 200-dimension vector representations, a.k.a. embeddings, for over 8 million Chinese words and phrases, which are pre-trained on large-scale high-quality data. These vectors, capturing semantic meanings for Chinese words and phrases, can be widely applied in many downstream Chinese processing tasks (e.g., named entity recognition and text classification) and in further research.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      CC BY 3.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution 3.0 International (CC BY 3.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="http://aclweb.org/anthology/N18-2028" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-align-left text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://ai.tencent.com/ailab/nlp/embedding.html" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Tencent AI Lab Embedding Corpus for Chinese Words and Phrases</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          This corpus provides 200-dimension vector representations, a.k.a. embeddings, for over 8 million Chinese words and phrases, which are pre-trained on large-scale high-quality data. These vectors, capturing semantic meanings for Chinese words and phrases, can be widely applied in many downstream Chinese processing tasks (e.g., named entity recognition and text classification) and in further research.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          CC BY 3.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution 3.0 International (CC BY 3.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></div> <div class="text-indigo-darker px-4 pb-4">2018</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="http://got-10k.aitestunion.com/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">GOT-10k (Generic Object Tracking Benchmark)</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2018</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        A large, high-diversity, one-shot database for generic object tracking in the wild. The dataset contains more than 10,000 video segments of real-world moving objects and over 1.5 million manually labeled bounding boxes. The dataset is backboned by WordNet and it covers a majority of 560+ classes of real-world moving objects and 80+ classes of motion patterns.The test set embodies 84 object classes and 32 motion classes with only 180 video segments, allowing for efficient evaluation.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Not found
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">License information not found</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1810.11981" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://got-10k.aitestunion.com/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">GOT-10k (Generic Object Tracking Benchmark)</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          A large, high-diversity, one-shot database for generic object tracking in the wild. The dataset contains more than 10,000 video segments of real-world moving objects and over 1.5 million manually labeled bounding boxes. The dataset is backboned by WordNet and it covers a majority of 560+ classes of real-world moving objects and 80+ classes of motion patterns.The test set embodies 84 object classes and 32 motion classes with only 180 video segments, allowing for efficient evaluation.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Not found
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">License information not found</div></div> <div class="text-indigo-darker px-4 pb-4">2018</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-question-circle text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">QA</div></div> <div class="truncate"><a href="https://github.com/allenai/OpenBookQA" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">OpenBookQA</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2018</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        OpenBookQA, modeled after open book exams for assessing human understanding of a subject. The open book that comes with our questions is a set of 1329 elementary level science facts. Roughly 6000 questions probe an understanding of these facts and their application to novel situations.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      Apache
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Apache License 2.0 - 

A permissive license whose main conditions require preservation of copyright and license notices. Contributors provide an express grant of patent rights. Licensed works, modifications, and larger works may be distributed under different terms and without source code.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1809.02789" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-question-circle text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://github.com/allenai/OpenBookQA" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">OpenBookQA</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          OpenBookQA, modeled after open book exams for assessing human understanding of a subject. The open book that comes with our questions is a set of 1329 elementary level science facts. Roughly 6000 questions probe an understanding of these facts and their application to novel situations.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          Apache
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Apache License 2.0 - 

A permissive license whose main conditions require preservation of copyright and license notices. Contributors provide an express grant of patent rights. Licensed works, modifications, and larger works may be distributed under different terms and without source code.</div></div> <div class="text-indigo-darker px-4 pb-4">2018</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://tracking-net.org/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">TrackingNet</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2018</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        A Large-Scale Dataset and Benchmark for Object Tracking in the Wild. &gt; 30K Video Sequences, &gt; 14M Bounding Boxes. Diversity ensured by Youtube.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Not found
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">License information not found</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://ivul.kaust.edu.sa/Documents/Publications/2018/TrackingNet%20A%20Large%20Scale%20Dataset%20and%20Benchmark%20for%20Object%20Tracking%20in%20the%20Wild.pdf" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://tracking-net.org/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">TrackingNet</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          A Large-Scale Dataset and Benchmark for Object Tracking in the Wild. &gt; 30K Video Sequences, &gt; 14M Bounding Boxes. Diversity ensured by Youtube.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Not found
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">License information not found</div></div> <div class="text-indigo-darker px-4 pb-4">2018</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-question-circle text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">QA</div></div> <div class="truncate"><a href="https://visualcommonsense.com/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">VCR (Visual Commonsense Reasoning)</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2018</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        Visual Commonsense Reasoning (VCR) is a new task and large-scale dataset for cognition-level visual understanding. It contains:
290k multiple choice questions
290k correct answers and rationales: one per question
110k images
Counterfactual choices obtained with minimal bias, via our new Adversarial Matching approach
Answers are 7.5 words on average; rationales are 16 words.
High human agreement (&gt;90%)
Scaffolded on top of 80 object categories from COCO
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1811.1083" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-question-circle text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://visualcommonsense.com/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">VCR (Visual Commonsense Reasoning)</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          Visual Commonsense Reasoning (VCR) is a new task and large-scale dataset for cognition-level visual understanding. It contains:
290k multiple choice questions
290k correct answers and rationales: one per question
110k images
Counterfactual choices obtained with minimal bias, via our new Adversarial Matching approach
Answers are 7.5 words on average; rationales are 16 words.
High human agreement (&gt;90%)
Scaffolded on top of 80 object categories from COCO
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2018</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://research.google.com/youtube8m/index.html" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Youtube-8M 2018</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2018</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        YouTube-8M is a large-scale labeled video dataset that consists of millions of YouTube video IDs and associated labels from a diverse vocabulary of 4700+ visual entities. It comes with precomputed state-of-the-art audio-visual features from billions of frames and audio segments, designed to fit on a single hard disk.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      CC BY 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution 4.0 International (CC BY 4.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1609.08675" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://research.google.com/youtube8m/index.html" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Youtube-8M 2018</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          YouTube-8M is a large-scale labeled video dataset that consists of millions of YouTube video IDs and associated labels from a diverse vocabulary of 4700+ visual entities. It comes with precomputed state-of-the-art audio-visual features from billions of frames and audio segments, designed to fit on a single hard disk.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          CC BY 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution 4.0 International (CC BY 4.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></div> <div class="text-indigo-darker px-4 pb-4">2018</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-align-left text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">NLP</div></div> <div class="truncate"><a href="http://multicomp.cs.cmu.edu/resources/cmu-mosei-dataset/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">CMU-MOSEI</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2018</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        CMU-MOSEI is the largest in-the-wild dataset of multimodal sentiment analysis and emotion recognition in NLP. It consists of 23,500 sentences from more than 1000 youtube identities and 200 topics. Sentences are annotated for sentiment and emotion intensity. The dataset also contains unsupervised data (unannotated sentences).
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://www.aclweb.org/anthology/P18-1208" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-align-left text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://multicomp.cs.cmu.edu/resources/cmu-mosei-dataset/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">CMU-MOSEI</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          CMU-MOSEI is the largest in-the-wild dataset of multimodal sentiment analysis and emotion recognition in NLP. It consists of 23,500 sentences from more than 1000 youtube identities and 200 topics. Sentences are annotated for sentiment and emotion intensity. The dataset also contains unsupervised data (unannotated sentences).
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2018</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-question-circle text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">QA</div></div> <div class="truncate"><a href="https://hucvl.github.io/recipeqa/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">RecipeQA</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2018</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        RecipeQA is a dataset for multimodal comprehension of cooking recipes. It consists of over 36K question-answer pairs automatically generated from approximately 20K unique recipes with step-by-step instructions and images. Each question in RecipeQA involves multiple modalities such as titles, descriptions or images, and working towards an answer requires (i) joint understanding of images and text, (ii) capturing the temporal flow of events, and (iii) making sense of procedural knowledge.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Various
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">RecipeQA contains question answer pairs generated from copyright free recipes found online under a variety of licences. The corresponding licence for each recipe is also provided in the dataset, see recipes.json.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1809.00812" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-question-circle text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://hucvl.github.io/recipeqa/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">RecipeQA</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          RecipeQA is a dataset for multimodal comprehension of cooking recipes. It consists of over 36K question-answer pairs automatically generated from approximately 20K unique recipes with step-by-step instructions and images. Each question in RecipeQA involves multiple modalities such as titles, descriptions or images, and working towards an answer requires (i) joint understanding of images and text, (ii) capturing the temporal flow of events, and (iii) making sense of procedural knowledge.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Various
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">RecipeQA contains question answer pairs generated from copyright free recipes found online under a variety of licences. The corresponding licence for each recipe is also provided in the dataset, see recipes.json.</div></div> <div class="text-indigo-darker px-4 pb-4">2018</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://ctwdataset.github.io/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Chinese Text in the Wild</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2018</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        A dataset of Chinese text with about 1 million Chinese characters annotated by experts in over 30 thousand street view images.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      CC BY-NC-SA 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution-NonCommercial-ShareAlike International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon,
Under the following terms:
Attribution - you must give approprate credit,
NonCommercial - you may not use the material for commercial purposes,
ShareAlike - if you make changes, you must distribute your contributions.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1803.00085" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://ctwdataset.github.io/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Chinese Text in the Wild</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          A dataset of Chinese text with about 1 million Chinese characters annotated by experts in over 30 thousand street view images.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          CC BY-NC-SA 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution-NonCommercial-ShareAlike International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon,
Under the following terms:
Attribution - you must give approprate credit,
NonCommercial - you may not use the material for commercial purposes,
ShareAlike - if you make changes, you must distribute your contributions.</div></div> <div class="text-indigo-darker px-4 pb-4">2018</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-align-left text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">NLP</div></div> <div class="truncate"><a href="https://summari.es/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">CORNELL NEWSROOM</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2018</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        CORNELL NEWSROOM is a large dataset for training and evaluating summarization systems. It contains 1.3 million articles and summaries written by authors and editors in the newsrooms of 38 major publications. The summaries are obtained from search and social metadata between 1998 and 2017 and use a variety of summarization strategies combining extraction and abstraction.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="http://aclweb.org/anthology/N18-1065" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-align-left text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://summari.es/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">CORNELL NEWSROOM</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          CORNELL NEWSROOM is a large dataset for training and evaluating summarization systems. It contains 1.3 million articles and summaries written by authors and editors in the newsrooms of 38 major publications. The summaries are obtained from search and social metadata between 1998 and 2017 and use a variety of summarization strategies combining extraction and abstraction.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2018</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-question-circle text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">QA</div></div> <div class="truncate"><a href="https://rajpurkar.github.io/SQuAD-explorer/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">The Stanford Question Answering Dataset (SQuAD) 2.0</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2018</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        Stanford Question Answering Dataset (SQuAD) is a new reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage. With 100,000+ question-answer pairs on 500+ articles, SQuAD is significantly larger than previous reading comprehension datasets. SQuAD2.0 combines the 100,000 questions in SQuAD1.1 with over 50,000 new, unanswerable questions written adversarially by crowdworkers to look similar to answerable ones.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      CC-BY-SA 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution-ShareAlike 4.0 International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit,
ShareAlike - if you make changes, you must distribute your contributions.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1806.03822" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-question-circle text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://rajpurkar.github.io/SQuAD-explorer/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">The Stanford Question Answering Dataset (SQuAD) 2.0</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          Stanford Question Answering Dataset (SQuAD) is a new reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage. With 100,000+ question-answer pairs on 500+ articles, SQuAD is significantly larger than previous reading comprehension datasets. SQuAD2.0 combines the 100,000 questions in SQuAD1.1 with over 50,000 new, unanswerable questions written adversarially by crowdworkers to look similar to answerable ones.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          CC-BY-SA 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution-ShareAlike 4.0 International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit,
ShareAlike - if you make changes, you must distribute your contributions.</div></div> <div class="text-indigo-darker px-4 pb-4">2018</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="http://multilingual-images.org/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">The Massively Multilingual Image Dataset (MMID)</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2018</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        MMID is a large-scale, massively multilingual dataset of images paired with the words they represent collected at the University of Pennsylvania. By far the largest dataset of its kind, it has 98 languages (including English) and up to 10,000 words per language! (and many more for English.)
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Not found
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">License information not found</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://aclweb.org/anthology/P18-1239" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://multilingual-images.org/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">The Massively Multilingual Image Dataset (MMID)</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          MMID is a large-scale, massively multilingual dataset of images paired with the words they represent collected at the University of Pennsylvania. By far the largest dataset of its kind, it has 98 languages (including English) and up to 10,000 words per language! (and many more for English.)
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Not found
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">License information not found</div></div> <div class="text-indigo-darker px-4 pb-4">2018</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-car text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 100px; margin-left: -16px;">Self-driving</div></div> <div class="truncate"><a href="http://bdd-data.berkeley.edu/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Berkeley Deep Drive (BDD100K)</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2018</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        The dataset contains over 100k videos of driving experience, each running 40 seconds at 30 frames per second. The total image count is 800 times larger than Baidu ApolloScape (released March 2018), 4,800 times larger than Mapillary and 8,000 times larger than KITTI.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1805.04687" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-car text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://bdd-data.berkeley.edu/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Berkeley Deep Drive (BDD100K)</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          The dataset contains over 100k videos of driving experience, each running 40 seconds at 30 frames per second. The total image count is 800 times larger than Baidu ApolloScape (released March 2018), 4,800 times larger than Mapillary and 8,000 times larger than KITTI.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2018</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-question-circle text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">QA</div></div> <div class="truncate"><a href="https://rowanzellers.com/swag/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">SWAG</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2018</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        Situations With Adversarial Generations is a large-scale dataset for this task of grounded commonsense inference, unifying natural language inference and physically grounded reasoning.
The dataset consists of 113k multiple choice questions about grounded situations. Each question is a video caption from LSMDC or ActivityNet Captions, with four answer choices about what might happen next in the scene. The correct answer is the (real) video caption for the next event in the video; the three incorrect answers are adversarially generated and human verified, so as to fool machines but not humans.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      MIT
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">MIT - You are free to: use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the work. Under the following terms: the work is provided "as is", you must include copyright and the license in all copies or substantial uses of the work.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1808.05326" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-question-circle text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://rowanzellers.com/swag/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">SWAG</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          Situations With Adversarial Generations is a large-scale dataset for this task of grounded commonsense inference, unifying natural language inference and physically grounded reasoning.
The dataset consists of 113k multiple choice questions about grounded situations. Each question is a video caption from LSMDC or ActivityNet Captions, with four answer choices about what might happen next in the scene. The correct answer is the (real) video caption for the next event in the video; the three incorrect answers are adversarially generated and human verified, so as to fool machines but not humans.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          MIT
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">MIT - You are free to: use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the work. Under the following terms: the work is provided "as is", you must include copyright and the license in all copies or substantial uses of the work.</div></div> <div class="text-indigo-darker px-4 pb-4">2018</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://www.highd-dataset.com/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">HighD - The Highway Drone Dataset</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2018</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        The highD dataset is a new dataset of naturalistic vehicle trajectories recorded on German highways. Using a drone, typical limitations of established traffic data collection methods such as occlusions are overcome by the aerial perspective. Traffic was recorded at six different locations and includes more than 110 500 vehicles. 
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      Non-commercial &amp; commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Non-commercial and commercial licenses available</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1810.05642" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://www.highd-dataset.com/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">HighD - The Highway Drone Dataset</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          The highD dataset is a new dataset of naturalistic vehicle trajectories recorded on German highways. Using a drone, typical limitations of established traffic data collection methods such as occlusions are overcome by the aerial perspective. Traffic was recorded at six different locations and includes more than 110 500 vehicles. 
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          Non-commercial &amp; commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Non-commercial and commercial licenses available</div></div> <div class="text-indigo-darker px-4 pb-4">2018</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-car text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 100px; margin-left: -16px;">Self-driving</div></div> <div class="truncate"><a href="https://github.com/commaai/comma2k19" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Comma 2k19</a></div> <div class="relative ml-2 hidden lg:block"><a href="https://academictorrents.com/details/65a2fbc964078aff62076ff4e103f18b951c5ddb" target="_blank" class="HoverTooltip rotate-180 fas fa-magnet text-xs text-indigo-lighter hover:text-indigo no-underline"></a> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 120px; margin-left: -16px;">Download torrent</div></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2018</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        comma.ai presents comma2k19, a dataset of over 33 hours of commute in California's 280 highway. This means 2019 segments, 1 minute long each, on a 20km section of highway driving between California's San Jose and San Francisco. comma2k19 is a fully reproducible and scalable dataset.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      MIT
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">MIT - You are free to: use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the work. Under the following terms: the work is provided "as is", you must include copyright and the license in all copies or substantial uses of the work.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1812.05752v1" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-car text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://github.com/commaai/comma2k19" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Comma 2k19</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          comma.ai presents comma2k19, a dataset of over 33 hours of commute in California's 280 highway. This means 2019 segments, 1 minute long each, on a 20km section of highway driving between California's San Jose and San Francisco. comma2k19 is a fully reproducible and scalable dataset.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          MIT
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">MIT - You are free to: use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the work. Under the following terms: the work is provided "as is", you must include copyright and the license in all copies or substantial uses of the work.</div></div> <div class="text-indigo-darker px-4 pb-4">2018</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="http://supervise.ly/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Supervisely Person</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2018</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        Dataset consists of 5,711 images with 6,884 high-quality annotated person instances. Can be found on Supervisaly.ai under “Datasets library”.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><i class="text-grey-light far fa-file"></i></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://supervise.ly/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Supervisely Person</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          Dataset consists of 5,711 images with 6,884 high-quality annotated person instances. Can be found on Supervisaly.ai under “Datasets library”.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2018</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-music text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">Audio</div></div> <div class="truncate"><a href="https://voices18.github.io/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">VOiCES</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2018</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        The Voices Obscured in Complex Environmental settings (VOiCES) corpus presents audio recorded in acoustically challenging conditions. Source Material: a total of 15 hours (3,903 audio files).
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      CC BY 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution 4.0 International (CC BY 4.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1804.05053" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-music text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://voices18.github.io/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">VOiCES</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          The Voices Obscured in Complex Environmental settings (VOiCES) corpus presents audio recorded in acoustically challenging conditions. Source Material: a total of 15 hours (3,903 audio files).
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          CC BY 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution 4.0 International (CC BY 4.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></div> <div class="text-indigo-darker px-4 pb-4">2018</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-align-left text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">NLP</div></div> <div class="truncate"><a href="https://github.com/mahnazkoupaee/WikiHow-Dataset" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">WikiHow-Dataset</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2018</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        WikiHow is a new large-scale dataset using the online WikiHow (http://www.wikihow.com/) knowledge base. Please refer to the paper for more information regarding the dataset and its properties. Each article consists of multiple paragraphs and each paragraph starts with a sentence summarizing it. By merging the paragraphs to form the article and the paragraph outlines to form the summary, the resulting version of the dataset contains more than 200,000 long-sequence pairs.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      CC-BY-NC-SA
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution-NonCommercial-ShareAlike International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon,
Under the following terms:
Attribution - you must give approprate credit,
NonCommercial - you may not use the material for commercial purposes,
ShareAlike - if you make changes, you must distribute your contributions.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1810.09305" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-align-left text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://github.com/mahnazkoupaee/WikiHow-Dataset" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">WikiHow-Dataset</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          WikiHow is a new large-scale dataset using the online WikiHow (http://www.wikihow.com/) knowledge base. Please refer to the paper for more information regarding the dataset and its properties. Each article consists of multiple paragraphs and each paragraph starts with a sentence summarizing it. By merging the paragraphs to form the article and the paragraph outlines to form the summary, the resulting version of the dataset contains more than 200,000 long-sequence pairs.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          CC-BY-NC-SA
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution-NonCommercial-ShareAlike International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon,
Under the following terms:
Attribution - you must give approprate credit,
NonCommercial - you may not use the material for commercial purposes,
ShareAlike - if you make changes, you must distribute your contributions.</div></div> <div class="text-indigo-darker px-4 pb-4">2018</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-car text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 100px; margin-left: -16px;">Self-driving</div></div> <div class="truncate"><a href="http://hci-benchmark.org/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">HD1K Benchmark Suite</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2018</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        An autonomous driving dataset and benchmark for optical flow. &gt; 1000 frames at 2560x1080 with diverse lighting and weather scenarios, reference data with error bars for optical flow, evaluation masks for dynamic objects, specific robustness evaluation on challenging scenes. The dataset includes:
110,500 vehicles
44,500 driven kilometers
147 driven hours
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="http://hci-benchmark.iwr.uni-heidelberg.de/media/publications//kondermann2016.pdf" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-car text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://hci-benchmark.org/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">HD1K Benchmark Suite</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          An autonomous driving dataset and benchmark for optical flow. &gt; 1000 frames at 2560x1080 with diverse lighting and weather scenarios, reference data with error bars for optical flow, evaluation masks for dynamic objects, specific robustness evaluation on challenging scenes. The dataset includes:
110,500 vehicles
44,500 driven kilometers
147 driven hours
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2018</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-question-circle text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">QA</div></div> <div class="truncate"><a href="http://www.visualqa.org/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">VQA Visual Question Answering</a></div> <div class="relative ml-2 hidden lg:block"><a href="https://academictorrents.com/details/f075ad12eccbbd665aec68db5d208dc68e7a384f" target="_blank" class="HoverTooltip rotate-180 fas fa-magnet text-xs text-indigo-lighter hover:text-indigo no-underline"></a> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 120px; margin-left: -16px;">Download torrent</div></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2018</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        VQA is a dataset containing open-ended questions about images. These questions require an understanding of vision and language. It contains 265,016 images (COCO and abstract scenes), at least 3 questions (5.4 questions on average) per image, 10 ground truth answers per question.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      CC BY 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution 4.0 International (CC BY 4.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1505.00468" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-question-circle text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://www.visualqa.org/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">VQA Visual Question Answering</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          VQA is a dataset containing open-ended questions about images. These questions require an understanding of vision and language. It contains 265,016 images (COCO and abstract scenes), at least 3 questions (5.4 questions on average) per image, 10 ground truth answers per question.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          CC BY 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution 4.0 International (CC BY 4.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></div> <div class="text-indigo-darker px-4 pb-4">2018</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="http://www.traffic-light-data.com/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">The DriveU Traffic Light Dataset (DTLD)</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2018</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        
DTLD contains more than 230 000 annotated traffic lights in camera images with a resolution of 2 megapixels. The dataset was recorded in 11 cities in Germany with a frequency of 15 Hz. Due to additional annotation attributes such as the traffic light pictogram, orientation or relevancy 344 unique classes exist. In addition to camera images and labels we provide stereo information in form of disparity images allowing stereo-based detection and depth-dependent evaluations.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Not found
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">License information not found</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://ieeexplore.ieee.org/document/8460737" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://www.traffic-light-data.com/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">The DriveU Traffic Light Dataset (DTLD)</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          
DTLD contains more than 230 000 annotated traffic lights in camera images with a resolution of 2 megapixels. The dataset was recorded in 11 cities in Germany with a frequency of 15 Hz. Due to additional annotation attributes such as the traffic light pictogram, orientation or relevancy 344 unique classes exist. In addition to camera images and labels we provide stereo information in form of disparity images allowing stereo-based detection and depth-dependent evaluations.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Not found
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">License information not found</div></div> <div class="text-indigo-darker px-4 pb-4">2018</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://medusa.fit.vutbr.cz/traffic/research-topics/fine-grained-vehicle-recognition/boxcars-improving-vehicle-fine-grained-recognition-using-3d-bounding-boxes-in-traffic-surveillance/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">BoxCars116K</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2018</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        A large fine-grained vehicle data set BoxCars116k, with 116k images of vehicles from various viewpoints taken by numerous surveillance cameras.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      CC BY-NC-SA 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution-NonCommercial-ShareAlike International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon,
Under the following terms:
Attribution - you must give approprate credit,
NonCommercial - you may not use the material for commercial purposes,
ShareAlike - if you make changes, you must distribute your contributions.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://ieeexplore.ieee.org/document/8307405" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://medusa.fit.vutbr.cz/traffic/research-topics/fine-grained-vehicle-recognition/boxcars-improving-vehicle-fine-grained-recognition-using-3d-bounding-boxes-in-traffic-surveillance/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">BoxCars116K</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          A large fine-grained vehicle data set BoxCars116k, with 116k images of vehicles from various viewpoints taken by numerous surveillance cameras.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          CC BY-NC-SA 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution-NonCommercial-ShareAlike International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon,
Under the following terms:
Attribution - you must give approprate credit,
NonCommercial - you may not use the material for commercial purposes,
ShareAlike - if you make changes, you must distribute your contributions.</div></div> <div class="text-indigo-darker px-4 pb-4">2018</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-align-left text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">NLP</div></div> <div class="truncate"><a href="https://www.nyu.edu/projects/bowman/multinli/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">MultiNLI</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2018</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        The Multi-Genre Natural Language Inference (MultiNLI) corpus is a crowd-sourced collection of 433k sentence pairs annotated with textual entailment information. The corpus is modeled on the SNLI corpus, but differs in that covers a range of genres of spoken and written text, and supports a distinctive cross-genre generalization evaluation.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Various
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">The majority of the
corpus is released under the OANC’s license,
which allows all content to be freely used, modified, and shared under permissive terms. The data
in the FICTION section falls under several permissive licenses; Seven Swords is available under
a Creative Commons Share-Alike 3.0 Unported
License, and with the explicit permission of the
author, Living History and Password Incorrect are
available under Creative Commons Attribution
3.0 Unported Licenses; the remaining works of
fiction are in the public domain in the United
States (but may be licensed differently elsewhere).</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1704.05426" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-align-left text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://www.nyu.edu/projects/bowman/multinli/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">MultiNLI</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          The Multi-Genre Natural Language Inference (MultiNLI) corpus is a crowd-sourced collection of 433k sentence pairs annotated with textual entailment information. The corpus is modeled on the SNLI corpus, but differs in that covers a range of genres of spoken and written text, and supports a distinctive cross-genre generalization evaluation.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Various
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">The majority of the
corpus is released under the OANC’s license,
which allows all content to be freely used, modified, and shared under permissive terms. The data
in the FICTION section falls under several permissive licenses; Seven Swords is available under
a Creative Commons Share-Alike 3.0 Unported
License, and with the explicit permission of the
author, Living History and Password Incorrect are
available under Creative Commons Attribution
3.0 Unported Licenses; the remaining works of
fiction are in the public domain in the United
States (but may be licensed differently elsewhere).</div></div> <div class="text-indigo-darker px-4 pb-4">2018</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-car text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 100px; margin-left: -16px;">Self-driving</div></div> <div class="truncate"><a href="http://apolloscape.auto/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">ApolloScape</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2018</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        ApolloScape is an order of magnitude bigger and more complex than existing similar datasets such as Kitti and CityScapes. ApolloScape offers 10 times more high-resolution images with pixel-by-pixel annotations, and includes 26 different recognizable objects such as cars, bicycles, pedestrians and buildings. The dataset offers several levels of scene complexity with increasing number of pedestrians and vehicles, up to 100 vehicles in a given scene, as well as a wider set of challenging environments such as heavy weather or extreme lighting conditions.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      Non-commercial &amp; commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Non-commercial and commercial licenses available</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1803.06184" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-car text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://apolloscape.auto/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">ApolloScape</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          ApolloScape is an order of magnitude bigger and more complex than existing similar datasets such as Kitti and CityScapes. ApolloScape offers 10 times more high-resolution images with pixel-by-pixel annotations, and includes 26 different recognizable objects such as cars, bicycles, pedestrians and buildings. The dataset offers several levels of scene complexity with increasing number of pedestrians and vehicles, up to 100 vehicles in a given scene, as well as a wider set of challenging environments such as heavy weather or extreme lighting conditions.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          Non-commercial &amp; commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Non-commercial and commercial licenses available</div></div> <div class="text-indigo-darker px-4 pb-4">2018</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-question-circle text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">QA</div></div> <div class="truncate"><a href="https://github.com/kushalkafle/DVQA_dataset" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">DVQA</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2018</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        DVQA: Understanding Data Visualizations via Question Answering, a dataset that
tests many aspects of bar chart understanding in a question answering framework. Contains over 3
million image-question pairs about bar charts. It tests
three forms of diagram understanding: a) structure understanding; b) data retrieval; and c) reasoning. 
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      CC BY-NC 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution-NonCommercial 4.0 International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon,
Under the following terms:
Attribution - you must give approprate credit,
NonCommercial - you may not use the material for commercial purposes.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1801.08163" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-question-circle text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://github.com/kushalkafle/DVQA_dataset" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">DVQA</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          DVQA: Understanding Data Visualizations via Question Answering, a dataset that
tests many aspects of bar chart understanding in a question answering framework. Contains over 3
million image-question pairs about bar charts. It tests
three forms of diagram understanding: a) structure understanding; b) data retrieval; and c) reasoning. 
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          CC BY-NC 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution-NonCommercial 4.0 International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon,
Under the following terms:
Attribution - you must give approprate credit,
NonCommercial - you may not use the material for commercial purposes.</div></div> <div class="text-indigo-darker px-4 pb-4">2018</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-car text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 100px; margin-left: -16px;">Self-driving</div></div> <div class="truncate"><a href="https://www.nuscenes.org/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">nuScenes</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2018</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        The nuScenes dataset is a large-scale autonomous driving dataset. It features:
● Full sensor suite (1x LIDAR, 5x RADAR, 6x camera, IMU, GPS)
● 1000 scenes of 20s each
● 1,440,000 camera images
● 400,000 lidar sweeps
● Two diverse cities: Boston and Singapore
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      CC BY-NC-SA 4.0 or commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution-NonCommercial-ShareAlike International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon,
Under the following terms:
Attribution - you must give approprate credit,
NonCommercial - you may not use the material for commercial purposes,
ShareAlike - if you make changes, you must distribute your contributions.</div></td> <td class="p-4 text-right hidden lg:table-cell"><i class="text-grey-light far fa-file"></i></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-car text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://www.nuscenes.org/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">nuScenes</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          The nuScenes dataset is a large-scale autonomous driving dataset. It features:
● Full sensor suite (1x LIDAR, 5x RADAR, 6x camera, IMU, GPS)
● 1000 scenes of 20s each
● 1,440,000 camera images
● 400,000 lidar sweeps
● Two diverse cities: Boston and Singapore
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          CC BY-NC-SA 4.0 or commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution-NonCommercial-ShareAlike International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon,
Under the following terms:
Attribution - you must give approprate credit,
NonCommercial - you may not use the material for commercial purposes,
ShareAlike - if you make changes, you must distribute your contributions.</div></div> <div class="text-indigo-darker px-4 pb-4">2018</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-plus-square text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 60px; margin-left: -16px;">Medical</div></div> <div class="truncate"><a href="https://stanfordmlgroup.github.io/competitions/mura/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">MURA</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2018</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        MURA (musculoskeletal radiographs) is a large dataset of bone X-rays that can be used to train algorithms tasked with detecting abnormalities in X-rays. MURA is believed to be the world’s largest public radiographic image dataset with 40,561 labeled images.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Stanford University School of Medicine MURA Dataset Research Use Agreement (see website for license)</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1712.06957" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-plus-square text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://stanfordmlgroup.github.io/competitions/mura/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">MURA</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          MURA (musculoskeletal radiographs) is a large dataset of bone X-rays that can be used to train algorithms tasked with detecting abnormalities in X-rays. MURA is believed to be the world’s largest public radiographic image dataset with 40,561 labeled images.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Stanford University School of Medicine MURA Dataset Research Use Agreement (see website for license)</div></div> <div class="text-indigo-darker px-4 pb-4">2018</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://bgshih.github.io/cocotext/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">COCO-Text</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2018</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        A Large-Scale Scene Text Dataset, Based on MSCOCO. COCO-Text V2.0 contains 63,686 images with 239,506 annotated text instances. Segmentation mask is annotated for every word, allowing fine-level detection. Three attributes are labeled for every word: machine-printed vs. handwritten, legible vs. illgible, and English vs. non-English.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      CC BY 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution 4.0 International (CC BY 4.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></td> <td class="p-4 text-right hidden lg:table-cell"><i class="text-grey-light far fa-file"></i></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://bgshih.github.io/cocotext/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">COCO-Text</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          A Large-Scale Scene Text Dataset, Based on MSCOCO. COCO-Text V2.0 contains 63,686 images with 239,506 annotated text instances. Segmentation mask is annotated for every word, allowing fine-level detection. Three attributes are labeled for every word: machine-printed vs. handwritten, legible vs. illgible, and English vs. non-English.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          CC BY 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution 4.0 International (CC BY 4.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></div> <div class="text-indigo-darker px-4 pb-4">2018</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://7dlabs.com/synscapes-overview" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Synscapes</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2018</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        A photorealistic synthetic dataset for street scene parsing. The images in the dataset do not follow a driven path through a single virtual world. Instead, an entirely unique scene was procedurally generated for each of the 25,000 images. As a result, the dataset contains a wide range of variations and unique combinations of features.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1810.08705" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://7dlabs.com/synscapes-overview" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Synscapes</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          A photorealistic synthetic dataset for street scene parsing. The images in the dataset do not follow a driven path through a single virtual world. Instead, an entirely unique scene was procedurally generated for each of the 25,000 images. As a result, the dataset contains a wide range of variations and unique combinations of features.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2018</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://xingangpan.github.io/projects/CULane.html" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">CULane</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2018</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        
CULane is a large scale challenging dataset for academic research on traffic lane detection. It is collected by cameras mounted on six different vehicles driven by different drivers in Beijing. More than 55 hours of videos were collected and 133,235 frames were extracted. Data examples are shown above. We have divided the dataset into 88880 for training set, 9675 for validation set, and 34680 for test set. The test set is divided into normal and 8 challenging categories, which correspond to the 9 examples above.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1712.0608" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://xingangpan.github.io/projects/CULane.html" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">CULane</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          
CULane is a large scale challenging dataset for academic research on traffic lane detection. It is collected by cameras mounted on six different vehicles driven by different drivers in Beijing. More than 55 hours of videos were collected and 133,235 frames were extracted. Data examples are shown above. We have divided the dataset into 88880 for training set, 9675 for validation set, and 34680 for test set. The test set is divided into normal and 8 challenging categories, which correspond to the 9 examples above.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2018</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-align-left text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">NLP</div></div> <div class="truncate"><a href="https://www.repository.cam.ac.uk/handle/1810/280608" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">MultiWOZ</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2018</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        The MultiWOZ dataset is a fully-labeled collection of human-human written conversations spanning over multiple domains and topics. At a size of 10k dialogues, it is at least one order of magnitude larger than all previous annotated task-oriented corpora. The dialogue are set between a tourist and a clerk in the information. It spans over 7 domains.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      CC BY 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution 4.0 International (CC BY 4.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1810.00278" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-align-left text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://www.repository.cam.ac.uk/handle/1810/280608" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">MultiWOZ</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          The MultiWOZ dataset is a fully-labeled collection of human-human written conversations spanning over multiple domains and topics. At a size of 10k dialogues, it is at least one order of magnitude larger than all previous annotated task-oriented corpora. The dialogue are set between a tourist and a clerk in the information. It spans over 7 domains.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          CC BY 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution 4.0 International (CC BY 4.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></div> <div class="text-indigo-darker px-4 pb-4">2018</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-question-circle text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">QA</div></div> <div class="truncate"><a href="https://stanfordnlp.github.io/coqa/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">CoQA</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2018</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        CoQA is a large-scale dataset for building Conversational Question Answering systems. CoQA contains 127k questions with answers, obtained from 8k conversations about text passages from seven diverse domains.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Various
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">CoQA contains passages from seven domains. We make five of these public under the following licenses:

Literature and Wikipedia passages are shared under CC BY-SA 4.0 license.
Children's stories are collected from MCTest which comes with MSR-LA license.
Middle/High school exam passages are collected from RACE which comes with its own license.
News passages are collected from the DeepMind CNN dataset which comes with Apache license.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1808.07042" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-question-circle text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://stanfordnlp.github.io/coqa/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">CoQA</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          CoQA is a large-scale dataset for building Conversational Question Answering systems. CoQA contains 127k questions with answers, obtained from 8k conversations about text passages from seven diverse domains.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Various
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">CoQA contains passages from seven domains. We make five of these public under the following licenses:

Literature and Wikipedia passages are shared under CC BY-SA 4.0 license.
Children's stories are collected from MCTest which comes with MSR-LA license.
Middle/High school exam passages are collected from RACE which comes with its own license.
News passages are collected from the DeepMind CNN dataset which comes with Apache license.</div></div> <div class="text-indigo-darker px-4 pb-4">2018</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-align-left text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">NLP</div></div> <div class="truncate"><a href="https://yale-lily.github.io/spider" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Spider 1.0</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2018</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        Spider is a large-scale complex and cross-domain semantic parsing and text-to-SQL dataset. Spider consists of 10,181 questions and 5,693 unique complex SQL queries on 200 databases with multiple tables covering 138 different domains.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      CC BY-SA 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution-ShareAlike 4.0 International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit,
ShareAlike - if you make changes, you must distribute your contributions.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1808.07042" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-align-left text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://yale-lily.github.io/spider" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Spider 1.0</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          Spider is a large-scale complex and cross-domain semantic parsing and text-to-SQL dataset. Spider consists of 10,181 questions and 5,693 unique complex SQL queries on 200 databases with multiple tables covering 138 different domains.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          CC BY-SA 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution-ShareAlike 4.0 International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit,
ShareAlike - if you make changes, you must distribute your contributions.</div></div> <div class="text-indigo-darker px-4 pb-4">2018</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-align-left text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">NLP</div></div> <div class="truncate"><a href="https://ai.google.com/research/ConceptualCaptions" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Google Conceptual Captions</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2018</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        We make available Conceptual Captions, a new dataset consisting of ~3.3M images annotated with captions. In contrast with the curated style of other image caption annotations, Conceptual Caption images and their raw descriptions are harvested from the web, and therefore represent a wider variety of styles. More precisely, the raw descriptions are harvested from the Alt-text HTML attribute associated with web images. To arrive at the current version of the captions, we have developed an automatic pipeline that extracts, filters, and transforms candidate image/caption pairs, with the goal of achieving a balance of cleanliness, informativeness, fluency, and learnability of the resulting captions.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Not found
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">License information not found</div></td> <td class="p-4 text-right hidden lg:table-cell"><i class="text-grey-light far fa-file"></i></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-align-left text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://ai.google.com/research/ConceptualCaptions" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Google Conceptual Captions</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          We make available Conceptual Captions, a new dataset consisting of ~3.3M images annotated with captions. In contrast with the curated style of other image caption annotations, Conceptual Caption images and their raw descriptions are harvested from the web, and therefore represent a wider variety of styles. More precisely, the raw descriptions are harvested from the Alt-text HTML attribute associated with web images. To arrive at the current version of the captions, we have developed an automatic pipeline that extracts, filters, and transforms candidate image/caption pairs, with the goal of achieving a balance of cleanliness, informativeness, fluency, and learnability of the resulting captions.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Not found
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">License information not found</div></div> <div class="text-indigo-darker px-4 pb-4">2018</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="http://densepose.org/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">DensePose</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2018</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        Dense human pose estimation aims at mapping all human pixels of an RGB image to the 3D surface of the human body. 

We introduce DensePose-COCO, a large-scale ground-truth dataset with image-to-surface correspondences manually annotated on 50K COCO images.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      CC BY-NC 2.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution-NonCommercial 2.0 International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon,
Under the following terms:
Attribution - you must give approprate credit,
NonCommercial - you may not use the material for commercial purposes.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1802.00434" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://densepose.org/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">DensePose</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          Dense human pose estimation aims at mapping all human pixels of an RGB image to the 3D surface of the human body. 

We introduce DensePose-COCO, a large-scale ground-truth dataset with image-to-surface correspondences manually annotated on 50K COCO images.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          CC BY-NC 2.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution-NonCommercial 2.0 International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon,
Under the following terms:
Attribution - you must give approprate credit,
NonCommercial - you may not use the material for commercial purposes.</div></div> <div class="text-indigo-darker px-4 pb-4">2018</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="http://imagelab.ing.unimore.it/dreyeve" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Dreyeve</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2018</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        Composed by 74 video sequences of 5 mins each, we have captured and annotated more than 500,000 frames. The labeling contains drivers’ gaze fixations and their temporal integration providing task-specific saliency maps. Geo-referenced locations, driving speed and course complete the set of released data.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1705.03854" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://imagelab.ing.unimore.it/dreyeve" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Dreyeve</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          Composed by 74 video sequences of 5 mins each, we have captured and annotated more than 500,000 frames. The labeling contains drivers’ gaze fixations and their temporal integration providing task-specific saliency maps. Geo-referenced locations, driving speed and course complete the set of released data.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2018</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-question-circle text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">QA</div></div> <div class="truncate"><a href="https://hotpotqa.github.io/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">HotpotQA</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2018</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        HotpotQA is a question answering dataset featuring natural, multi-hop questions, with strong supervision for supporting facts to enable more explainable question answering systems. The dataset is composed of 113,000 QA pairs based on Wikipedia.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      CC BY-SA 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution-ShareAlike 4.0 International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit,
ShareAlike - if you make changes, you must distribute your contributions.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1809.096" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-question-circle text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://hotpotqa.github.io/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">HotpotQA</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          HotpotQA is a question answering dataset featuring natural, multi-hop questions, with strong supervision for supporting facts to enable more explainable question answering systems. The dataset is composed of 113,000 QA pairs based on Wikipedia.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          CC BY-SA 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution-ShareAlike 4.0 International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit,
ShareAlike - if you make changes, you must distribute your contributions.</div></div> <div class="text-indigo-darker px-4 pb-4">2018</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://github.com/Tencent/tencent-ml-images" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Tencent ML — Images</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2018</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        Tencent ML — Images is the largest open-source multi-label image dataset, including 17,609,752 training and 88,739 validation image URLs which are annotated with up to 11,166 categories.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      CC BY 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution 4.0 International (CC BY 4.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1901.01703" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://github.com/Tencent/tencent-ml-images" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Tencent ML — Images</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          Tencent ML — Images is the largest open-source multi-label image dataset, including 17,609,752 training and 88,739 validation image URLs which are annotated with up to 11,166 categories.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          CC BY 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution 4.0 International (CC BY 4.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></div> <div class="text-indigo-darker px-4 pb-4">2018</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-plus-square text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 60px; margin-left: -16px;">Medical</div></div> <div class="truncate"><a href="http://fastmri.org/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">fastMRI Dataset</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2018</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        Acollaborative research project from Facebook AI Research (FAIR) and NYU Langone Health to investigate the use of AI to make MRI scans up to 10 times faster. The dataset includes more than 1.5 million anonymous MRI images of the knee, drawn from 10,000 scans, and raw measurement data from nearly 1,600 scans.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      MIT
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">MIT - You are free to: use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the work. Under the following terms: the work is provided "as is", you must include copyright and the license in all copies or substantial uses of the work.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1811.08839" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-plus-square text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://fastmri.org/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">fastMRI Dataset</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          Acollaborative research project from Facebook AI Research (FAIR) and NYU Langone Health to investigate the use of AI to make MRI scans up to 10 times faster. The dataset includes more than 1.5 million anonymous MRI images of the knee, drawn from 10,000 scans, and raw measurement data from nearly 1,600 scans.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          MIT
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">MIT - You are free to: use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the work. Under the following terms: the work is provided "as is", you must include copyright and the license in all copies or substantial uses of the work.</div></div> <div class="text-indigo-darker px-4 pb-4">2018</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-question-circle text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">QA</div></div> <div class="truncate"><a href="https://ai.baidu.com/broad/subordinate?dataset=dureader" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Baidu DuReader 2.0</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2018</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        DuReader 2.0 is a large-scale open-domain Chinese dataset for Machine Reading Comprehension (MRC) and Question Answering (QA). It contains more than 300K questions, 1.4M evident documents and corresponding human generated answers.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1711.05073" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-question-circle text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://ai.baidu.com/broad/subordinate?dataset=dureader" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Baidu DuReader 2.0</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          DuReader 2.0 is a large-scale open-domain Chinese dataset for Machine Reading Comprehension (MRC) and Question Answering (QA). It contains more than 300K questions, 1.4M evident documents and corresponding human generated answers.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2018</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://www.mapillary.com/dataset/vistas" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Mapillary Vistas</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2017</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        The Mapillary Vistas Dataset is the most diverse publicly available dataset of manually annotated training data for semantic segmentation of street scenes. 25,000 images pixel-accurately labeled into 152 object categories, 100 of those instance-specific. 
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      Research or commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Research and commercial licenses available.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://research.mapillary.com/img/publications/ICCV17a.pdf" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://www.mapillary.com/dataset/vistas" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Mapillary Vistas</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          The Mapillary Vistas Dataset is the most diverse publicly available dataset of manually annotated training data for semantic segmentation of street scenes. 25,000 images pixel-accurately labeled into 152 object categories, 100 of those instance-specific. 
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          Research or commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Research and commercial licenses available.</div></div> <div class="text-indigo-darker px-4 pb-4">2017</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="http://www.eecs.qmul.ac.uk/~hs308/WebLogo-2M.html/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">WebLogo-2M
</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2017</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        The WebLogo-2M dataset is a weakly labelled (at image level rather than object bounding box level) logo detection dataset. The dataset was constructed automatically by sampling the Twitter stream data. It contains 194 unique logo classes and over 2 million logo images.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1612.09322" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://www.eecs.qmul.ac.uk/~hs308/WebLogo-2M.html/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">WebLogo-2M
</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          The WebLogo-2M dataset is a weakly labelled (at image level rather than object bounding box level) logo detection dataset. The dataset was constructed automatically by sampling the Twitter stream data. It contains 194 unique logo classes and over 2 million logo images.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2017</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-music text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">Audio</div></div> <div class="truncate"><a href="https://github.com/mdeff/fma" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">FMA: A Dataset For Music Analysis</a></div> <div class="relative ml-2 hidden lg:block"><a href="https://academictorrents.com/details/dba20c45d4d6fa6453a4e99d2f8a4817893cfb94" target="_blank" class="HoverTooltip rotate-180 fas fa-magnet text-xs text-indigo-lighter hover:text-indigo no-underline"></a> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 120px; margin-left: -16px;">Download torrent</div></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2017</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        We introduce the Free Music Archive (FMA), an open and easily accessible dataset suitable for evaluating several tasks in MIR, a field concerned with browsing, searching, and organizing large music collections. The community's growing interest in feature and end-to-end learning is however restrained by the limited availability of large audio datasets. The FMA aims to overcome this hurdle by providing 917 GiB and 343 days of Creative Commons-licensed audio from 106,574 tracks from 16,341 artists and 14,854 albums, arranged in a hierarchical taxonomy of 161 genres. It provides full-length and high-quality audio, pre-computed features, together with track- and user-level metadata, tags, and free-form text such as biographies.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      CC BY 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution 4.0 International (CC BY 4.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1612.0184" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-music text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://github.com/mdeff/fma" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">FMA: A Dataset For Music Analysis</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          We introduce the Free Music Archive (FMA), an open and easily accessible dataset suitable for evaluating several tasks in MIR, a field concerned with browsing, searching, and organizing large music collections. The community's growing interest in feature and end-to-end learning is however restrained by the limited availability of large audio datasets. The FMA aims to overcome this hurdle by providing 917 GiB and 343 days of Creative Commons-licensed audio from 106,574 tracks from 16,341 artists and 14,854 albums, arranged in a hierarchical taxonomy of 161 genres. It provides full-length and high-quality audio, pre-computed features, together with track- and user-level metadata, tags, and free-form text such as biographies.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          CC BY 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution 4.0 International (CC BY 4.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></div> <div class="text-indigo-darker px-4 pb-4">2017</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-align-left text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">NLP</div></div> <div class="truncate"><a href="https://github.com/huyt16/Twitter100k" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Twitter100k</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2017</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        Twitter100k dataset is characterized by two aspects: 1) it has 100,000 image-text pairs randomly crawled from Twitter and thus has no constraint in the image categories; 2) text in Twitter100k is written in informal language by the users. 
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1703.06618" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-align-left text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://github.com/huyt16/Twitter100k" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Twitter100k</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          Twitter100k dataset is characterized by two aspects: 1) it has 100,000 image-text pairs randomly crawled from Twitter and thus has no constraint in the image categories; 2) text in Twitter100k is written in informal language by the users. 
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2017</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://www.citycam-cmu.com/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">CityCam</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2017</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        CITYCAM aims to understand the city by analyzing the vehicles. We collected and annotated 60,000 frames

with rich information, leading to about 900,000 annotated objects.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      CC BY 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution 4.0 International (CC BY 4.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1703.05868" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://www.citycam-cmu.com/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">CityCam</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          CITYCAM aims to understand the city by analyzing the vehicles. We collected and annotated 60,000 frames

with rich information, leading to about 900,000 annotated objects.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          CC BY 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution 4.0 International (CC BY 4.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></div> <div class="text-indigo-darker px-4 pb-4">2017</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://github.com/googlecreativelab/quickdraw-dataset" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">The Quick, Draw! Dataset</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2017</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        The Quick Draw Dataset is a collection of 50 million drawings across 345 categories, contributed by players of the game Quick, Draw!. The drawings were captured as timestamped vectors, tagged with metadata including what the player was asked to draw and in which country the player was located. You can browse the recognized drawings on quickdraw.withgoogle.com/data.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      CC BY 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution 4.0 International (CC BY 4.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></td> <td class="p-4 text-right hidden lg:table-cell"><i class="text-grey-light far fa-file"></i></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://github.com/googlecreativelab/quickdraw-dataset" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">The Quick, Draw! Dataset</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          The Quick Draw Dataset is a collection of 50 million drawings across 345 categories, contributed by players of the game Quick, Draw!. The drawings were captured as timestamped vectors, tagged with metadata including what the player was asked to draw and in which country the player was located. You can browse the recognized drawings on quickdraw.withgoogle.com/data.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          CC BY 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution 4.0 International (CC BY 4.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></div> <div class="text-indigo-darker px-4 pb-4">2017</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="http://vmmrdb.cecsresearch.org/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Vehicle Make and Model Recognition Dataset (VMMRdb)</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2017</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        The Vehicle Make and Model Recognition dataset (VMMRdb) is large in scale and diversity, containing 9,170 classes consisting of 291,752 images, covering models manufactured between 1950 to 2016. VMMRdb dataset contains images that were taken by different users, different imaging devices, and multiple view angles, ensuring a wide range of variations to account for various scenarios that could be encountered in a real-life scenario. The cars are not well aligned, and some images contain irrelevant background. The data was gathered by crawling web pages related to vehicle sales on craigslist.com, including 712 areas covering all 412 sub-domains corresponding to US metro areas.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Not found
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">License information not found</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="http://vmmrdb.cecsresearch.org/papers/VMMR_TSWC.pdf" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://vmmrdb.cecsresearch.org/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Vehicle Make and Model Recognition Dataset (VMMRdb)</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          The Vehicle Make and Model Recognition dataset (VMMRdb) is large in scale and diversity, containing 9,170 classes consisting of 291,752 images, covering models manufactured between 1950 to 2016. VMMRdb dataset contains images that were taken by different users, different imaging devices, and multiple view angles, ensuring a wide range of variations to account for various scenarios that could be encountered in a real-life scenario. The cars are not well aligned, and some images contain irrelevant background. The data was gathered by crawling web pages related to vehicle sales on craigslist.com, including 712 areas covering all 412 sub-domains corresponding to US metro areas.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Not found
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">License information not found</div></div> <div class="text-indigo-darker px-4 pb-4">2017</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="http://places2.csail.mit.edu/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Places2</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2017</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        Places contains more than 10 million images comprising 400+ unique scene categories. The dataset features 5000 to 30,000 training images per class, consistent with real-world frequencies of occurrence.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      CC BY 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution 4.0 International (CC BY 4.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1610.02055" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://places2.csail.mit.edu/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Places2</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          Places contains more than 10 million images comprising 400+ unique scene categories. The dataset features 5000 to 30,000 training images per class, consistent with real-world frequencies of occurrence.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          CC BY 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution 4.0 International (CC BY 4.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></div> <div class="text-indigo-darker px-4 pb-4">2017</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://susanqq.github.io/UTKFace/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">UTKFace</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2017</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        UTKFace dataset is a large-scale face dataset with long age span (range from 0 to 116 years old). The dataset consists of over 20,000 face images with annotations of age, gender, and ethnicity. The images cover large variation in pose, facial expression, illumination, occlusion, resolution, etc. This dataset could be used on a variety of tasks, e.g., face detection, age estimation, age progression/regression, landmark localization, etc.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1702.08423" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://susanqq.github.io/UTKFace/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">UTKFace</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          UTKFace dataset is a large-scale face dataset with long age span (range from 0 to 116 years old). The dataset consists of over 20,000 face images with annotations of age, gender, and ethnicity. The images cover large variation in pose, facial expression, illumination, occlusion, resolution, etc. This dataset could be used on a variety of tasks, e.g., face detection, age estimation, age progression/regression, landmark localization, etc.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2017</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-music text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">Audio</div></div> <div class="truncate"><a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">VoxCeleb</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2017</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        VoxCeleb is an audio-visual dataset consisting of short clips of human speech, extracted from interview videos uploaded to YouTube.
It contains data from 7,000+ speakers, 1 million+ utterances, 2,000+ hours. VoxCeleb consists of both audio and video. Each segment is at least 3 seconds long.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      CC BY-SA 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">"Attribution-ShareAlike 4.0 International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit,
ShareAlike - if you make changes, you must distribute your contributions."</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="http://www.robots.ox.ac.uk/~vgg/publications/2017/Nagrani17/nagrani17.pdf" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-music text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">VoxCeleb</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          VoxCeleb is an audio-visual dataset consisting of short clips of human speech, extracted from interview videos uploaded to YouTube.
It contains data from 7,000+ speakers, 1 million+ utterances, 2,000+ hours. VoxCeleb consists of both audio and video. Each segment is at least 3 seconds long.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          CC BY-SA 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">"Attribution-ShareAlike 4.0 International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit,
ShareAlike - if you make changes, you must distribute your contributions."</div></div> <div class="text-indigo-darker px-4 pb-4">2017</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://hci.iwr.uni-heidelberg.de/node/6132" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Bosch Small Traffic Lights Dataset</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2017</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        This dataset contains 13,427 camera images at a resolution of 1280x720 pixels and contains about 24,000 annotated traffic lights. The annotations include bounding boxes of traffic lights as well as the current state (active light) of each traffic light. 
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><i class="text-grey-light far fa-file"></i></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://hci.iwr.uni-heidelberg.de/node/6132" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Bosch Small Traffic Lights Dataset</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          This dataset contains 13,427 camera images at a resolution of 1280x720 pixels and contains about 24,000 annotated traffic lights. The annotations include bounding boxes of traffic lights as well as the current state (active light) of each traffic light. 
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2017</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://github.com/zalandoresearch/fashion-mnist" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Fashion MNIST</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2017</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        Fashion-MNIST is a dataset of Zalando's article images—consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      MIT
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">MIT - You are free to: use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the work. Under the following terms: the work is provided "as is", you must include copyright and the license in all copies or substantial uses of the work.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1708.07747" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://github.com/zalandoresearch/fashion-mnist" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Fashion MNIST</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          Fashion-MNIST is a dataset of Zalando's article images—consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          MIT
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">MIT - You are free to: use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the work. Under the following terms: the work is provided "as is", you must include copyright and the license in all copies or substantial uses of the work.</div></div> <div class="text-indigo-darker px-4 pb-4">2017</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-align-left text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">NLP</div></div> <div class="truncate"><a href="https://github.com/google-research-datasets/sentence-compression" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Google sentence compression</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2017</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        Large corpus of uncompressed and compressed sentences from news articles. Contains over 200,000 sentence compression pairs.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Not found
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">License information not found</div></td> <td class="p-4 text-right hidden lg:table-cell"><i class="text-grey-light far fa-file"></i></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-align-left text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://github.com/google-research-datasets/sentence-compression" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Google sentence compression</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          Large corpus of uncompressed and compressed sentences from news articles. Contains over 200,000 sentence compression pairs.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Not found
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">License information not found</div></div> <div class="text-indigo-darker px-4 pb-4">2017</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://research.google.com/youtube-bb/index.html" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Youtube-BoundingBoxes</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2017</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        YouTube-BoundingBoxes is a large-scale data set of video URLs with densely-sampled high-quality single-object bounding box annotations. The data set consists of approximately 380,000 15-20s video segments extracted from 240,000 different publicly visible YouTube videos, automatically selected to feature objects in natural settings without editing or post-processing, with a recording quality often akin to that of a hand-held cell phone camera.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      CC BY 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution 4.0 International (CC BY 4.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1702.00824" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://research.google.com/youtube-bb/index.html" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Youtube-BoundingBoxes</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          YouTube-BoundingBoxes is a large-scale data set of video URLs with densely-sampled high-quality single-object bounding box annotations. The data set consists of approximately 380,000 15-20s video segments extracted from 240,000 different publicly visible YouTube videos, automatically selected to feature objects in natural settings without editing or post-processing, with a recording quality often akin to that of a hand-held cell phone camera.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          CC BY 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution 4.0 International (CC BY 4.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></div> <div class="text-indigo-darker px-4 pb-4">2017</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-align-left text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">NLP</div></div> <div class="truncate"><a href="https://www.reddit.com/r/datasets/comments/65o7py/updated_reddit_comment_dataset_as_torrents/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Reddit comments</a></div> <div class="relative ml-2 hidden lg:block"><a href="http://academictorrents.com/details/85a5bd50e4c365f8df70240ffd4ecc7dec59912b" target="_blank" class="HoverTooltip rotate-180 fas fa-magnet text-xs text-indigo-lighter hover:text-indigo no-underline"></a> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 120px; margin-left: -16px;">Download torrent</div></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2017</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        Reddit Comments from 2005-12 to 2017-03. Downloaded from https://files.pushshift.io/comments.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><i class="text-grey-light far fa-file"></i></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-align-left text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://www.reddit.com/r/datasets/comments/65o7py/updated_reddit_comment_dataset_as_torrents/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Reddit comments</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          Reddit Comments from 2005-12 to 2017-03. Downloaded from https://files.pushshift.io/comments.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2017</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-question-circle text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">QA</div></div> <div class="truncate"><a href="https://github.com/deepmind/narrativeqa" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">NarrativeQA</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2017</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        NarrativeQA is a dataset built to encourage deeper comprehension of language. This dataset involves reasoning over reading entire books or movie scripts. This dataset contains approximately 45K question answer pairs in free form text. There are two modes of this dataset (1) reading comprehension over summaries and (2) reading comprehension over entire books/scripts.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      Apache
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Apache License 2.0 - 

A permissive license whose main conditions require preservation of copyright and license notices. Contributors provide an express grant of patent rights. Licensed works, modifications, and larger works may be distributed under different terms and without source code.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1712.0704" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-question-circle text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://github.com/deepmind/narrativeqa" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">NarrativeQA</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          NarrativeQA is a dataset built to encourage deeper comprehension of language. This dataset involves reasoning over reading entire books or movie scripts. This dataset contains approximately 45K question answer pairs in free form text. There are two modes of this dataset (1) reading comprehension over summaries and (2) reading comprehension over entire books/scripts.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          Apache
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Apache License 2.0 - 

A permissive license whose main conditions require preservation of copyright and license notices. Contributors provide an express grant of patent rights. Licensed works, modifications, and larger works may be distributed under different terms and without source code.</div></div> <div class="text-indigo-darker px-4 pb-4">2017</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="http://www.scan-net.org/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">ScanNet</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2017</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        ScanNet is an RGB-D video dataset containing 2.5 million views in more than 1500 scans, annotated with 3D camera poses, surface reconstructions, and instance-level semantic segmentations. To collect this data, we designed an easy-to-use and scalable RGB-D capture system that includes automated surface reconstruction and crowdsourced semantic annotation. 
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1702.04405" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://www.scan-net.org/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">ScanNet</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          ScanNet is an RGB-D video dataset containing 2.5 million views in more than 1500 scans, annotated with 3D camera poses, surface reconstructions, and instance-level semantic segmentations. To collect this data, we designed an easy-to-use and scalable RGB-D capture system that includes automated surface reconstruction and crowdsourced semantic annotation. 
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2017</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-music text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">Audio</div></div> <div class="truncate"><a href="https://magenta.tensorflow.org/datasets/nsynth" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">NSynth</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2017</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        A large-scale and high-quality dataset of annotated musical notes. The NSynth Dataset is an audio dataset containing ~300k musical notes, each with a unique pitch, timbre, and envelope. Each note is annotated with three additional pieces of information based on a combination of human evaluation and heuristic algorithms: the method of sound production for the note's instrument, the high-level family of which the note's instrument is a member and sonic qualities of the note.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      CC BY 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution 4.0 International (CC BY 4.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1704.01279" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-music text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://magenta.tensorflow.org/datasets/nsynth" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">NSynth</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          A large-scale and high-quality dataset of annotated musical notes. The NSynth Dataset is an audio dataset containing ~300k musical notes, each with a unique pitch, timbre, and envelope. Each note is annotated with three additional pieces of information based on a combination of human evaluation and heuristic algorithms: the method of sound production for the note's instrument, the high-level family of which the note's instrument is a member and sonic qualities of the note.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          CC BY 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution 4.0 International (CC BY 4.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></div> <div class="text-indigo-darker px-4 pb-4">2017</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="http://groups.csail.mit.edu/vision/datasets/ADE20K/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">ADE20K</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2017</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        A dataset for scene parsing. There are 20,210 images in the training set, 2,000 images
in the validation set, and 3,000 images in the testing set. All
the images are exhaustively annotated with objects. Many
objects are also annotated with their parts. For each object
there is additional information about whether it is occluded
or cropped, and other attributes.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Not found
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">License information not found</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1608.05442" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://groups.csail.mit.edu/vision/datasets/ADE20K/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">ADE20K</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          A dataset for scene parsing. There are 20,210 images in the training set, 2,000 images
in the validation set, and 3,000 images in the testing set. All
the images are exhaustively annotated with objects. Many
objects are also annotated with their parts. For each object
there is additional information about whether it is occluded
or cropped, and other attributes.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Not found
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">License information not found</div></div> <div class="text-indigo-darker px-4 pb-4">2017</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-question-circle text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">QA</div></div> <div class="truncate"><a href="https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Question Pairs (Quora)</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2017</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        A dataset of questions from Quora aimed at determining if pairs of question text actually correspond to semantically equivalent queries. Over 400,000 lines of potential question duplicate pairs. 
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><i class="text-grey-light far fa-file"></i></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-question-circle text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Question Pairs (Quora)</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          A dataset of questions from Quora aimed at determining if pairs of question text actually correspond to semantically equivalent queries. Over 400,000 lines of potential question duplicate pairs. 
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2017</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-align-left text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">NLP</div></div> <div class="truncate"><a href="https://www.yelp.com/dataset" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Yelp open dataset</a></div> <div class="relative ml-2 hidden lg:block"><a href="https://academictorrents.com/details/66ab083bda0c508de6c641baabb1ec17f72dc480" target="_blank" class="HoverTooltip rotate-180 fas fa-magnet text-xs text-indigo-lighter hover:text-indigo no-underline"></a> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 120px; margin-left: -16px;">Download torrent</div></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2017</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        The Yelp dataset contains data about businesses, reviews, and user data for use in personal, educational, and academic purposes. Available in both JSON and SQL files.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><i class="text-grey-light far fa-file"></i></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-align-left text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://www.yelp.com/dataset" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Yelp open dataset</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          The Yelp dataset contains data about businesses, reviews, and user data for use in personal, educational, and academic purposes. Available in both JSON and SQL files.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2017</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-music text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">Audio</div></div> <div class="truncate"><a href="https://research.google.com/audioset/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Google Audioset</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2017</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        AudioSet consists of an expanding ontology of 632 audio event classes and a collection of 2,084,320 human-labeled 10-second sound clips drawn from YouTube videos. The ontology is specified as a hierarchical graph of event categories, covering a wide range of human and animal sounds, musical instruments and genres, and common everyday environmental sounds.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      CC-BY 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution 4.0 International (CC BY 4.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://research.google.com/pubs/pub45857.html" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-music text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://research.google.com/audioset/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Google Audioset</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          AudioSet consists of an expanding ontology of 632 audio event classes and a collection of 2,084,320 human-labeled 10-second sound clips drawn from YouTube videos. The ontology is specified as a hierarchical graph of event categories, covering a wide range of human and animal sounds, musical instruments and genres, and common everyday environmental sounds.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          CC-BY 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution 4.0 International (CC BY 4.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></div> <div class="text-indigo-darker px-4 pb-4">2017</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://zenodo.org/record/259444" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">HASY
</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2017</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        HASY is a publicly available, free of charge dataset of single symbols similar to MNIST. It contains 168233 instances of 369 classes. 
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Not found
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">License information not found</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1701.0838" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://zenodo.org/record/259444" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">HASY
</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          HASY is a publicly available, free of charge dataset of single symbols similar to MNIST. It contains 168233 instances of 369 classes. 
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Not found
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">License information not found</div></div> <div class="text-indigo-darker px-4 pb-4">2017</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://www.eth3d.net/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">ETH3D</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2017</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        A multi-view stereo / 3D reconstruction benchmark covering a variety of indoor and outdoor scenes.
Ground truth geometry has been obtained using a high-precision laser scanner. Contains 13 / 12 DSLR datasets for training / testing,
5 / 5 multi-cam rig videos for training / testing,
27 / 20 frames for two-view stereo training / testing.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      CC BY-NC-SA 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution-NonCommercial-ShareAlike International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon,
Under the following terms:
Attribution - you must give approprate credit,
NonCommercial - you may not use the material for commercial purposes,
ShareAlike - if you make changes, you must distribute your contributions.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://www.eth3d.net/data/schoeps2017cvpr.pdf" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://www.eth3d.net/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">ETH3D</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          A multi-view stereo / 3D reconstruction benchmark covering a variety of indoor and outdoor scenes.
Ground truth geometry has been obtained using a high-precision laser scanner. Contains 13 / 12 DSLR datasets for training / testing,
5 / 5 multi-cam rig videos for training / testing,
27 / 20 frames for two-view stereo training / testing.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          CC BY-NC-SA 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution-NonCommercial-ShareAlike International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon,
Under the following terms:
Attribution - you must give approprate credit,
NonCommercial - you may not use the material for commercial purposes,
ShareAlike - if you make changes, you must distribute your contributions.</div></div> <div class="text-indigo-darker px-4 pb-4">2017</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-align-left text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">NLP</div></div> <div class="truncate"><a href="https://github.com/taivop/joke-dataset" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">A dataset of English plaintext jokes</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2017</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        There are about 208,000 jokes in this database scraped from three sources (reddit, stupidstuff.org, wocka.com).
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Various
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Parts of the dataset could be under different licenses, check the dataset web page for more information</div></td> <td class="p-4 text-right hidden lg:table-cell"><i class="text-grey-light far fa-file"></i></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-align-left text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://github.com/taivop/joke-dataset" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">A dataset of English plaintext jokes</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          There are about 208,000 jokes in this database scraped from three sources (reddit, stupidstuff.org, wocka.com).
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Various
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Parts of the dataset could be under different licenses, check the dataset web page for more information</div></div> <div class="text-indigo-darker px-4 pb-4">2017</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="http://www.wilddash.cc/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">WildDash</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2017</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        The main focus of this dataset is testing. It contains data recorded under real world driving situations. Aims of it are:
to compile and provide standard data which can be used for evaluation.
to establish accepted evaluation protocols, data and measures.
to boost the algorithm development on driving applications using computer vision techniques.
The WildDash dataset does not offer enough material to train algorithms by itself.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Oliver_Zendel_WildDash_-_Creating_ECCV_2018_paper.pdf" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://www.wilddash.cc/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">WildDash</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          The main focus of this dataset is testing. It contains data recorded under real world driving situations. Aims of it are:
to compile and provide standard data which can be used for evaluation.
to establish accepted evaluation protocols, data and measures.
to boost the algorithm development on driving applications using computer vision techniques.
The WildDash dataset does not offer enough material to train algorithms by itself.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2017</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-car text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 100px; margin-left: -16px;">Self-driving</div></div> <div class="truncate"><a href="http://robotcar-dataset.robots.ox.ac.uk/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Oxford RobotCar Dataset</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2017</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        The Oxford RobotCar Dataset contains over 100 repetitions of a consistent route through Oxford, UK, captured over a period of over a year. The dataset captures many different combinations of weather, traffic and pedestrians, along with longer term changes such as construction and roadworks.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      CC BY-NC-SA 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution-NonCommercial-ShareAlike International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon,
Under the following terms:
Attribution - you must give approprate credit,
NonCommercial - you may not use the material for commercial purposes,
ShareAlike - if you make changes, you must distribute your contributions.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://robotcar-dataset.robots.ox.ac.uk/images/robotcar_ijrr.pdf" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-car text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://robotcar-dataset.robots.ox.ac.uk/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Oxford RobotCar Dataset</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          The Oxford RobotCar Dataset contains over 100 repetitions of a consistent route through Oxford, UK, captured over a period of over a year. The dataset captures many different combinations of weather, traffic and pedestrians, along with longer term changes such as construction and roadworks.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          CC BY-NC-SA 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution-NonCommercial-ShareAlike International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon,
Under the following terms:
Attribution - you must give approprate credit,
NonCommercial - you may not use the material for commercial purposes,
ShareAlike - if you make changes, you must distribute your contributions.</div></div> <div class="text-indigo-darker px-4 pb-4">2017</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-align-left text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">NLP</div></div> <div class="truncate"><a href="https://research.fb.com/downloads/babi/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Facebook bAbI</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2017</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        A set of datasets for automatic text understanding and reasoning.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1502.05698" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-align-left text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://research.fb.com/downloads/babi/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Facebook bAbI</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          A set of datasets for automatic text understanding and reasoning.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2017</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="http://pic2recipe.csail.mit.edu/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Recipe1M</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2017</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        Recipe1M, a new large-scale, structured corpus of over one million cooking recipes and 13 million food images. As the largest publicly available collection of recipe data, Recipe1M affords the ability to train high-capacity models on aligned, multi-modal data. 
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Not found
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">License information not found</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1810.06553" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://pic2recipe.csail.mit.edu/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Recipe1M</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          Recipe1M, a new large-scale, structured corpus of over one million cooking recipes and 13 million food images. As the largest publicly available collection of recipe data, Recipe1M affords the ability to train high-capacity models on aligned, multi-modal data. 
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Not found
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">License information not found</div></div> <div class="text-indigo-darker px-4 pb-4">2017</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="http://cvgl.stanford.edu/projects/uav_data/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Stanford Drone Dataset</a></div> <div class="relative ml-2 hidden lg:block"><a href="https://academictorrents.com/details/01f95ea32e160e6c251ea55a87bd5a24b23cb03d" target="_blank" class="HoverTooltip rotate-180 fas fa-magnet text-xs text-indigo-lighter hover:text-indigo no-underline"></a> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 120px; margin-left: -16px;">Download torrent</div></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2016</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        A large scale dataset that collects images and videos of various types of agents (not just pedestrians, but also bicyclists, skateboarders, cars, buses, and golf carts) that navigate in a real world outdoor environment such as a university campus. In the above images, pedestrians are labeled in pink, bicyclists in red, skateboarders in orange, and cars in green. 60 videos of 8 distinct scenes.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      CC BY-NC-SA 3.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution-NonCommercial-ShareAlike International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon,
Under the following terms:
Attribution - you must give approprate credit,
NonCommercial - you may not use the material for commercial purposes,
ShareAlike - if you make changes, you must distribute your contributions.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="http://www.eccv2016.org/files/posters/P-3C-31.pdf" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://cvgl.stanford.edu/projects/uav_data/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Stanford Drone Dataset</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          A large scale dataset that collects images and videos of various types of agents (not just pedestrians, but also bicyclists, skateboarders, cars, buses, and golf carts) that navigate in a real world outdoor environment such as a university campus. In the above images, pedestrians are labeled in pink, bicyclists in red, skateboarders in orange, and cars in green. 60 videos of 8 distinct scenes.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          CC BY-NC-SA 3.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution-NonCommercial-ShareAlike International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon,
Under the following terms:
Attribution - you must give approprate credit,
NonCommercial - you may not use the material for commercial purposes,
ShareAlike - if you make changes, you must distribute your contributions.</div></div> <div class="text-indigo-darker px-4 pb-4">2016</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://cg.cs.tsinghua.edu.cn/traffic-sign/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Tsinghua-Tencent 100k (traffic signs)</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2016</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        It provides 100,000 images containing 30,000 traffic-sign instances. These images cover large variations in illuminance and weather conditions. Each traffic-sign in the benchmark is annotated with a class label, its bounding box and pixel mask.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Not found
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">License information not found</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://cg.cs.tsinghua.edu.cn/traffic-sign/0682.pdf" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://cg.cs.tsinghua.edu.cn/traffic-sign/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Tsinghua-Tencent 100k (traffic signs)</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          It provides 100,000 images containing 30,000 traffic-sign instances. These images cover large variations in illuminance and weather conditions. Each traffic-sign in the benchmark is annotated with a class label, its bounding box and pixel mask.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Not found
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">License information not found</div></div> <div class="text-indigo-darker px-4 pb-4">2016</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="http://megaface.cs.washington.edu/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">MegaFace</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2016</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        The MF2 training dataset is the largest (in number of identities) publicly available facial recognition dataset with a 4.7 million faces, 672K identities, and their respective bounding boxes. All images obtained from Flickr (Yahoo's dataset) and licensed under Creative Commons.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1505.02108" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://megaface.cs.washington.edu/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">MegaFace</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          The MF2 training dataset is the largest (in number of identities) publicly available facial recognition dataset with a 4.7 million faces, 672K identities, and their respective bounding boxes. All images obtained from Flickr (Yahoo's dataset) and licensed under Creative Commons.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2016</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://download.visinf.tu-darmstadt.de/data/from_games/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">GTA dataset</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2016</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        The datasets consists of 24,966 densely labelled frames split into 10 parts for convenience. The class labels are compatible with the CamVid and CityScapes datasets. 
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1608.02192" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://download.visinf.tu-darmstadt.de/data/from_games/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">GTA dataset</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          The datasets consists of 24,966 densely labelled frames split into 10 parts for convenience. The class labels are compatible with the CamVid and CityScapes datasets. 
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2016</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-plus-square text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 60px; margin-left: -16px;">Medical</div></div> <div class="truncate"><a href="https://mimic.physionet.org/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">MIMIC</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2016</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        MIMIC is an openly available dataset developed by the MIT Lab for Computational Physiology, comprising deidentified health data associated with ~40,000 critical care patients. It includes demographics, vital signs, laboratory tests, medications, and more. The latest version of MIMIC is MIMIC-III v1.4, which comprises over 58,000 hospital admissions for 38,645 adults and 7,875 neonates. The data spans June 2001 - October 2012. The database, although de-identified, still contains detailed information regarding the clinical care of patients, so must be treated with appropriate care and respect.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Not found
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">License information not found</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://www.nature.com/articles/sdata201635.pdf" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-plus-square text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://mimic.physionet.org/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">MIMIC</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          MIMIC is an openly available dataset developed by the MIT Lab for Computational Physiology, comprising deidentified health data associated with ~40,000 critical care patients. It includes demographics, vital signs, laboratory tests, medications, and more. The latest version of MIMIC is MIMIC-III v1.4, which comprises over 58,000 hospital admissions for 38,645 adults and 7,875 neonates. The data spans June 2001 - October 2012. The database, although de-identified, still contains detailed information regarding the clinical care of patients, so must be treated with appropriate care and respect.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Not found
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">License information not found</div></div> <div class="text-indigo-darker px-4 pb-4">2016</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://robotvault.bitbucket.io/scenenet-rgbd.html" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">SceneNet RGB-D</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2016</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
         It provides pixel-perfect ground truth for scene understanding problems such as semantic segmentation, instance segmentation, and object detection, and also for geometric computer vision problems such as optical flow, depth estimation, camera pose estimation, and 3D reconstruction. A set of 5M rendered RGB-D images from over 15K trajectories in synthetic layouts with random but physically simulated object poses.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      GPL
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">GPL - You are free to: copy, distribute and modify the software as long as you track changes/dates in source files. Under the following terms: any modifications to or software including (via compiler) GPL-licensed code must also be made available under the GPL along with build &amp; install instructions.
</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1612.05079" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://robotvault.bitbucket.io/scenenet-rgbd.html" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">SceneNet RGB-D</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
           It provides pixel-perfect ground truth for scene understanding problems such as semantic segmentation, instance segmentation, and object detection, and also for geometric computer vision problems such as optical flow, depth estimation, camera pose estimation, and 3D reconstruction. A set of 5M rendered RGB-D images from over 15K trajectories in synthetic layouts with random but physically simulated object poses.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          GPL
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">GPL - You are free to: copy, distribute and modify the software as long as you track changes/dates in source files. Under the following terms: any modifications to or software including (via compiler) GPL-licensed code must also be made available under the GPL along with build &amp; install instructions.
</div></div> <div class="text-indigo-darker px-4 pb-4">2016</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-align-left text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">NLP</div></div> <div class="truncate"><a href="http://www.msmarco.org/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">MS MARCO</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2016</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        Microsoft Machine Reading Comprehension (MS MARCO) is a new large scale dataset for reading comprehension and question answering. In MS MARCO, all questions are sampled from real anonymized user queries. The context passages, from which answers in the dataset are derived, are extracted from real web documents using the most advanced version of the Bing search engine. The answers to the queries are human generated if they could summarize the answer. It contains 1,010,916 user queries and 182,669 natural language answers.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1611.09268" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-align-left text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://www.msmarco.org/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">MS MARCO</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          Microsoft Machine Reading Comprehension (MS MARCO) is a new large scale dataset for reading comprehension and question answering. In MS MARCO, all questions are sampled from real anonymized user queries. The context passages, from which answers in the dataset are derived, are extracted from real web documents using the most advanced version of the Bing search engine. The answers to the queries are human generated if they could summarize the answer. It contains 1,010,916 user queries and 182,669 natural language answers.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2016</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="http://synthia-dataset.net/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">SYNTHIA</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2016</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        The  SYNTHetic collection of Imagery and Annotations, is a dataset that has been generated with the purpose of aiding semantic segmentation and related scene understanding problems in the context of driving scenarios. SYNTHIA consists of a collection of photo-realistic frames rendered from a virtual city and comes with precise pixel-level semantic annotations. It contains: +200,000 HD images from video streams and +20,000 HD images from independent snapshots. Scene diversity: European style town, modern city, highway and green areas. Variety of dynamic objects: cars, pedestrians and cyclists.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      CC BY-SA 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution-ShareAlike 4.0 International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit,
ShareAlike - if you make changes, you must distribute your contributions.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Ros_The_SYNTHIA_Dataset_CVPR_2016_paper.pdf" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://synthia-dataset.net/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">SYNTHIA</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          The  SYNTHetic collection of Imagery and Annotations, is a dataset that has been generated with the purpose of aiding semantic segmentation and related scene understanding problems in the context of driving scenarios. SYNTHIA consists of a collection of photo-realistic frames rendered from a virtual city and comes with precise pixel-level semantic annotations. It contains: +200,000 HD images from video streams and +20,000 HD images from independent snapshots. Scene diversity: European style town, modern city, highway and green areas. Variety of dynamic objects: cars, pedestrians and cyclists.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          CC BY-SA 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution-ShareAlike 4.0 International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit,
ShareAlike - if you make changes, you must distribute your contributions.</div></div> <div class="text-indigo-darker px-4 pb-4">2016</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-question-circle text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">QA</div></div> <div class="truncate"><a href="https://www.microsoft.com/en-us/research/project/newsqa-dataset" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">NewsQA</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2016</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        The purpose of the NewsQA dataset is to help the research community build algorithms that are capable of answering questions requiring human-level comprehension and reasoning skills. Leveraging CNN articles from the DeepMind Q&amp;A Dataset, we prepared a crowd-sourced machine reading comprehension dataset of 120K Q&amp;A pairs.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Various
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Parts of the dataset are under different licenses, check the dataset web page for more information</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1611.0983" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-question-circle text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://www.microsoft.com/en-us/research/project/newsqa-dataset" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">NewsQA</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          The purpose of the NewsQA dataset is to help the research community build algorithms that are capable of answering questions requiring human-level comprehension and reasoning skills. Leveraging CNN articles from the DeepMind Q&amp;A Dataset, we prepared a crowd-sourced machine reading comprehension dataset of 120K Q&amp;A pairs.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Various
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Parts of the dataset are under different licenses, check the dataset web page for more information</div></div> <div class="text-indigo-darker px-4 pb-4">2016</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="http://www.umdfaces.io/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">UMD Faces</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2016</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        The dataset contains 367,888 face annotations for 8,277 subjects divided into 3 batches. Contains bounding boxes, the extimated pose (yaw, pitch, and roll), locations of twenty-one keypoints, and gender information generated by a pre-trained neural network.
The second part contains 3,735,476 annotated video frames extracted from a total of 22,075 for 3,107 subjects.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1611.01484v2" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://www.umdfaces.io/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">UMD Faces</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          The dataset contains 367,888 face annotations for 8,277 subjects divided into 3 batches. Contains bounding boxes, the extimated pose (yaw, pitch, and roll), locations of twenty-one keypoints, and gender information generated by a pre-trained neural network.
The second part contains 3,735,476 annotated video frames extracted from a total of 22,075 for 3,107 subjects.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2016</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://github.com/commaai/research" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">comma.ai</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2016</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        7 and a quarter hours of largely highway driving.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1608.0123" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://github.com/commaai/research" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">comma.ai</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          7 and a quarter hours of largely highway driving.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2016</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="http://explore.digitalglobe.com/spacenet" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Spacenet</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2016</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        SpaceNet is an online repository of freely available satellite imagery, co-registered map data to train algorithms, and a series of public challenges designed to accelerate innovation in machine learning using geospatial data. This first of its kind open innovation project for the geospatial industry is a collaboration between CosmiQ Works, DigitalGlobe and NVIDIA. In the first year, over 5,700 km2 of very high-resolution imagery and more than 520,000 vectors were released through SpaceNet on AWS.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Various
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Parts of the dataset are under different licenses, check the dataset web page for more information</div></td> <td class="p-4 text-right hidden lg:table-cell"><i class="text-grey-light far fa-file"></i></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://explore.digitalglobe.com/spacenet" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Spacenet</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          SpaceNet is an online repository of freely available satellite imagery, co-registered map data to train algorithms, and a series of public challenges designed to accelerate innovation in machine learning using geospatial data. This first of its kind open innovation project for the geospatial industry is a collaboration between CosmiQ Works, DigitalGlobe and NVIDIA. In the first year, over 5,700 km2 of very high-resolution imagery and more than 520,000 vectors were released through SpaceNet on AWS.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Various
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Parts of the dataset are under different licenses, check the dataset web page for more information</div></div> <div class="text-indigo-darker px-4 pb-4">2016</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="http://mmlab.ie.cuhk.edu.hk/datasets/comp_cars/index.html" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">CompCars</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2015</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        The Comprehensive Cars (CompCars) dataset contains data from two scenarios, including images from web-nature and surveillance-nature. The web-nature data contains 163 car makes with 1,716 car models. There are a total of 136,726 images capturing the entire cars and 27,618 images capturing the car parts. The full car images are labeled with bounding boxes and viewpoints. Each car model is labeled with five attributes, including maximum speed, displacement, number of doors, number of seats, and type of car.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1506.08959" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://mmlab.ie.cuhk.edu.hk/datasets/comp_cars/index.html" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">CompCars</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          The Comprehensive Cars (CompCars) dataset contains data from two scenarios, including images from web-nature and surveillance-nature. The web-nature data contains 163 car makes with 1,716 car models. There are a total of 136,726 images capturing the entire cars and 27,618 images capturing the car parts. The full car images are labeled with bounding boxes and viewpoints. Each car model is labeled with five attributes, including maximum speed, displacement, number of doors, number of seats, and type of car.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2015</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://shapenet.org/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">ShapeNet</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2015</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        ShapeNet is an ongoing effort to establish a richly-annotated, large-scale dataset of 3D shapes. ShapeNet is organized according to the WordNet hierarchy. Each meaningful concept in WordNet, possibly described by multiple words or word phrases, is called a "synonym set" or "synset". There are more than 100,000 synsets in WordNet, the majority of them being nouns (80,000+).
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1512.03012" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://shapenet.org/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">ShapeNet</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          ShapeNet is an ongoing effort to establish a richly-annotated, large-scale dataset of 3D shapes. ShapeNet is organized according to the WordNet hierarchy. Each meaningful concept in WordNet, possibly described by multiple words or word phrases, is called a "synonym set" or "synset". There are more than 100,000 synsets in WordNet, the majority of them being nouns (80,000+).
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2015</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">WIDER Face</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2015</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        WIDER FACE dataset is a face detection benchmark dataset, of which images are selected from the publicly available WIDER dataset. We choose 32,203 images and label 393,703 faces with a high degree of variability in scale, pose and occlusion as depicted in the sample images. WIDER FACE dataset is organized based on 61 event classes.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1511.06523" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">WIDER Face</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          WIDER FACE dataset is a face detection benchmark dataset, of which images are selected from the publicly available WIDER dataset. We choose 32,203 images and label 393,703 faces with a high degree of variability in scale, pose and occlusion as depicted in the sample images. WIDER FACE dataset is organized based on 61 event classes.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2015</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="http://yjxiong.me/event_recog/WIDER/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">WIDER</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2015</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        WIDER is a dataset for complex event recognition from static images. As of v0.1, it contains 61 event categories and around 50574 images annotated with event class labels. We provide a split of 50% for training and 50% for testing.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="http://yjxiong.me/papers/cvpr15event_supp.pdf" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://yjxiong.me/event_recog/WIDER/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">WIDER</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          WIDER is a dataset for complex event recognition from static images. As of v0.1, it contains 61 event categories and around 50574 images annotated with event class labels. We provide a split of 50% for training and 50% for testing.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2015</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="http://www.yf.io/p/lsun" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">LSUN</a></div> <div class="relative ml-2 hidden lg:block"><a href="https://academictorrents.com/details/c53c374bd6de76da7fe76ed5c9e3c7c6c691c489" target="_blank" class="HoverTooltip rotate-180 fas fa-magnet text-xs text-indigo-lighter hover:text-indigo no-underline"></a> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 120px; margin-left: -16px;">Download torrent</div></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2015</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        LSUN contains around one million labeled images for each of 10 scene categories and 20 object categories.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Not found
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">License information not found</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1506.03365" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://www.yf.io/p/lsun" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">LSUN</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          LSUN contains around one million labeled images for each of 10 scene categories and 20 object categories.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Not found
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">License information not found</div></div> <div class="text-indigo-darker px-4 pb-4">2015</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">CelebA</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2015</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        CelebFaces Attributes Dataset (CelebA) is a large-scale face attributes dataset with more than 200K celebrity images, each with 40 attribute annotations. The images in this dataset cover large pose variations and background clutter. CelebA has large diversities, large quantities, and rich annotations.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1411.7766" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">CelebA</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          CelebFaces Attributes Dataset (CelebA) is a large-scale face attributes dataset with more than 200K celebrity images, each with 40 attribute annotations. The images in this dataset cover large pose variations and background clutter. CelebA has large diversities, large quantities, and rich annotations.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2015</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="http://visualgenome.org/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Visual Genome</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2015</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        Visual Genome is a dataset, a knowledge base, an ongoing effort to connect structured image concepts to language. It contains: 108,077 Images
5.4 Million Region Descriptions
1.7 Million Visual Question Answers
3.8 Million Object Instances
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      CC BY 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution 4.0 International (CC BY 4.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1602.07332" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://visualgenome.org/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Visual Genome</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          Visual Genome is a dataset, a knowledge base, an ongoing effort to connect structured image concepts to language. It contains: 108,077 Images
5.4 Million Region Descriptions
1.7 Million Visual Question Answers
3.8 Million Object Instances
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          CC BY 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution 4.0 International (CC BY 4.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></div> <div class="text-indigo-darker px-4 pb-4">2015</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-align-left text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">NLP</div></div> <div class="truncate"><a href="https://drive.google.com/drive/u/0/folders/0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">DBPedia, Amazon, Yelp, Yahoo!, Sogou, and AG</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2015</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        An extensive set of eight datasets for text classification. Datasets from DBPedia, Amazon, Yelp, Yahoo!, Sogou, and AG. Sample size of 120K to 3.6M, ranging from binary to 14 class problems. 
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Various
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Parts of the dataset are under different licenses, check the dataset web page for more information</div></td> <td class="p-4 text-right hidden lg:table-cell"><i class="text-grey-light far fa-file"></i></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-align-left text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://drive.google.com/drive/u/0/folders/0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">DBPedia, Amazon, Yelp, Yahoo!, Sogou, and AG</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          An extensive set of eight datasets for text classification. Datasets from DBPedia, Amazon, Yelp, Yahoo!, Sogou, and AG. Sample size of 120K to 3.6M, ranging from binary to 14 class problems. 
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Various
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Parts of the dataset are under different licenses, check the dataset web page for more information</div></div> <div class="text-indigo-darker px-4 pb-4">2015</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-align-left text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">NLP</div></div> <div class="truncate"><a href="https://cs.nyu.edu/~kcho/DMQA/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">CNN and Daily Mail summarization</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2015</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        
Two datasets using news articles for Q&amp;A research. Each dataset contains many documents (90k and 197k each), and each document companies on average 4 questions approximately. Each question is a sentence with one missing word/phrase which can be found from the accompanying document/context.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><i class="text-grey-light far fa-file"></i></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-align-left text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://cs.nyu.edu/~kcho/DMQA/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">CNN and Daily Mail summarization</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          
Two datasets using news articles for Q&amp;A research. Each dataset contains many documents (90k and 197k each), and each document companies on average 4 questions approximately. Each question is a sentence with one missing word/phrase which can be found from the accompanying document/context.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2015</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://www.cityscapes-dataset.com/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Cityscapes</a></div> <div class="relative ml-2 hidden lg:block"><a href="https://academictorrents.com/details/4f76b97fbb851fac002dcc55dcc55883e9728db7" target="_blank" class="HoverTooltip rotate-180 fas fa-magnet text-xs text-indigo-lighter hover:text-indigo no-underline"></a> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 120px; margin-left: -16px;">Download torrent</div></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2015</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        Large-scale dataset that contains a diverse set of stereo video sequences recorded in street scenes from 50 different cities, with high quality pixel-level annotations of 5 000 frames in addition to a larger set of 20 000 weakly annotated frames.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1604.01685" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://www.cityscapes-dataset.com/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Cityscapes</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          Large-scale dataset that contains a diverse set of stereo video sequences recorded in street scenes from 50 different cities, with high quality pixel-level annotations of 5 000 frames in addition to a larger set of 20 000 weakly annotated frames.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2015</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="http://activity-net.org/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">ACTIVITYNET</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2015</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        ActivityNet is a new large-scale video benchmark for human activity understanding. ActivityNet aims at covering a wide range of complex human activities that are of interest to people in their daily living. In its current version, ActivityNet provides samples from 203 activity classes with an average of 137 untrimmed videos per class and 1.41 activity instances per video, for a total of 849 video hours.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Not found
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">License information not found</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Heilbron_ActivityNet_A_Large-Scale_2015_CVPR_paper.pdf" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://activity-net.org/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">ACTIVITYNET</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          ActivityNet is a new large-scale video benchmark for human activity understanding. ActivityNet aims at covering a wide range of complex human activities that are of interest to people in their daily living. In its current version, ActivityNet provides samples from 203 activity classes with an average of 137 untrimmed videos per class and 1.41 activity instances per video, for a total of 849 video hours.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Not found
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">License information not found</div></div> <div class="text-indigo-darker px-4 pb-4">2015</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-music text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">Audio</div></div> <div class="truncate"><a href="http://www.openslr.org/12/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">LibriSpeech</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2015</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        Large-scale (1000 hours) corpus of read English speech.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      CC BY 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution 4.0 International (CC BY 4.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="http://www.danielpovey.com/files/2015_icassp_librispeech.pdf" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-music text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://www.openslr.org/12/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">LibriSpeech</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          Large-scale (1000 hours) corpus of read English speech.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          CC BY 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution 4.0 International (CC BY 4.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></div> <div class="text-indigo-darker px-4 pb-4">2015</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">IMDB-WIKI faces</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2015</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        Faces from the list of the most popular 100,000 actors as listed on the IMDb website and (automatically) crawled from their profiles date of birth, name, gender and all images related to that person. 460,723 face images from 20,284 celebrities from IMDb and 62,328 from Wikipedia, thus 523,051 in total. 
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commciral
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://www.vision.ee.ethz.ch/publications/papers/proceedings/eth_biwi_01229.pdf" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">IMDB-WIKI faces</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          Faces from the list of the most popular 100,000 actors as listed on the IMDb website and (automatically) crawled from their profiles date of birth, name, gender and all images related to that person. 460,723 face images from 20,284 celebrities from IMDb and 62,328 from Wikipedia, thus 523,051 in total. 
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commciral
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2015</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-align-left text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">NLP</div></div> <div class="truncate"><a href="https://nlp.stanford.edu/projects/snli/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">SNLI</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2015</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        The SNLI corpus (version 1.0) is a collection of 570k human-written English sentence pairs manually labeled for balanced classification with the labels entailment, contradiction, and neutral, supporting the task of natural language inference (NLI), also known as recognizing textual entailment (RTE).
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      CC BY-SA 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution-ShareAlike 4.0 International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit,
ShareAlike - if you make changes, you must distribute your contributions.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1508.05326" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-align-left text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://nlp.stanford.edu/projects/snli/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">SNLI</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          The SNLI corpus (version 1.0) is a collection of 570k human-written English sentence pairs manually labeled for balanced classification with the labels entailment, contradiction, and neutral, supporting the task of natural language inference (NLI), also known as recognizing textual entailment (RTE).
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          CC BY-SA 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution-ShareAlike 4.0 International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit,
ShareAlike - if you make changes, you must distribute your contributions.</div></div> <div class="text-indigo-darker px-4 pb-4">2015</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="http://cocodataset.org/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">COCO</a></div> <div class="relative ml-2 hidden lg:block"><a href="https://academictorrents.com/details/74dec1dd21ae4994dfd9069f9cb0443eb960c962" target="_blank" class="HoverTooltip rotate-180 fas fa-magnet text-xs text-indigo-lighter hover:text-indigo no-underline"></a> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 120px; margin-left: -16px;">Download torrent</div></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2014</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        COCO is a large-scale object detection, segmentation, and captioning dataset. It contains: 330K images (&gt;200K labeled), 1.5 million object instances, 80 object categories.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      CC BY 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution 4.0 International (CC BY 4.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1405.0312" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://cocodataset.org/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">COCO</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          COCO is a large-scale object detection, segmentation, and captioning dataset. It contains: 330K images (&gt;200K labeled), 1.5 million object instances, 80 object categories.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          CC BY 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution 4.0 International (CC BY 4.0) - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon, even commercialy,
Under the following terms:
Attribution - you must give approprate credit.</div></div> <div class="text-indigo-darker px-4 pb-4">2014</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://webscope.sandbox.yahoo.com/catalog.php?datatype=i&amp;did=67" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Yahoo Flickr Creative Commons 100M</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2014</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        This dataset contains a list of photos and videos. This list is compiled from data available on Yahoo! Flickr. All the photos and videos provided in the list are licensed under one of the Creative Commons copyright licenses.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Various
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Parts of the dataset are under different licenses, check the dataset web page for more information</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1503.01817" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://webscope.sandbox.yahoo.com/catalog.php?datatype=i&amp;did=67" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Yahoo Flickr Creative Commons 100M</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          This dataset contains a list of photos and videos. This list is compiled from data available on Yahoo! Flickr. All the photos and videos provided in the list are licensed under one of the Creative Commons copyright licenses.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Various
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Parts of the dataset are under different licenses, check the dataset web page for more information</div></div> <div class="text-indigo-darker px-4 pb-4">2014</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://www.cancerimagingarchive.net/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">The Cancer Imaging Archive</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2014</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        TCIA is a service which de-identifies and hosts a large archive of medical images of cancer accessible for public download. The data are organized as “Collections”, typically patients related by a common disease (e.g. lung cancer), image modality (MRI, CT, etc) or research focus. DICOM is the primary file format used by TCIA for image storage.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Various
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Dataset are under different licenses, check the dataset web page for more information</div></td> <td class="p-4 text-right hidden lg:table-cell"><i class="text-grey-light far fa-file"></i></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://www.cancerimagingarchive.net/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">The Cancer Imaging Archive</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          TCIA is a service which de-identifies and hosts a large archive of medical images of cancer accessible for public download. The data are organized as “Collections”, typically patients related by a common disease (e.g. lung cancer), image modality (MRI, CT, etc) or research focus. DICOM is the primary file format used by TCIA for image storage.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Various
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Dataset are under different licenses, check the dataset web page for more information</div></div> <div class="text-indigo-darker px-4 pb-4">2014</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="http://www.stat.ucla.edu/~xianjie.chen/pascal_part_dataset/pascal_part.html" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Pascal part</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2014</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        This dataset is a set of additional annotations for PASCAL VOC 2010. It goes beyond the original PASCAL object detection task by providing segmentation masks for each body part of the object.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Not found
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">License information not found</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1406.2031" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://www.stat.ucla.edu/~xianjie.chen/pascal_part_dataset/pascal_part.html" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Pascal part</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          This dataset is a set of additional annotations for PASCAL VOC 2010. It goes beyond the original PASCAL object detection task by providing segmentation masks for each body part of the object.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Not found
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">License information not found</div></div> <div class="text-indigo-darker px-4 pb-4">2014</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="http://mmlab.ie.cuhk.edu.hk/projects/PETA.html" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">PETA</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2014</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        Pedestrian Attribute Recognition At Far Distance dataset. The PETA dataset consists of 19000 images, with resolution ranging from 17-by-39 to 169-by-365 pixels. Those 19000 images include 8705 persons, each annotated with 61 binary and 4 multi-class attributes.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="http://mmlab.ie.cuhk.edu.hk/projects/PETA_files/Pedestrian%20Attribute%20Recognition%20At%20Far%20Distance.pdf" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://mmlab.ie.cuhk.edu.hk/projects/PETA.html" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">PETA</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          Pedestrian Attribute Recognition At Far Distance dataset. The PETA dataset consists of 19000 images, with resolution ranging from 17-by-39 to 169-by-365 pixels. Those 19000 images include 8705 persons, each annotated with 61 binary and 4 multi-class attributes.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2014</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="http://hockenmaier.cs.illinois.edu/DenotationGraph/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Flickr30k</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2014</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        An image caption corpus consisting of 158,915 crowd-sourced captions describing 31,783 images. This is an extension of the Flickr 8k Dataset. The new images and captions focus on people involved in everyday activities and events.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="http://hockenmaier.cs.illinois.edu/DenotationGraph/TACLDenotationGraph.pdf" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://hockenmaier.cs.illinois.edu/DenotationGraph/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Flickr30k</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          An image caption corpus consisting of 158,915 crowd-sourced captions describing 31,783 images. This is an extension of the Flickr 8k Dataset. The new images and captions focus on people involved in everyday activities and events.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2014</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://www.vision.ee.ethz.ch/datasets_extra/food-101/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Food 101</a></div> <div class="relative ml-2 hidden lg:block"><a href="https://academictorrents.com/details/470791483f8441764d3b01dbc4d22b3aa58ef46f" target="_blank" class="HoverTooltip rotate-180 fas fa-magnet text-xs text-indigo-lighter hover:text-indigo no-underline"></a> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 120px; margin-left: -16px;">Download torrent</div></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2014</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        We introduce a challenging data set of 101 food categories, with 101'000 images. For each class, 250 manually reviewed test images are provided as well as 750 training images. On purpose, the training images were not cleaned, and thus still contain some amount of noise. This comes mostly in the form of intense colors and sometimes wrong labels. All images were rescaled to have a maximum side length of 512 pixels.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Not found
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">License information not found</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://www.vision.ee.ethz.ch/datasets_extra/food-101/static/bossard_eccv14_food-101.pdf" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://www.vision.ee.ethz.ch/datasets_extra/food-101/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Food 101</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          We introduce a challenging data set of 101 food categories, with 101'000 images. For each class, 250 manually reviewed test images are provided as well as 750 training images. On purpose, the training images were not cleaned, and thus still contain some amount of noise. This comes mostly in the form of intense colors and sometimes wrong labels. All images were rescaled to have a maximum side length of 512 pixels.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Not found
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">License information not found</div></div> <div class="text-indigo-darker px-4 pb-4">2014</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-car text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 100px; margin-left: -16px;">Self-driving</div></div> <div class="truncate"><a href="http://www.cvlibs.net/datasets/kitti/index.php" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">KITTI</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2013</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        A novel dataset captured from a VW station wagon for use in mobile robotics and autonomous driving
research. In total, 6 hours of traffic scenarios recorded at
10-100 Hz. The scenarios are diverse, capturing real-world traffic
situations and range from freeways over rural areas to innercity
scenes with many static and dynamic objects.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      CC BY-NC-SA 4.0
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Attribution-NonCommercial-ShareAlike International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon,
Under the following terms:
Attribution - you must give approprate credit,
NonCommercial - you may not use the material for commercial purposes,
ShareAlike - if you make changes, you must distribute your contributions.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="http://www.cvlibs.net/publications/Geiger2013IJRR.pdf" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-car text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://www.cvlibs.net/datasets/kitti/index.php" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">KITTI</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          A novel dataset captured from a VW station wagon for use in mobile robotics and autonomous driving
research. In total, 6 hours of traffic scenarios recorded at
10-100 Hz. The scenarios are diverse, capturing real-world traffic
situations and range from freeways over rural areas to innercity
scenes with many static and dynamic objects.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          CC BY-NC-SA 4.0
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Attribution-NonCommercial-ShareAlike International - 
You are free to:
Share - copy and redistribute,
Adapt - remix, transform, and build upon,
Under the following terms:
Attribution - you must give approprate credit,
NonCommercial - you may not use the material for commercial purposes,
ShareAlike - if you make changes, you must distribute your contributions.</div></div> <div class="text-indigo-darker px-4 pb-4">2013</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://ai.stanford.edu/~jkrause/cars/car_dataset.html" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Stanford cars</a></div> <div class="relative ml-2 hidden lg:block"><a href="https://academictorrents.com/details/9c90b7f6208d430bff288845d45667ab2670da56" target="_blank" class="HoverTooltip rotate-180 fas fa-magnet text-xs text-indigo-lighter hover:text-indigo no-underline"></a> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 120px; margin-left: -16px;">Download torrent</div></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2013</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        Stanford Cars dataset contains 16,185 images of 196 classes of cars. The data is split into 8,144 training images and 8,041 testing images, where each class has been split roughly in a 50-50 split. Classes are typically at the level of Make, Model, Year, e.g. 2012 Tesla Model S or 2012 BMW M3 coupe.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Not found
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">License information not found</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://ai.stanford.edu/~jkrause/papers/3drr13.pdf" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://ai.stanford.edu/~jkrause/cars/car_dataset.html" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Stanford cars</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          Stanford Cars dataset contains 16,185 images of 196 classes of cars. The data is split into 8,144 training images and 8,041 testing images, where each class has been split roughly in a 50-50 split. Classes are typically at the level of Make, Model, Year, e.g. 2012 Tesla Model S or 2012 BMW M3 coupe.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Not found
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">License information not found</div></div> <div class="text-indigo-darker px-4 pb-4">2013</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://www.vision.rwth-aachen.de/page/paris500k" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Paris500k</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2013</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        The Paris500k dataset consists of 501,356 geotagged images collected from Flickr and Panoramio. The dataset was collected from a geographic bounding box rather than using keyword queries. Thus, the images have a "natural" distribution, as shown in the figure on the right. The dataset is very challenging due to the presence of duplicates and near-duplicates, as well as a large fraction of unrelated images, such as photos of parties, pets, etc.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://vision.rwth-aachen.de/media/papers/weyandiccv13.pdf" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://www.vision.rwth-aachen.de/page/paris500k" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Paris500k</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          The Paris500k dataset consists of 501,356 geotagged images collected from Flickr and Panoramio. The dataset was collected from a geographic bounding box rather than using keyword queries. Thus, the images have a "natural" distribution, as shown in the figure on the right. The dataset is very challenging due to the presence of duplicates and near-duplicates, as well as a large fraction of unrelated images, such as photos of parties, pets, etc.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2013</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-align-left text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">NLP</div></div> <div class="truncate"><a href="http://www.statmt.org/lm-benchmark/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Billion Words</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2013</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        The purpose of the project is to make available a standard training and test setup for language modeling experiments.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Not found
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">License information not found</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1312.3005" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-align-left text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://www.statmt.org/lm-benchmark/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Billion Words</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          The purpose of the project is to make available a standard training and test setup for language modeling experiments.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Not found
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">License information not found</div></div> <div class="text-indigo-darker px-4 pb-4">2013</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-align-left text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">NLP</div></div> <div class="truncate"><a href="https://nlp.stanford.edu/sentiment/code.html" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Stanford Sentiment Treebank</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2013</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        A dataset for sentiment analysis that includes fine grained
sentiment labels for 215,154 phrases in the
parse trees of 11,855 sentences and presents
new challenges for sentiment compositionality.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Not found
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">License information not found</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-align-left text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://nlp.stanford.edu/sentiment/code.html" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Stanford Sentiment Treebank</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          A dataset for sentiment analysis that includes fine grained
sentiment labels for 215,154 phrases in the
parse trees of 11,855 sentences and presents
new challenges for sentiment compositionality.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Not found
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">License information not found</div></div> <div class="text-indigo-darker px-4 pb-4">2013</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="http://host.robots.ox.ac.uk/pascal/VOC/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">PASCAL VOC 2012</a></div> <div class="relative ml-2 hidden lg:block"><a href="https://academictorrents.com/details/e6d591cef9ea2840f7d8dfb6bb0e0503d5592128" target="_blank" class="HoverTooltip rotate-180 fas fa-magnet text-xs text-indigo-lighter hover:text-indigo no-underline"></a> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 120px; margin-left: -16px;">Download torrent</div></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2012</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        PASCAL VOC (2012 version) has 20 classes. The train/val data has 11,530 images containing 27,450 ROI annotated objects and 6,929 segmentations.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="http://host.robots.ox.ac.uk/pascal/VOC/pubs/everingham10.pdf" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://host.robots.ox.ac.uk/pascal/VOC/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">PASCAL VOC 2012</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          PASCAL VOC (2012 version) has 20 classes. The train/val data has 11,530 images containing 27,450 ROI annotated objects and 6,929 segmentations.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2012</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="http://benchmark.ini.rub.de/?section=gtsrb" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">The German Traffic Sign Recognition Benchmark</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2012</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        The German Traffic Sign Benchmark is a multi-class, single-image classification challenge held at the IJCNN 2011. The dataset contains: more than 40 classes, more than 50,000 images in total.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Not found
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">License information not found</div></td> <td class="p-4 text-right hidden lg:table-cell"><i class="text-grey-light far fa-file"></i></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://benchmark.ini.rub.de/?section=gtsrb" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">The German Traffic Sign Recognition Benchmark</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          The German Traffic Sign Benchmark is a multi-class, single-image classification challenge held at the IJCNN 2011. The dataset contains: more than 40 classes, more than 50,000 images in total.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Not found
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">License information not found</div></div> <div class="text-indigo-darker px-4 pb-4">2012</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="http://ufldl.stanford.edu/housenumbers/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">SVHN Street View House Numbers</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2011</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        SVHN is a real-world image dataset for developing machine learning and object recognition algorithms with minimal requirement on data preprocessing and formatting. It can be seen as similar to MNIST (e.g., the images are of small cropped digits), but incorporates an order of magnitude more labeled data (over 600,000 digit images) and comes from a significantly harder, unsolved, real world problem (recognizing digits and numbers in natural scene images). SVHN is obtained from house numbers in Google Street View images.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="http://ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://ufldl.stanford.edu/housenumbers/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">SVHN Street View House Numbers</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          SVHN is a real-world image dataset for developing machine learning and object recognition algorithms with minimal requirement on data preprocessing and formatting. It can be seen as similar to MNIST (e.g., the images are of small cropped digits), but incorporates an order of magnitude more labeled data (over 600,000 digit images) and comes from a significantly harder, unsolved, real world problem (recognizing digits and numbers in natural scene images). SVHN is obtained from house numbers in Google Street View images.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2011</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip fas fa-align-left text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">NLP</div></div> <div class="truncate"><a href="http://ai.stanford.edu/~amaas/data/sentiment/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Large Movie Review Dataset</a></div> <div class="relative ml-2 hidden lg:block"><a href="https://academictorrents.com/details/fd24bc44d461b10288469e05a64a8344eb079f15" target="_blank" class="HoverTooltip rotate-180 fas fa-magnet text-xs text-indigo-lighter hover:text-indigo no-underline"></a> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 120px; margin-left: -16px;">Download torrent</div></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2011</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided. 
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Not found
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">License information not found</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="http://www.aclweb.org/anthology/P11-1015" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="fas fa-align-left text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://ai.stanford.edu/~amaas/data/sentiment/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Large Movie Review Dataset</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided. 
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Not found
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">License information not found</div></div> <div class="text-indigo-darker px-4 pb-4">2011</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-plus-square text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 60px; margin-left: -16px;">Medical</div></div> <div class="truncate"><a href="http://wordnet.princeton.edu/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Princeton WordNet</a></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2010</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        WordNet is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept. Synsets are interlinked by means of conceptual-semantic and lexical relations. The resulting network of meaningfully related words and concepts can be navigated with the browser. WordNet is also freely and publicly available for download.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
      WordNet license
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">WordNet® is unencumbered, and may be used in commercial applications in accordance with the following license agreement. (see website for license)</div></td> <td class="p-4 text-right hidden lg:table-cell"><i class="text-grey-light far fa-file"></i></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-plus-square text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://wordnet.princeton.edu/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Princeton WordNet</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          WordNet is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept. Synsets are interlinked by means of conceptual-semantic and lexical relations. The resulting network of meaningfully related words and concepts can be navigated with the browser. WordNet is also freely and publicly available for download.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-green-lightest text-green-dark">
          WordNet license
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">WordNet® is unencumbered, and may be used in commercial applications in accordance with the following license agreement. (see website for license)</div></div> <div class="text-indigo-darker px-4 pb-4">2010</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="https://www.cs.toronto.edu/~kriz/cifar.html" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">CIFAR-100</a></div> <div class="relative ml-2 hidden lg:block"><a href="https://academictorrents.com/details/4fb115df73d3313fae9264fd6c0bad061add2d63" target="_blank" class="HoverTooltip rotate-180 fas fa-magnet text-xs text-indigo-lighter hover:text-indigo no-underline"></a> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 120px; margin-left: -16px;">Download torrent</div></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2009</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        This dataset is just like the CIFAR-10, except it has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. Each image comes with a “fine” label (the class to which it belongs) and a “coarse” label (the superclass to which it belongs).
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Not found
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">License information not found</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file-pdf"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="https://www.cs.toronto.edu/~kriz/cifar.html" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">CIFAR-100</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          This dataset is just like the CIFAR-10, except it has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. Each image comes with a “fine” label (the class to which it belongs) and a “coarse” label (the superclass to which it belongs).
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Not found
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">License information not found</div></div> <div class="text-indigo-darker px-4 pb-4">2009</div></div></td></tr> <tr class="border-b border-grey-light text-grey-darker text-sm"><td class="relative hidden lg:table-cell"><div class="p-4 flex items-center bg-white"><div class="relative mr-2"><i class="HoverTooltip far fa-eye text-sm text-indigo-dark cursor-pointer"></i> <div class="HoverPopup TopLeftArrow absolute text-center text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 50px; margin-left: -16px;">CV</div></div> <div class="truncate"><a href="http://www.image-net.org/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">ImageNet</a></div> <div class="relative ml-2 hidden lg:block"><a href="https://academictorrents.com/details/564a77c1e1119da199ff32622a1609431b9f1c47" target="_blank" class="HoverTooltip rotate-180 fas fa-magnet text-xs text-indigo-lighter hover:text-indigo no-underline"></a> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 120px; margin-left: -16px;">Download torrent</div></div></div></td> <td class="p-4 text-indigo-darker hidden lg:table-cell">2009</td> <td class="relative hidden lg:table-cell"><div class="flex items-center px-4"><div class="cursor-pointer text-indigo-darker leading-normal py-2 truncate">
        ImageNet is an image database organized according to the WordNet hierarchy (currently only the nouns), in which each node of the hierarchy is depicted by hundreds and thousands of images.
      </div> <i class="fas fa-chevron-down text-indigo-lighter text-xs cursor-pointer"></i></div></td> <td class="relative hidden lg:table-cell"><span class="License HoverTooltip cursor-default px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
      Non-commercial
    </span> <div class="HoverPopup TopLeftArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></td> <td class="p-4 text-right hidden lg:table-cell"><a href="https://arxiv.org/abs/1409.0575" target="_blank" class="text-indigo-dark hover:text-indigo no-underline far fa-file"></a></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col"><div class="p-4 pb-5 flex items-center"><i class="far fa-eye text-sm mr-2 text-indigo-dark cursor-pointer"></i> <div class="truncate"><a href="http://www.image-net.org/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">ImageNet</a></div></div> <div class="flex items-center px-4 pb-4"><div class="cursor-pointer text-indigo-darker leading-normal truncate">
          ImageNet is an image database organized according to the WordNet hierarchy (currently only the nouns), in which each node of the hierarchy is depicted by hundreds and thousands of images.
        </div> <i class="fas fa-chevron-down text-indigo-lighter ml-1 text-xs"></i></div></div></td> <td class="relative lg:hidden" style="vertical-align: top;"><div class="flex flex-col items-end"><div class="p-4"><span class="License cursor-pointer px-2 py-1 rounded font-12 truncate inline-block bg-grey-lightest text-grey-darker">
          Non-commercial
        </span> <div class="HoverPopup TopRightArrow absolute text-grey-darker bg-black text-white rounded p-2 mt-4 pin-r" style="width: 240px;">Can only be used for research and educational purposes. Commercial use is prohibited.</div></div> <div class="text-indigo-darker px-4 pb-4">2009</div></div></td></tr></tbody></table></div></div></div></div>

<div class="pb-20 text-center px-2">
  <div class="text-indigo-darker">You can find more datasets at the <a href="https://archive.ics.uci.edu/ml/datasets" target="blank" class="text-indigo-darker hover:text-indigo font-semibold no-underline">UCI machine learning repository</a> and <a href="https://www.kaggle.com/datasets" target="_blank" class="text-indigo-darker hover:text-indigo font-semibold no-underline">Kaggle datasets</a>.</div>
</div>

<script>
var datasetList = [{"id":198,"name":"ObjectNet","type":"cv","description":"ObjectNet is a large real-world test set for object recognition with control where object backgrounds, rotations, and imaging viewpoints are random. Collected to intentionally show objects from new viewpoints on new backgrounds. 50,000 image test set, same as ImageNet, with controls for rotation, background, and viewpoint. 313 object classes with 113 overlapping ImageNet.","year":2019,"link":"https://objectnet.dev/","license":"CC BY 4.0","licenseType":"commercial","licenseText":"Attribution 4.0 International (CC BY 4.0) - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon, even commercialy,\nUnder the following terms:\nAttribution - you must give approprate credit.","pdf":"https://objectnet.dev/objectnet-a-large-scale-bias-controlled-dataset-for-pushing-the-limits-of-object-recognition-models.pdf","status":"new"},{"id":197,"name":"JRDB","type":"cv","description":"JRDB is the largest benchmark data for 2D-3D person tracking, including: Over 60K frames (67 minutes) sensor data captured from 5 stereo camera and two LiDAR sensors, 54 sequences from different locations, during day and night time, indoors and outdoors in a university campus environment. Around 2 milion high quality 2D bounding box annotations on 360° cylindrical video streams generated from 5 stereo cameras ","year":2019,"link":"https://jrdb.stanford.edu/dataset/about","license":"CC-BY-NC-SA 4.0","licenseType":"non-commercial","licenseText":"Attribution-NonCommercial-ShareAlike International - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon,\nUnder the following terms:\nAttribution - you must give approprate credit,\nNonCommercial - you may not use the material for commercial purposes,\nShareAlike - if you make changes, you must distribute your contributions.","arxiv":1910.11792,"status":"new"},{"id":196,"name":"xBD","type":"cv","description":"A dataset for assessing building damage from satellite imagery. With over 850,000 building polygons from six different types of natural disaster around the world, covering a total area of over 45,000 square kilometers, the xBD dataset is one of the largest and highest quality public datasets of annotated high-resolution satellite imagery.","year":2019,"link":"https://xview2.org/dataset","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","arxiv":1911.09296,"status":"new"},{"id":195,"name":"BLiMP","type":"nlp","description":"The Benchmark of Linguistic Minimal Pairs. BLiMP is a challenge set for evaluating what language models (LMs) know about major grammatical phenomena in English. BLiMP consists of 67 sub-datasets, each containing 1000 minimal pairs isolating specific contrasts in syntax, morphology, or semantics. The data is automatically generated according to expert-crafted grammars.","year":2019,"link":"https://github.com/alexwarstadt/blimp","license":"Not found","licenseType":"non-commercial","licenseText":"License information not found","arxiv":1912.00582,"status":"new"},{"id":194,"name":"Logo-2k+","type":"cv","description":"A Large-Scale Logo Dataset for Scalable Logo Classiﬁcation. Our resulting logo dataset contains 167,140 images with 10 root categories and 2,341 categories.","year":2019,"link":"https://github.com/msn199959/Logo-2k-plus-Dataset","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","arxiv":1911.07924,"status":"new"},{"id":193,"name":"SemanticKITTI","type":"self-driving","description":"SemanticKITTI is based on the KITTI Vision Benchmark and we provide semantic annotation for all sequences of the Odometry Benchmark. The dataset contains 28 classes including classes distinguishing non-moving and moving objects.","year":2019,"link":"http://www.semantic-kitti.org/index.html","license":"CC-BY-NC-SA 4.0","licenseType":"non-commercial","licenseText":"Attribution-NonCommercial-ShareAlike International - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon,\nUnder the following terms:\nAttribution - you must give approprate credit,\nNonCommercial - you may not use the material for commercial purposes,\nShareAlike - if you make changes, you must distribute your contributions.","arxiv":1904.01416,"status":"new"},{"id":192,"name":"HACS","type":"cv","description":"This project introduces a novel video dataset, named HACS (Human Action Clips and Segments). It consists of two kinds of manual annotations. HACS Clips contains 1.55M 2-second clip annotations; HACS Segments has complete action segments (from action start to end) on 50K videos. The large-scale dataset is effective for pretraining action recognition and localization models, and also serves as a new benchmark for temporal action localization.","year":2019,"link":"http://hacs.csail.mit.edu/","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","arxiv":1712.09374,"status":"new"},{"id":191,"name":"Astyx HiRes2019","type":"self-driving","description":"A   radar-centric   automotive   datasetbased   on   radar,   lidar   and   camera   data   for   the   purposeof   3D   object   detection. ","year":2019,"link":"https://www.astyx.com/development/astyx-hires2019-dataset.html","license":"CC-BY-NC-SA 4.0","licenseType":"non-commercial","licenseText":"Attribution-NonCommercial-ShareAlike International - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon,\nUnder the following terms:\nAttribution - you must give approprate credit,\nNonCommercial - you may not use the material for commercial purposes,\nShareAlike - if you make changes, you must distribute your contributions.","pdf":"https://www.astyx.com/fileadmin/redakteur/dokumente/Automotive_Radar_Dataset_for_Deep_learning_Based_3D_Object_Detection.PDF","status":"new"},{"id":190,"name":"SEN12MS","type":"cv","description":"SEN12MS is a dataset consisting of 180,748 corresponding image triplets containing Sentinel-1 dual-pol SAR data, Sentinel-2 multi-spectral imagery, and MODIS-derived land cover maps. \n","year":2019,"link":"https://www.isprs-ann-photogramm-remote-sens-spatial-inf-sci.net/IV-2-W7/153/2019/","license":"CC BY 4.0","licenseType":"commercial","licenseText":"Attribution 4.0 International (CC BY 4.0) - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon, even commercialy,\nUnder the following terms:\nAttribution - you must give approprate credit.","arxiv":1906.07789,"status":"new"},{"id":189,"name":"VisDrone2019","type":"cv","description":"The VisDrone2019 dataset is collected by the AISKYEYE team at Lab of Machine Learning and Data Mining , Tianjin University, China. The benchmark dataset consists of 288 video clips formed by 261,908 frames and 10,209 static images, captured by various drone-mounted cameras, covering a wide range of aspects including location (taken from 14 different cities separated by thousands of kilometers in China), environment (urban and country), objects (pedestrian, vehicles, bicycles, etc.), and density (sparse and crowded scenes). ","year":2019,"link":"http://www.aiskyeye.com/","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","pdf":"http://www.aiskyeye.com/upfile/Vision_Meets_Drones_A_Challenge.pdf","status":"new"},{"id":188,"name":"Datasets for skin image analysis","type":"medical\n","description":"A list of datasets for skin image analysis, from the 'Visual Diagnosis of Dermatological Disorders: Human and Machine Performance' paper.","year":2019,"link":"https://www.medicalimageanalysis.com/research/skinia","license":"Various","licenseType":"non-commercial","licenseText":"This is a list of several datasets, check the links on the website for individual licenses","arxiv":1906.01256,"status":"new"},{"id":187,"name":"OPIEC","type":"nlp","description":"OPIEC is an Open Information Extraction (OIE) corpus, constructed from the entire English Wikipedia. It containing more than 341M triples. Each triple from the corpus is composed of rich meta-data: each token from the subj / obj / rel along with NLP annotations (POS tag, NER tag, ...), provenance sentence (along with its dependency parse, sentence order relative to the article), original (golden) links contained in the Wikipedia articles, space / time, etc.","year":2019,"link":"https://www.uni-mannheim.de/en/dws/research/resources/opiec/","license":"CC-BY-SA 4.0","licenseType":"commercial","licenseText":"Attribution-ShareAlike 4.0 International - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon, even commercialy,\nUnder the following terms:\nAttribution - you must give approprate credit,\nShareAlike - if you make changes, you must distribute your contributions.","arxiv":1904.12324,"status":"new"},{"id":186,"name":"AEV Autonomous Driving Dataset","type":"self-driving","description":"The dataset features 2D semantic segmentation, 3D point clouds, 3D bounding boxes, and vehicle bus data. Dataset includes more than 40,000 frames with semantic segmentation image and point cloud labels, of which more than 12,000 frames also have annotations for 3D bounding boxes. In addition, we provide unlabelled sensor data (approx. 390,000 frames) for sequences with several loops, recorded in three cities. A2D2 is around 2.3 TB in total.","year":2019,"link":"https://www.audi-electronics-venture.de/aev/web/en/driving-dataset.html","license":"CC BY-ND 4.0","licenseType":"commercial","licenseText":"Attribution No Derivatives 4.0 International (CC BY ND 4.0) - \nYou are free to:\nShare - copy and redistribute,\nUnder the following terms:\nAttribution - you must give approprate credit.,\nNoDerivatives - you may not redistribute the modified material."},{"id":185,"name":"BigEarthNet","type":"cv","description":"The BigEarthNet is a new large-scale Sentinel-2 benchmark archive, consisting of 590,326 Sentinel-2 image patches. To construct the BigEarthNet, 125 Sentinel-2 tiles acquired between June 2017 and May 2018 over the 10 countries (Austria, Belgium, Finland, Ireland, Kosovo, Lithuania, Luxembourg, Portugal, Serbia, Switzerland) of Europe were initially selected. All the tiles were atmospherically corrected by the Sentinel-2 Level 2A product generation and formatting tool (sen2cor). Then, they were divided into 590,326 non-overlapping image patches. Each image patch was annotated by the multiple land-cover classes (i.e., multi-labels) that were provided from the CORINE Land Cover database of the year 2018.","year":2019,"link":"http://bigearth.net/","license":"CDLA Permissive","licenseType":"commercial","licenseText":"The CDLA-Permissive agreement is similar to permissive open source licenses in that the publisher of data allows anyone to use, modify and do what they want with the data with no obligations to share any of their changes or modifications.","pdf":"http://bigearth.net/static/documents/BigEarthNet_IGARSS_2019.pdf"},{"id":184,"name":"Deepfake Detection Challenge Dataset","type":"cv","description":"Facebook, Microsoft, Amazon Web Services, and the Partnership on AI have created the Deepfake Detection Challenge to encourage research into deepfake detection. Dataset consists of around 5000 videos, both original and manipulated. To build the dataset, the researchers crowdsourced videos from people while \"ensuring a variability in gender, skin tone and age\".","year":2019,"link":"https://deepfakedetectionchallenge.ai/dataset","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","arxiv":1910.08854},{"id":183,"name":"WiderPerson","type":"cv","description":"The WiderPerson dataset is a pedestrian detection benchmark dataset in the wild, of which images are selected from a wide range of scenarios, no longer limited to the traffic scenario. We choose 13,382 images and label about 400K annotations with various kinds of occlusions. We randomly select 8000/1000/4382 images as training, validation and testing subsets. ","year":2019,"link":"http://www.cbsr.ia.ac.cn/users/sfzhang/WiderPerson/","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","arxiv":1909.12118},{"id":182,"name":"3D60","type":"cv","description":"3D60 is a collective dataset generated in the context of various 360 vision research works. It comprises multi-modal (i.e. color, depth and normal) omnidirectional stereo renders (i.e. horizontal and vertical) of scenes from realistic and synthetic large-scale 3D datasets (Matterport3D, Stanford2D3D, SunCG). Contains 224,406 spherical panoramas.","year":2019,"link":"https://vcl3d.github.io/3D60/","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","arxiv":1909.08112},{"id":181,"name":"CommonsenseQA","type":"qa","description":"CommonsenseQA is a new multiple-choice question answering dataset that requires different types of commonsense knowledge to predict the correct answers . It contains 12,102 questions with one correct answer and four distractor answers.  The dataset is provided in two major training/validation/testing set splits: \"Random split\" which is the main evaluation split, and \"Question token split\".","year":2019,"link":"https://www.tau-nlp.org/commonsenseqa","license":"Not found","licenseType":"non-commercial","licenseText":"License information not found","arxiv":1811.00937},{"id":180,"name":"Oxford Radar RobotCar Dataset","type":"self-driving","description":"The Oxford Radar RobotCar Dataset is a radar extension to The Oxford RobotCar Dataset. We provide data from a Navtech CTS350-X Millimetre-Wave FMCW radar and Dual Velodyne HDL-32E LIDARs with optimised ground truth radar odometry for 280 km of driving around Oxford, UK (in addition to all sensors in the original Oxford RobotCar Dataset).","year":2019,"link":"https://dbarnes.github.io/radar-robotcar-dataset/","license":"CC-BY-NC-SA 4.0","licenseType":"non-commercial","licenseText":"Attribution-NonCommercial-ShareAlike International - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon,\nUnder the following terms:\nAttribution - you must give approprate credit,\nNonCommercial - you may not use the material for commercial purposes,\nShareAlike - if you make changes, you must distribute your contributions.","arxiv":1909.013},{"id":179,"name":"Total Text","type":"nlp","description":"The Total-Text consists of 1555 images with more than 3 different text orientations: Horizontal, Multi-Oriented, and Curved, one of a kind.","year":2019,"link":"https://github.com/cs-chan/Total-Text-Dataset","license":"BSD","licenseType":"commercial","licenseText":"BSD 3-Clause \"New\" or \"Revised\" License -\n\nA permissive license similar to the BSD 2-Clause License, but with a 3rd clause that prohibits others from using the name of the project or its contributors to promote derived products without written consent."},{"id":178,"name":"ArT","type":"nlp","description":"ArT is a combination of Total-Text, SCUT-CTW1500 and Baidu Curved Scene Text, which were collected with the motive of introducing the arbitrary-shaped text problem to the scene text community. There is a total of 10,166 images in the ArT dataset. The ArT dataset was collected with text shape diversity in mind, hence all existing text shapes (i.e. horizontal, multi-oriented, and curved) have high number of existence in the dataset, which makes it an unique dataset.\n","year":2019,"link":"https://rrc.cvc.uab.es/?ch=14","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","arxiv":1909.07145},{"id":177,"name":"Celeb-DF","type":"cv","description":"DeepFake Forensics (Celeb-DF) dataset contains real and DeepFake synthesized videos having similar visual quality on par with those circulated online. The Celeb-DF dataset includes 408 original videos collected from YouTube with subjects of different ages, ethic groups and genders, and 795 DeepFake videos synthesized from these real videos.","year":2019,"link":"http://www.cs.albany.edu/~lsw/celeb-deepfakeforensics.html","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","arxiv":1909.12962},{"id":176,"name":"ExDARK Dataset","type":"cv","description":"The Exclusively Dark (ExDARK) dataset is a collection of 7,363 low-light images from very low-light environments to twilight (i.e 10 different conditions) with 12 object classes (similar to PASCAL VOC) annotated on both image class level and local object bounding boxes","year":2019,"link":"https://github.com/cs-chan/Exclusively-Dark-Image-Dataset","license":"BSD","licenseType":"commercial","licenseText":"BSD 3-Clause \"New\" or \"Revised\" License -\n\nA permissive license similar to the BSD 2-Clause License, but with a 3rd clause that prohibits others from using the name of the project or its contributors to promote derived products without written consent.","arxiv":1805.11227},{"id":175,"name":"Schema-Guided Dialogue","type":"nlp","description":"Schema-Guided Dialogue (SGD) dataset, containing over 16k multi-domain conversations spanning 16 domains. Our dataset exceeds the existing task-oriented dialogue corpora in scale, while also highlighting the challenges associated with building large-scale virtual assistants. It provides a challenging testbed for a number of tasks including language understanding, slot filling, dialogue state tracking and response generation.","year":2019,"link":"https://github.com/google-research-datasets/dstc8-schema-guided-dialogue","license":"Not found","licenseType":"non-commercial","licenseText":"License information not found","arxiv":1909.05855},{"id":174,"name":"SIDD","type":"cv","description":"The Smartphone Image Denoising Dataset (SIDD), of ~30,000 noisy images from 10 scenes under different lighting conditions using five representative smartphone cameras and generated their ground truth images.","year":2019,"link":"https://www.eecs.yorku.ca/~kamel/sidd/","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","pdf":"https://www.eecs.yorku.ca/~kamel/sidd/files/SIDD_CVPR_2018.pdf"},{"id":173,"name":"Brno Urban Dataset","type":"self-driving","description":"A new dataset recorded in Brno, Czech Republic. It offers data from four WUXGA cameras, two 3D LiDARs, inertial measurement unit, infrared camera and especially differential RTK GNSS receiver with centimetre accuracy which, to the best knowledge of the authors, is not available from any other public dataset so far. In addition, all the data are precisely timestamped with sub-millisecond precision to allow wider range of applications. At the time of publishing of the paper, it contains recordings of more than 350 km of rides in varying environments.","year":2019,"link":"https://github.com/Robotics-BUT/Brno-Urban-Dataset","license":"MIT","licenseType":"commercial","licenseText":"MIT - You are free to: use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the work. Under the following terms: the work is provided \"as is\", you must include copyright and the license in all copies or substantial uses of the work.","arxiv":1909.06897},{"id":172,"name":"CrowdFix","type":"cv","description":"Dataset of Human Eye Fixation over Crowd Videos. CrowdFix includes 434 videos with diverse crowd scenes, containing a total of 37,493 frames and 1,249 seconds. The diverse content refers to different crowd activities under three distinct categories - Sparse, Dense Free Flowing and Dense Congested. All videos are at 720p resolution and 30 Hz frame rate.","year":2019,"link":"https://github.com/MemoonaTahira/CrowdFix","license":"Not found","licenseType":"non-commercial","licenseText":"License information not found","arxiv":1910.02618},{"id":171,"name":"INTERACTION Dataset","type":"self-driving","description":"The INTERACTION dataset contains naturalistic motions of various traffic participants in a variety of highly interactive driving scenarios. Using drones and traffic cameras, trajectories were captured from different countries, including the US, Germany, China and other countries.","year":2019,"link":"https://interaction-dataset.com/","license":"Research and commercial","licenseType":"commercial","licenseText":"Research and commercial licenses available.","arxiv":1910.03088},{"id":170,"name":"DIODE: A Dense Indoor and Outdoor DEpth Dataset","type":"cv","description":"DIODE (Dense Indoor and Outdoor DEpth) is a dataset that contains diverse high-resolution color images with accurate, dense, wide-range depth measurements. It is the first public dataset to include RGBD images of indoor and outdoor scenes obtained with one sensor suite. ","year":2019,"link":"https://diode-dataset.org/","license":"MIT","licenseType":"commercial","licenseText":"MIT - You are free to: use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the work. Under the following terms: the work is provided \"as is\", you must include copyright and the license in all copies or substantial uses of the work.","arxiv":1908.00463},{"id":169,"name":"100,000 Faces","type":"cv","description":"100,000 Faces Generated by AI. We have built an original machine learning dataset, and used StyleGAN (an amazing resource by NVIDIA) to construct a realistic set of 100,000 faces. Our dataset has been built by taking 29,000+ photos of 69 different models over the last 2 years in our studio.","year":2019,"link":"https://generated.photos/","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited."},{"id":168,"name":"Objects365","type":"cv","description":"Objects365 is a brand new dataset, designed to spur object detection research with a focus on diverse objects in the Wild:\n\n365 categories\n600k images\n10 million bounding boxes","year":2019,"link":"https://www.objects365.org/overview.html","license":"CC BY 4.0","licenseType":"commercial","licenseText":"Attribution 4.0 International (CC BY 4.0) - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon, even commercialy,\nUnder the following terms:\nAttribution - you must give approprate credit."},{"id":167,"name":"FaceForensics Benchmark","type":"cv","description":"FaceForensics++ is a forensics dataset consisting of 1000 original video sequences that have been manipulated with four automated face manipulation methods: Deepfakes, Face2Face, FaceSwap and NeuralTextures. The data has been sourced from 977 youtube videos and all videos contain a trackable mostly frontal face without occlusions which enables automated tampering methods to generate realistic forgeries. As we provide binary masks the data can be used for image and video classification as well as segmentation. In addition, we provide 1000 Deepfakes models to generate and augment new data.","year":2019,"link":"http://kaldir.vc.in.tum.de/faceforensics_benchmark/","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","arxiv":1901.08971},{"id":166,"name":"TabFact: A Large-scale Dataset for Table-based Fact Verification","type":"nlp","description":"We introduce a large-scale dataset called TabFact(website: https://tabfact.github.io/), which consists of 117,854 manually annotated statements with regard to 16,573 Wikipedia tables, their relations are classified as ENTAILED and REFUTED.","year":2019,"link":"https://tabfact.github.io/","license":"MIT","licenseType":"commercial","licenseText":"MIT - You are free to: use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the work. Under the following terms: the work is provided \"as is\", you must include copyright and the license in all copies or substantial uses of the work.","arxiv":1909.02164},{"id":165,"name":"CURE-TSD","type":"cv","description":"CURE-TSD: Challenging Unreal and Real Environments for Traffic Sign Detection. The video sequences in the CURE-TSD dataset are grouped into two classes: real data and unreal data. Real data correspond to processed versions of sequences acquired from real world. Unreal data corresponds to synthesized sequences generated in a virtual environment. There are 49 real sequences and 49 unreal sequences that do not include any specific challenge. We have 34 training videos and 15 test videos in both real and unreal sequences that are challenge-free. There are 300 frames in each video sequence. There are 49 challenge-free real video sequences processed with 12 different types of effects and 5 different challenge levels. Moreover, there are 49 synthesized video sequences processed with 11 different types of effects and 5 different challenge levels. In total, there are 5,733 video sequences, which include around 1.72 million frames.","year":2019,"link":"https://github.com/olivesgatech/CURE-TSD","license":"Not found","licenseType":"non-commercial","licenseText":"License information not found","arxiv":1908.11262},{"id":164,"name":"DublinCity: Annotated LiDAR Point Cloud ","type":"cv","description":"Urban Modelling Group at University College Dublin (UCD) captured major area of Dublin city centre (i.e. around 5.6 km^2 including partially covered areas) was scanned via an ALS device which was carried out by helicopter in 2015. However, the actual focused area was around 2 km^2 which contains the most densest LiDAR point cloud and imagery dataset. The flight altitude was mostly around 300m and the total journey was performed in 41 flight path strips. The datasets is made up of over 260 million laser scanning points labelled into 100,000 objects.","year":2019,"link":"https://v-sense.scss.tcd.ie/DublinCity/","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","arxiv":1909.03613},{"id":163,"name":"A*3D","type":"self-driving","description":"A*3D dataset is a step forward to make autonomous driving safer for pedestrians and the public in the real world.\n230K human-labeled 3D object annotations in 39,179 LiDAR point cloud frames and corresponding frontal-facing RGB images.\nCaptured at different times (day, night) and weathers (sun, cloud, rain).","year":2019,"link":"https://github.com/I2RDL2/ASTAR-3D","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","arxiv":1909.07541},{"id":162,"name":"Google Coached Conversational Preference Elicitation","type":"nlp","description":"A dataset consisting of 502 dialogs with 12,000 annotated utterances between a user and an assistant discussing movie preferences in natural language. It was collected using a Wizard-of-Oz methodology between two paid crowd-workers, where one worker plays the role of an 'assistant', while the other plays the role of a 'user'.","year":2019,"link":"https://ai.google/tools/datasets/coached-conversational-preference-elicitation","license":"CC-BY-SA 4.0","licenseType":"commercial","licenseText":"Attribution-ShareAlike 4.0 International - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon, even commercialy,\nUnder the following terms:\nAttribution - you must give approprate credit,\nShareAlike - if you make changes, you must distribute your contributions.","pdf":"https://ai.google/research/pubs/pub48414.pdf"},{"id":161,"name":"QMUL-OpenLogo","type":"cv","description":"QMUL-OpenLogo contains 27,083 images from 352 logo classes, built by aggregating and refining 7 existing datasets and establishing an open logo detection evaluation protocol. ","year":2019,"link":"https://qmul-openlogo.github.io/","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","arxiv":1807.01964},{"id":160,"name":"Taskmaster-1 ","type":"nlp","description":"The dataset consists of 13,215 task-based dialogs, including 5,507 spoken and 7,708 written dialogs created with two distinct procedures. Each conversation falls into one of six domains: ordering pizza, creating auto repair appointments, setting up ride service, ordering movie tickets, ordering coffee drinks and making restaurant reservations.","year":2019,"link":"https://ai.google/tools/datasets/taskmaster-1","license":"CC-BY-SA 4.0","licenseType":"commercial","licenseText":"Attribution-ShareAlike 4.0 International - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon, even commercialy,\nUnder the following terms:\nAttribution - you must give approprate credit,\nShareAlike - if you make changes, you must distribute your contributions.","pdf":"https://ai.google/research/pubs/pub48484.pdf"},{"id":159,"name":"Waymo Open Dataset","type":"self-driving","description":"The Waymo Open Dataset is comprised of high resolution sensor data collected by Waymo self-driving cars in a wide variety of conditions. We are releasing this dataset publicly to aid the research community in making advancements in machine perception and self-driving technology. The Waymo Open Dataset currently contains lidar and camera data from 1,000 segments (20s each): 1,000 segments of 20s each, collected at 10Hz (200,000 frames) in diverse geographies and conditions, Labels for 4 object classes - Vehicles, Pedestrians, Cyclists, Signs, 12M 3D bounding box labels with tracking IDs on lidar data, 1.2M 2D bounding box labels with tracking IDs on camera data...","year":2019,"link":"https://waymo.com/open/","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited."},{"id":158,"name":"Lyft Level 5","type":"self-driving","description":"A comprehensive, large-scale dataset featuring the raw sensor camera and LiDAR inputs as perceived by a fleet of multiple, high-end, autonomous vehicles in a bounded geographic area. This dataset also includes high quality, human-labelled 3D bounding boxes of traffic agents, an underlying HD spatial semantic map. Contains over 55,000 human-labeled 3D annotated frames; data from 7 cameras and up to 3 lidars; a drivable surface map; and, an underlying HD spatial semantic map. A semantic map provides context to reason about the presence and motion of the agents in the scenes. The provided map has over 4000 lane segments (2000 road segment lanes and about 2000 junction lanes) , 197 pedestrian crosswalks, 60 stop signs, 54 parking zones, 8 speed bumps, 11 speed humps.","year":2019,"link":"https://level5.lyft.com/dataset/","license":"CC-BY-NC-SA 4.0","licenseType":"non-commercial","licenseText":"Attribution-NonCommercial-ShareAlike International - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon,\nUnder the following terms:\nAttribution - you must give approprate credit,\nNonCommercial - you may not use the material for commercial purposes,\nShareAlike - if you make changes, you must distribute your contributions."},{"id":157,"name":"OpenWebText","type":"nlp","description":"Open WebText – an open source effort to reproduce OpenAI’s WebText dataset. This distribution was created by Aaron Gokaslan and Vanya Cohen of Brown University. Dataset was created by extracting all Reddit post urls from the Reddit submissions dataset. These links were deduplicated, filtered to exclude non-html content, and then shuffled randomly. The links were then distributed to several machines in parallel for download, and all web pages were extracted using the newspaper python package. Documents were hashed into sets of 5-grams and all documents that had a similarity threshold of greater than 0.5 were removed. The the remaining documents were tokenized, and documents with fewer than 128 tokens were removed. This left 38GB of text data (40GB using SI units) from 8,013,769 documents.","year":2019,"link":"https://skylion007.github.io/OpenWebTextCorpus/","license":"Various","licenseType":"non-commercial","licenseText":"Dataset packaging is licensed under CC-0 but contains content that can have a different license, check the dataset download for more details.","torrent":"https://academictorrents.com/details/36c39b25657ce1639ccec0a91cf242b42e1f01db"},{"id":156,"name":"LVIS","type":"cv","description":"LVIS is a new dataset for long tail object instance segmentation. 1000+ Categories: found by data-driven object discovery in 164k images. More than 2.2 million high quality instance segmentation masks.","year":2019,"link":"https://www.lvisdataset.org/","license":"CC BY 4.0","licenseType":"commercial","licenseText":"Attribution 4.0 International (CC BY 4.0) - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon, even commercialy,\nUnder the following terms:\nAttribution - you must give approprate credit.","arxiv":1908.03195},{"id":155,"name":"CODAH","type":"qa","description":"CODAH is an adversarially-constructed evaluation dataset with 2.8k questions for testing common sense. CODAH forms a challenging extension to the SWAG dataset, which tests commonsense knowledge using sentence-completion questions that describe situations observed in video.","year":2019,"link":"https://github.com/Websail-NU/CODAH","license":"Not found","licenseType":"non-commercial","licenseText":"License information not found","arxiv":"1904.04365v4"},{"id":154,"name":"TACO (Trash Annotations in Context)","type":"cv","description":"Taco is an open image dataset of waste in the wild. It contains photos of litter taken under diverse environments, from tropical beaches to London streets. These images are manually labeled and segmented according to a hierarchical taxonomy to train and evaluate object detection algorithms.","year":2019,"link":"http://tacodataset.org/","license":"CC BY 4.0","licenseType":"commercial","licenseText":"Attribution 4.0 International (CC BY 4.0) - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon, even commercialy,\nUnder the following terms:\nAttribution - you must give approprate credit."},{"id":153,"name":"Mapillary Traffic Sign Dataset","type":"cv","description":"A diverse street-level imagery dataset with bounding box annotations for detecting and classifying traffic signs around the world. 100,000 high-resolution images from all over the world with bounding box annotations of over 300 classes of traffic signs. The fully annotated set of the Mapillary Traffic Sign Dataset (MTSD) includes a total of 52,453 images with 257,543 traffic sign bounding boxes. The additional, partially annotated dataset contains 47,547 images with more than 80,000 signs that are automatically labeled with correspondence information from 3D reconstruction.","year":2019,"link":"https://www.mapillary.com/dataset/trafficsign","license":"Research and commercial","licenseType":"commercial","licenseText":"Research and commercial licenses available."},{"id":152,"name":"Argoverse","type":"self-driving","description":"Argoverse is a research collection with three distinct types of data. The first is a dataset with sensor data from 113 scenes observed by our fleet, with 3D tracking annotations on all objects. The second is a dataset of 300,000-plus scenarios observed by our fleet, wherein each scenario contains motion trajectories of all observed objects. The third is a set of HD maps of several neighborhoods in Pittsburgh and Miami, to add rich context for all of the data mentioned above. ","year":2019,"link":"https://www.argoverse.org/","license":"CC-BY-NC-SA 4.0","licenseType":"non-commercial","licenseText":"Attribution-NonCommercial-ShareAlike International - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon,\nUnder the following terms:\nAttribution - you must give approprate credit,\nNonCommercial - you may not use the material for commercial purposes,\nShareAlike - if you make changes, you must distribute your contributions.","pdf":"http://openaccess.thecvf.com/content_CVPR_2019/papers/Chang_Argoverse_3D_Tracking_and_Forecasting_With_Rich_Maps_CVPR_2019_paper.pdf"},{"id":151,"name":"Social-IQ","type":"qa","description":"The dataset contains rigorously annotated and validated videos, questions and answers, as well as annotations for the complexity level of each question and answer. Social-IQ brings novel challenges to the field of artificial intelligence which sparks future research in social intelligence modeling, visual reasoning, and multimodal question answering. 1,250 videos, 7,500 questions, 33,000 correct answers, 22,500 incorrect answers.","year":2019,"link":"https://www.thesocialiq.com/","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","pdf":"http://openaccess.thecvf.com/content_CVPR_2019/papers/Zadeh_Social-IQ_A_Question_Answering_Benchmark_for_Artificial_Social_Intelligence_CVPR_2019_paper.pdf"},{"id":150,"name":"DROP","type":"qa","description":"DROP is a crowdsourced, adversarially-created, 96k-question benchmark, in which a system must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting). These operations require a much more comprehensive understanding of the content of paragraphs than what was necessary for prior datasets.","year":2019,"link":"https://allennlp.org/drop","license":"CC-BY-SA 4.0","licenseType":"commercial","licenseText":"Attribution-ShareAlike 4.0 International - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon, even commercialy,\nUnder the following terms:\nAttribution - you must give approprate credit,\nShareAlike - if you make changes, you must distribute your contributions.","pdf":"https://pdfs.semanticscholar.org/dda6/fb309f62e2557a071522354d8c2c897a2805.pdf"},{"id":149,"name":"SuperGLUE benchmark","type":"nlp","description":"SuperGLUE, a new benchmark styled after GLUE with a new set of more difficult language understanding tasks, improved resources, and a new public leaderboard. Full citation list of the datasets contained: {The CommitmentBank}: Investigating projection in naturally occurring discourse, Choice of plausible alternatives: An evaluation of commonsense causal reasoning, Looking beyond the surface: A challenge set for reading comprehension over multiple sentences, The {PASCAL} recognising textual entailment challenge, The second {PASCAL} recognising textual entailment challenge, The third {PASCAL} recognizing textual entailment challenge, The Fifth {PASCAL} Recognizing Textual Entailment Challenge, {WiC}: The Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations, The {W}inograd schema challenge.","year":2019,"link":"https://super.gluebenchmark.com","license":"Various","licenseType":"non-commercial","licenseText":"The dataset contains data from several sources, check the links on the website for individual licenses","arxiv":1905.00537},{"id":148,"name":"Human Activity Knowledge Engine (HAKE)","type":"cv","description":"Human Activity Knowledge Engine (HAKE) aims at promoting the human activity/action understanding. As a large-scale knowledge base, HAKE is built upon existing activity datasets, and supplies human instance action labels and corresponding body part level atomic action labels (Part States). Dataset contains 104 K+ images, 154 activity classes, 677 K+ human instances.","year":2019,"link":"http://hake-mvig.cn/home/","license":"Not found","licenseType":"non-commercial","licenseText":"License information not found","arxiv":1904.06539},{"id":147,"name":"PedX","type":"cv","description":"PedX is a large-scale multi-modal collection of pedestrians at complex urban intersections. The dataset provides high-resolution stereo images and LiDAR data with manual 2D and automatic 3D annotations. The data was captured using two pairs of stereo cameras and four Velodyne LiDAR sensors.","year":2019,"link":"http://pedx.io/","license":"MIT","licenseType":"commercial","licenseText":"MIT - You are free to: use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the work. Under the following terms: the work is provided \"as is\", you must include copyright and the license in all copies or substantial uses of the work.","arxiv":1809.03605},{"id":146,"name":"Replica","type":"cv","description":"The Replica Dataset is a dataset of high quality reconstructions of a variety of indoor spaces. Each reconstruction has clean dense geometry, high resolution and high dynamic range textures, glass and mirror surface information, planar segmentation as well as semantic class and instance segmentation.","year":2019,"link":"https://github.com/facebookresearch/Replica-Dataset","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","arxiv":1906.05797},{"id":145,"name":"VERI-Wild","type":"cv","description":"A large-scale vehicle ReID dataset in the wild (VERI-Wild) is captured from a large CCTV surveillance system consisting of 174 cameras across one month (30× 24h) under unconstrained scenarios. The cameras are distributed in a large urban district of more than 200km2. After data cleaning and annotation, 416,314 vehicle images of 40,671 identities are collected.","year":2019,"link":"https://github.com/PKU-IMRE/VERI-Wild","license":"Not found","licenseType":"non-commercial","licenseText":"License information not found","pdf":"http://openaccess.thecvf.com/content_CVPR_2019/papers/Lou_VERI-Wild_A_Large_Dataset_and_a_New_Method_for_Vehicle_CVPR_2019_paper.pdf"},{"id":144,"name":"Semantic Drone Dataset","type":"cv","description":"The Semantic Drone Dataset focuses on semantic understanding of urban scenes for increasing the safety of autonomous drone flight and landing procedures. The imagery depicts  more than 20 houses from nadir (bird's eye) view acquired at an altitude of 5 to 30 meters above ground. A high resolution camera was used to acquire images at a size of 6000x4000px (24Mpx). The training set contains 400 publicly available images and the test set is made up of 200 private images.","year":2019,"link":"http://dronedataset.icg.tugraz.at/","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited."},{"id":143,"name":"Google Landmarks V2","type":"cv","description":"This is the second version of the Google Landmarks dataset, which contains images annotated with labels representing human-made and natural landmarks. The dataset can be used for landmark recognition and retrieval experiments. This version of the dataset contains approximately 5 million images, split into 3 sets of images: train, index and test.","year":2019,"link":"https://github.com/cvdfoundation/google-landmark","license":"CC BY 4.0","licenseType":"commercial","licenseText":"Attribution 4.0 International (CC BY 4.0) - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon, even commercialy,\nUnder the following terms:\nAttribution - you must give approprate credit."},{"id":142,"name":"The Boxy Vehicles Dataset","type":"cv","description":"A large dataset of almost two million annotated vehicles\nfor training and evaluating object detection methods. 200,000 images. 1,990,000 annotated vehicles. 5 Megapixel resolution.","year":2019,"link":"https://boxy-dataset.com/boxy/","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","pdf":"https://boxy-dataset.com/static/boxy/boxy_preview.pdf"},{"id":141,"name":"The Unsupervised Labeled Lane Markers Dataset","type":"cv","description":"The Unsupervised Llamas dataset was annotated by creating high definition maps for automated driving including lane markers based on Lidar. The automated vehicle can be localized against these maps and the lane markers are projected into the camera frame. The 3D projection is optimized by minimizing the difference between already detected markers in the image and projected ones. Further improvements can likely be achieved by using better detectors, optimizing difference metrics, and adding some temporal consistency. \n\nOver 100,000 annotated images.\nAnnotations of over 100 meters.\nResolution of 1276 x 717 pixels.","year":2019,"link":"https://unsupervised-llamas.com/llamas/","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","pdf":"https://unsupervised-llamas.com/static/llamas/llamas_preview.pdf"},{"id":140,"name":"Google Open Images V5","type":"cv","Stats":"9 million images","description":"Open Images is a dataset of ~9M images annotated with image-level labels, object bounding boxes, object segmentation masks, and visual relationships. It contains a total of 16M bounding boxes for 600 object classes on 1.9M images, making it the largest existing dataset with object location annotations. Open Images V5 features segmentation masks for 2.8 million object instances in 350 categories. Unlike bounding-boxes, which only identify regions in which an object is located, segmentation masks mark the outline of objects, characterizing their spatial extent to a much higher level of detail.","year":2019,"link":"https://storage.googleapis.com/openimages/web/index.html","license":"CC BY 4.0","licenseType":"commercial","licenseText":"Attribution 4.0 International (CC BY 4.0) - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon, even commercialy,\nUnder the following terms:\nAttribution - you must give approprate credit.","arxiv":1811.00982,"torrent":"https://academictorrents.com/details/9e9194e21ce045deee8d811481b4cd676b20b06b"},{"id":139,"name":"Mid-Air","type":"cv","description":"Mid-Air is a multi-modal synthetic dataset for low altitude drone flights in unstructured environments. It contains synchronized data captured by multiple sensors for a total of 54 trajectories and more than 420k video frames simulated in various climate conditions.","year":2019,"link":"https://midair.ulg.ac.be/","license":"CC-BY-NC-SA 4.0","licenseType":"non-commercial","licenseText":"Attribution-NonCommercial-ShareAlike International - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon,\nUnder the following terms:\nAttribution - you must give approprate credit,\nNonCommercial - you may not use the material for commercial purposes,\nShareAlike - if you make changes, you must distribute your contributions.","pdf":"https://orbi.uliege.be/bitstream/2268/234665/3/Fonder2019MidAir.pdf"},{"id":138,"name":"CQ500","type":"medical","description":"We have made the CQ500 dataset of 491 scans with 193,317 slices publicly available so that others can compare and build upon the results we have achieved in the paper. We provide anonymized dicoms for all the 491 scans and the corresponding radiologists' reads.\n\nThe scans in the CQ500 dataset were generously provided by Centre for Advanced Research in Imaging, Neurosciences and Genomics(CARING), New Delhi, IN. The reads were done by three radiologists with an experience of 8, 12 and 20 years in cranial CT interpretation respectively.","year":2019,"link":"http://headctstudy.qure.ai/dataset","license":"CC-BY-NC-SA 4.0","licenseType":"non-commercial","licenseText":"Attribution-NonCommercial-ShareAlike International - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon,\nUnder the following terms:\nAttribution - you must give approprate credit,\nNonCommercial - you may not use the material for commercial purposes,\nShareAlike - if you make changes, you must distribute your contributions.","arxiv":1803.05854},{"id":137,"name":"TextVQA","type":"qa","description":"TextVQA requires models to read and reason about text in images to answer questions about them. Specifically, models need to incorporate a new modality of text present in the images and reason over it to answer TextVQA questions. Dataset contains 28,408 images from OpenImages, 45,336 questions, 453,360 ground truth answers.","year":2019,"link":"https://textvqa.org/","license":"CC BY 4.0","licenseType":"commercial","licenseText":"Attribution 4.0 International (CC BY 4.0) - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon, even commercialy,\nUnder the following terms:\nAttribution - you must give approprate credit.","arxiv":1904.0892},{"id":136,"name":"MRNet","type":"medical","description":"\nThe MRNet dataset consists of 1,370 knee MRI exams performed at Stanford University Medical Center. The dataset contains 1,104 (80.6%) abnormal exams, with 319 (23.3%) ACL tears and 508 (37.1%) meniscal tears; labels were obtained through manual extraction from clinical reports.","year":2019,"link":"https://stanfordmlgroup.github.io/competitions/mrnet/","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","pdf":"https://stanfordmlgroup.github.io/projects/mrnet/"},{"id":135,"name":"DeepFashion2","type":"cv","description":"It is a versatile benchmark of four tasks including clothes detection, pose estimation, segmentation, and retrieval. It has 801K clothing items where each item has rich annotations such as style, scale, viewpoint, occlusion, bounding box, dense landmarks and masks. There are also 873K Commercial-Consumer clothes pairs.","year":2019,"link":"https://github.com/switchablenorms/DeepFashion2","license":"Not found","licenseType":"non-commercial","licenseText":"License information not found","arxiv":1901.07973},{"id":134,"name":"iMat Fashion 2019","type":"cv","description":"While early work in computer vision addressed related clothing recognition tasks, these are not designed with fashion insiders’ needs in mind, possibly due to the research gap in fashion design and computer vision. To address this, we first propose a fashion taxonomy built by fashion experts, informed by product description from the internet. To capture the complex structure of fashion objects and ambiguity in descriptions obtained from crawling the web, our standardized taxonomy contains 46 apparel objects (27 main apparel items and 19 apparel parts), and 92 related fine-grained attributes. Secondly, a total of around 50K clothing images (10K with both segmentation and fine-grained attributes, 40K with apparel instance segmentation) in daily-life, celebrity events, and online shopping are labeled by both domain experts and crowd workers for fine-grained segmentation.","year":2019,"link":"https://github.com/visipedia/imat_comp","license":"Not found","licenseType":"non-commercial","licenseText":"License information not found"},{"id":133,"name":"EuroCity Persons Dataset","type":"cv","description":"With over 238,200 person instances manually labeled in over 47,300 images, EuroCity Persons is nearly one order of magnitude larger than person datasets used previously for benchmarking. Diversity is gained by recording this dataset throughout Europe.\n\nAll objects were annotated with tight bounding boxes delineating their full extent. If objects were partly occluded, their full extents were estimated (this is useful for later processing steps such as tracking) and the level of occlusion was annotated.","year":2019,"link":"https://eurocity-dataset.tudelft.nl/","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","pdf":"http://intelligent-vehicles.org/wp-content/uploads/2019/04/braun2019tpami_eurocity_persons.pdf"},{"id":132,"name":"Mozilla Common Voice","type":"audio","Stats":"1400 hours","description":"Mozilla crowdsources the largest dataset of human voices available for use, including 18 different languages, adding up to almost 1,400 hours of recorded voice data from more than 42,000 contributors.","year":2019,"link":"https://voice.mozilla.org/","license":"CC-0","licenseType":"commercial","licenseText":"CC-0 - No Copyright"},{"id":131,"name":"IBM Diversity in Faces Dataset","type":"cv","Stats":"1M images","description":"The Diversity in Faces(DiF)is a large and diverse dataset that seeks to advance the study of fairness and accuracy in facial recognition technology. The first of its kind available to the global research community, DiF provides a dataset of annotations of 1 million human facial images.","year":2019,"link":"https://www.research.ibm.com/artificial-intelligence/trusted-ai/diversity-in-faces/","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","arxiv":1901.10436},{"id":130,"name":"Google Natural Questions","type":"qa","Stats":"300k questions","description":"Natural Questions (NQ), a new, large-scale corpus for training and evaluating open-domain question answering systems, and the first to replicate the end-to-end process in which people find answers to questions. NQ is large, consisting of 300,000 naturally occurring questions, along with human annotated answers from Wikipedia pages, to be used in training QA systems. We have additionally included 16,000 examples where answers (to the same questions) are provided by 5 different annotators, useful for evaluating the performance of the learned QA systems.","year":2019,"link":"https://ai.google.com/research/NaturalQuestions","license":"CC-BY-SA 3.0","licenseType":"commercial","licenseText":"Attribution-ShareAlike International - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon, even commercialy,\nUnder the following terms:\nAttribution - you must give approprate credit,\nShareAlike - if you make changes, you must distribute your contributions.","pdf":"https://ai.google/research/pubs/pub47761.pdf"},{"id":129,"name":"Large Scale Chinese Corpus for NLP","type":"nlp","description":"Dataset contents: 1. Wikipedia (wiki2019zh), 1 million well-formed Chinese entries\n2. News corpus (news2016zh), 2.5 million news, including keywords, description\n3. Encyclopedia question and answer (baike2018qa), 1.5 million questions and answers with question types\n4. Community Q&A json version (webtext2019zh), 4.1 million high quality community Q&A, suitable for training oversized models\n5. Translation corpus (translation2019zh), 5.2 million pairs of Chinese and English sentences","year":2019,"link":"https://github.com/brightmart/nlp_chinese_corpus","license":"Various","licenseType":"non-commercial","licenseText":"The dataset contains data from several sources, check the links on the website for individual licenses"},{"id":128,"name":"ActivityNet-QA","type":"qa","description":"The ActivityNet-QA dataset contains 58,000 human-annotated QA pairs on 5,800 videos derived from the popular ActivityNet dataset. The dataset provides a benckmark for testing the performance of VideoQA models on long-term spatio-temporal reasoning.","year":2019,"link":"https://github.com/MILVLG/activitynet-qa","license":"MIT","licenseType":"commercial","licenseText":"MIT - You are free to: use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the work. Under the following terms: the work is provided \"as is\", you must include copyright and the license in all copies or substantial uses of the work.","arxiv":1906.02467},{"id":127,"name":"Ten Thousand German News Articles Dataset","type":"nlp","description":"The 10kGNAD dataset is intended to solve part of this problem as the first german topic classification dataset. It consists of 10273 german language news articles from an austrian online newspaper categorized into nine topics. These articles are a till now unused part of the One Million Posts Corpus.\n\n","year":2019,"link":"https://tblock.github.io/10kGNAD/","license":"CC-BY-NC-SA 4.0","licenseType":"non-commercial","licenseText":"Attribution-NonCommercial-ShareAlike International - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon,\nUnder the following terms:\nAttribution - you must give approprate credit,\nNonCommercial - you may not use the material for commercial purposes,\nShareAlike - if you make changes, you must distribute your contributions."},{"id":126,"name":"Facebook BISON","type":"cv","description":"Facebook BISON (Binary Image Selection) dataset complements the COCO Captions dataset. BISON-COCO is not a training dataset, but rather an evaluation dataset that can be used to test existing models’ ability for pairing visual content with appropriate text descriptions.","year":2019,"link":"http://hexianghu.com/bison/","license":"Not found","licenseType":"non-commercial","licenseText":"License information not found","arxiv":1901.06595},{"id":125,"name":"MIMIC-CXR","type":"medical","description":"MIMIC-CXR is a large, publicly-available database comprising of de-identified chest radiographs from patients admitted to the Beth Israel Deaconess Medical Center between 2011 and 2016. The dataset contains 371,920 chest x-rays associated with 227,943 imaging studies. Each imaging study can pertain to one or more images, but most often are associated with two images: a frontal view and a lateral view. Images are provided with 14 labels derived from a natural language processing tool applied to the corresponding free-text radiology reports. ","year":2019,"link":"https://www.physionet.org/physiobank/database/mimiccxr/","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","arxiv":1901.07042},{"id":124,"name":"CheXpert","type":"medical","description":"CheXpert is a large public dataset for chest radiograph interpretation, consisting of 224,316 chest radiographs of 65,240 patients.","year":2019,"link":"https://stanfordmlgroup.github.io/competitions/chexpert/","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","arxiv":1901.07031},{"id":123,"name":"GQA","type":"qa","description":"The dataset consists of 22M questions about various day-to-day images. Each image is associated with a scene graph of the image's objects, attributes and relations, a new cleaner version based on Visual Genome.","year":2019,"link":"https://cs.stanford.edu/people/dorarad/gqa/","license":"CC BY 4.0","licenseType":"commercial","licenseText":"Attribution 4.0 International (CC BY 4.0) - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon, even commercialy,\nUnder the following terms:\nAttribution - you must give approprate credit.","pdf":"https://cs.stanford.edu/people/dorarad/gqa/gqaPaper.pdf"},{"id":122,"name":"Spacecraft Pose Estimation Dataset (SPEED)","type":"cv","description":"SPEED consists of synthetic as well as actual camera images of a mock-up of the Tango spacecraft from the PRISMA mission. The synthetic images are created by fusing OpenGL-based renderings of the spacecraft’s3D model with actual images of the Earth captured by the Himawari-8 meteorolog-ical satellite. Dataset contains over 12,000 images with a resolution of 1920×1200 pixels.","year":2019,"link":"https://kelvins.esa.int/satellite-pose-estimation-challenge/home/","license":"CC-BY-NC-SA 3.0","licenseType":"non-commercial","licenseText":"Attribution-NonCommercial-ShareAlike International - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon,\nUnder the following terms:\nAttribution - you must give approprate credit,\nNonCommercial - you may not use the material for commercial purposes,\nShareAlike - if you make changes, you must distribute your contributions.","pdf":"https://damicos.people.stanford.edu/sites/g/files/sbiybj2226/f/asm2019_sharmadamico_final.pdf"},{"id":121,"name":"Baidu Large-scale Street View Text with Partial Labeling (LSVT)","type":"cv","description":"A new large-scale scene text dataset, namely Large-scale Street View Text with Partial Labeling (LSVT), with 30,000 training data and 20,000 testing images in full annotations, and 400,000 training data in weak annotations, which are referred to as partial labels.","year":2019,"link":"https://ai.baidu.com/broad/subordinate?dataset=lsvt","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited."},{"id":120,"name":"NVIDIA Flickr-Faces-HQ Dataset","type":"cv","description":"Flickr-Faces-HQ (FFHQ) is a high-quality image dataset of human faces, originally created as a benchmark for generative adversarial networks (GAN). The dataset consists of 70,000 high-quality PNG images at 1024×1024 resolution and contains considerable variation in terms of age, ethnicity and image background. It also has good coverage of accessories such as eyeglasses, sunglasses, hats, etc.","year":2019,"link":"https://github.com/NVlabs/ffhq-dataset","license":"CC-BY-NC-SA 4.0","licenseType":"non-commercial","licenseText":"Attribution-NonCommercial-ShareAlike International - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon,\nUnder the following terms:\nAttribution - you must give approprate credit,\nNonCommercial - you may not use the material for commercial purposes,\nShareAlike - if you make changes, you must distribute your contributions.","arxiv":1812.04948},{"id":119,"name":"Danbooru2018","type":"cv","description":"Danbooru2018 is a large-scale anime image database with 3.33m+ images annotated with 99.7m+ tags; It can be useful for machine learning purposes such as image recognition and generation.","year":2019,"link":"https://www.gwern.net/Danbooru2018","license":"Not found","licenseType":"non-commercial","licenseText":"License information not found","torrent":"https://www.gwern.net/Danbooru2018#torrent"},{"id":118,"name":"Flickr1024","type":"cv","description":"Flickr1024 is a large stereo dataset, which consists of 1024 high-quality images pairs and covers diverse senarios. This dataset can be employed for stereo image super-resolution (SR). ","year":2019,"link":"https://yingqianwang.github.io/Flickr1024/","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","arxiv":1903.06332},{"id":117,"name":"Question Answering in Context (QuAC)","type":"qa","description":"QuAC, a dataset for Question Answering in Context that contains 14K information-seeking QA dialogs (100K questions in total). Question Answering in Context is a dataset for modeling, understanding, and participating in information seeking dialog. Data instances consist of an interactive dialog between two crowd workers: (1) a student who poses a sequence of freeform questions to learn as much as possible about a hidden Wikipedia text, and (2) a teacher who answers the questions by providing short excerpts (spans) from the text. QuAC introduces challenges not found in existing machine comprehension datasets: its questions are often more open-ended, unanswerable, or only meaningful within the dialog context.","year":2018,"link":"https://quac.ai/","license":"CC-BY-SA 4.0","licenseType":"commercial","licenseText":"Attribution-ShareAlike 4.0 International - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon, even commercialy,\nUnder the following terms:\nAttribution - you must give approprate credit,\nShareAlike - if you make changes, you must distribute your contributions.","arxiv":1808.07036},{"id":116,"name":"Vehicle-1M","type":"cv","description":"The Vehicle-1M dataset is constructed by National Laboratory of Pattern Recognition, Institute of Automation, University of Chinese Academy of Sciences (NLPR, CASIA). This dataset involves vehicle images captured across day and night, from head or rear, by multiple surveillance cameras installed in several cities in China. There are totally 936,051 images from 55,527 vehicles and 400 vehicle models in the dataset. Each image is attached with a vehicle ID label denoting its identity in real world as well as a vehicle model label indicating the make, model and year of the vehicle(i.e. \"Audi-A6-2013\").","year":2018,"link":"http://www.nlpr.ia.ac.cn/iva/homepage/jqwang/Vehicle1M.htm","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","pdf":"https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16206/16270"},{"id":115,"name":"Tencent AI Lab Embedding Corpus for Chinese Words and Phrases","type":"nlp","description":"This corpus provides 200-dimension vector representations, a.k.a. embeddings, for over 8 million Chinese words and phrases, which are pre-trained on large-scale high-quality data. These vectors, capturing semantic meanings for Chinese words and phrases, can be widely applied in many downstream Chinese processing tasks (e.g., named entity recognition and text classification) and in further research.","year":2018,"link":"https://ai.tencent.com/ailab/nlp/embedding.html","license":"CC BY 3.0","licenseType":"commercial","licenseText":"Attribution 3.0 International (CC BY 3.0) - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon, even commercialy,\nUnder the following terms:\nAttribution - you must give approprate credit.","pdf":"http://aclweb.org/anthology/N18-2028"},{"id":114,"name":"GOT-10k (Generic Object Tracking Benchmark)","type":"cv","description":"A large, high-diversity, one-shot database for generic object tracking in the wild. The dataset contains more than 10,000 video segments of real-world moving objects and over 1.5 million manually labeled bounding boxes. The dataset is backboned by WordNet and it covers a majority of 560+ classes of real-world moving objects and 80+ classes of motion patterns.The test set embodies 84 object classes and 32 motion classes with only 180 video segments, allowing for efficient evaluation.","year":2018,"link":"http://got-10k.aitestunion.com/","license":"Not found","licenseType":"non-commercial","licenseText":"License information not found","arxiv":1810.11981},{"id":113,"name":"OpenBookQA","type":"qa","description":"OpenBookQA, modeled after open book exams for assessing human understanding of a subject. The open book that comes with our questions is a set of 1329 elementary level science facts. Roughly 6000 questions probe an understanding of these facts and their application to novel situations.","year":2018,"link":"https://github.com/allenai/OpenBookQA","license":"Apache","licenseType":"commercial","licenseText":"Apache License 2.0 - \n\nA permissive license whose main conditions require preservation of copyright and license notices. Contributors provide an express grant of patent rights. Licensed works, modifications, and larger works may be distributed under different terms and without source code.","arxiv":1809.02789},{"id":112,"name":"TrackingNet","type":"cv","description":"A Large-Scale Dataset and Benchmark for Object Tracking in the Wild. > 30K Video Sequences, > 14M Bounding Boxes. Diversity ensured by Youtube.","year":2018,"link":"https://tracking-net.org","license":"Not found","licenseType":"non-commercial","licenseText":"License information not found","pdf":"https://ivul.kaust.edu.sa/Documents/Publications/2018/TrackingNet%20A%20Large%20Scale%20Dataset%20and%20Benchmark%20for%20Object%20Tracking%20in%20the%20Wild.pdf"},{"id":111,"name":"VCR (Visual Commonsense Reasoning)","type":"qa","description":"Visual Commonsense Reasoning (VCR) is a new task and large-scale dataset for cognition-level visual understanding. It contains:\n290k multiple choice questions\n290k correct answers and rationales: one per question\n110k images\nCounterfactual choices obtained with minimal bias, via our new Adversarial Matching approach\nAnswers are 7.5 words on average; rationales are 16 words.\nHigh human agreement (>90%)\nScaffolded on top of 80 object categories from COCO","year":2018,"link":"https://visualcommonsense.com/","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","arxiv":1811.1083},{"id":110,"name":"Youtube-8M 2018","type":"cv","Stats":"7 Million\nVideo URLs\t450,000 \nHours of Video\t3.2 Billion\nAudio/Visual Features\n4716\nClasses","description":"YouTube-8M is a large-scale labeled video dataset that consists of millions of YouTube video IDs and associated labels from a diverse vocabulary of 4700+ visual entities. It comes with precomputed state-of-the-art audio-visual features from billions of frames and audio segments, designed to fit on a single hard disk.","year":2018,"link":"https://research.google.com/youtube8m/index.html","license":"CC BY 4.0","licenseType":"commercial","licenseText":"Attribution 4.0 International (CC BY 4.0) - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon, even commercialy,\nUnder the following terms:\nAttribution - you must give approprate credit.","arxiv":1609.08675},{"id":109,"name":"CMU-MOSEI","type":"nlp","description":"CMU-MOSEI is the largest in-the-wild dataset of multimodal sentiment analysis and emotion recognition in NLP. It consists of 23,500 sentences from more than 1000 youtube identities and 200 topics. Sentences are annotated for sentiment and emotion intensity. The dataset also contains unsupervised data (unannotated sentences).","year":2018,"link":"http://multicomp.cs.cmu.edu/resources/cmu-mosei-dataset/","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","pdf":"https://www.aclweb.org/anthology/P18-1208"},{"id":108,"name":"RecipeQA","type":"qa","description":"RecipeQA is a dataset for multimodal comprehension of cooking recipes. It consists of over 36K question-answer pairs automatically generated from approximately 20K unique recipes with step-by-step instructions and images. Each question in RecipeQA involves multiple modalities such as titles, descriptions or images, and working towards an answer requires (i) joint understanding of images and text, (ii) capturing the temporal flow of events, and (iii) making sense of procedural knowledge.","year":2018,"link":"https://hucvl.github.io/recipeqa/","license":"Various","licenseType":"non-commercial","licenseText":"RecipeQA contains question answer pairs generated from copyright free recipes found online under a variety of licences. The corresponding licence for each recipe is also provided in the dataset, see recipes.json.","arxiv":1809.00812},{"id":107,"name":"Chinese Text in the Wild","type":"cv","description":"A dataset of Chinese text with about 1 million Chinese characters annotated by experts in over 30 thousand street view images.","year":2018,"link":"https://ctwdataset.github.io/","license":"CC BY-NC-SA 4.0","licenseType":"non-commercial","licenseText":"Attribution-NonCommercial-ShareAlike International - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon,\nUnder the following terms:\nAttribution - you must give approprate credit,\nNonCommercial - you may not use the material for commercial purposes,\nShareAlike - if you make changes, you must distribute your contributions.","arxiv":1803.00085},{"id":106,"name":"CORNELL NEWSROOM","type":"nlp","description":"CORNELL NEWSROOM is a large dataset for training and evaluating summarization systems. It contains 1.3 million articles and summaries written by authors and editors in the newsrooms of 38 major publications. The summaries are obtained from search and social metadata between 1998 and 2017 and use a variety of summarization strategies combining extraction and abstraction.","year":2018,"link":"https://summari.es/","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","pdf":"http://aclweb.org/anthology/N18-1065"},{"id":105,"name":"The Stanford Question Answering Dataset (SQuAD) 2.0","type":"qa","Stats":"100,000+ question-answer pairs on 500+ articles","description":"Stanford Question Answering Dataset (SQuAD) is a new reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage. With 100,000+ question-answer pairs on 500+ articles, SQuAD is significantly larger than previous reading comprehension datasets. SQuAD2.0 combines the 100,000 questions in SQuAD1.1 with over 50,000 new, unanswerable questions written adversarially by crowdworkers to look similar to answerable ones.","year":2018,"link":"https://rajpurkar.github.io/SQuAD-explorer/","license":"CC-BY-SA 4.0","licenseType":"commercial","licenseText":"Attribution-ShareAlike 4.0 International - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon, even commercialy,\nUnder the following terms:\nAttribution - you must give approprate credit,\nShareAlike - if you make changes, you must distribute your contributions.","arxiv":1806.03822},{"id":104,"name":"The Massively Multilingual Image Dataset (MMID)","type":"cv","description":"MMID is a large-scale, massively multilingual dataset of images paired with the words they represent collected at the University of Pennsylvania. By far the largest dataset of its kind, it has 98 languages (including English) and up to 10,000 words per language! (and many more for English.)","year":2018,"link":"http://multilingual-images.org/","license":"Not found","licenseType":"non-commercial","licenseText":"License information not found","pdf":"https://aclweb.org/anthology/P18-1239"},{"id":103,"name":"Berkeley Deep Drive (BDD100K)","type":"self-driving","Stats":"over 100K videos with diverse kinds of annotations including image level tagging, object bounding boxes, drivable areas, lane markings, and full-frame instance segmentation","description":"The dataset contains over 100k videos of driving experience, each running 40 seconds at 30 frames per second. The total image count is 800 times larger than Baidu ApolloScape (released March 2018), 4,800 times larger than Mapillary and 8,000 times larger than KITTI.","year":2018,"link":"http://bdd-data.berkeley.edu/","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","arxiv":1805.04687},{"id":102,"name":"SWAG","type":"qa","description":"Situations With Adversarial Generations is a large-scale dataset for this task of grounded commonsense inference, unifying natural language inference and physically grounded reasoning.\nThe dataset consists of 113k multiple choice questions about grounded situations. Each question is a video caption from LSMDC or ActivityNet Captions, with four answer choices about what might happen next in the scene. The correct answer is the (real) video caption for the next event in the video; the three incorrect answers are adversarially generated and human verified, so as to fool machines but not humans.","year":2018,"link":"https://rowanzellers.com/swag/","license":"MIT","licenseType":"commercial","licenseText":"MIT - You are free to: use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the work. Under the following terms: the work is provided \"as is\", you must include copyright and the license in all copies or substantial uses of the work.","arxiv":1808.05326},{"id":101,"name":"HighD - The Highway Drone Dataset","type":"cv","description":"The highD dataset is a new dataset of naturalistic vehicle trajectories recorded on German highways. Using a drone, typical limitations of established traffic data collection methods such as occlusions are overcome by the aerial perspective. Traffic was recorded at six different locations and includes more than 110 500 vehicles. ","year":2018,"link":"https://www.highd-dataset.com","license":"Non-commercial & commercial","licenseType":"commercial","licenseText":"Non-commercial and commercial licenses available","arxiv":1810.05642},{"id":100,"name":"Comma 2k19","type":"self-driving","description":"comma.ai presents comma2k19, a dataset of over 33 hours of commute in California's 280 highway. This means 2019 segments, 1 minute long each, on a 20km section of highway driving between California's San Jose and San Francisco. comma2k19 is a fully reproducible and scalable dataset.","year":2018,"link":"https://github.com/commaai/comma2k19","license":"MIT","licenseType":"commercial","licenseText":"MIT - You are free to: use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the work. Under the following terms: the work is provided \"as is\", you must include copyright and the license in all copies or substantial uses of the work.","arxiv":"1812.05752v1","torrent":"https://academictorrents.com/details/65a2fbc964078aff62076ff4e103f18b951c5ddb"},{"id":99,"name":"Supervisely Person","type":"cv","description":"Dataset consists of 5,711 images with 6,884 high-quality annotated person instances. Can be found on Supervisaly.ai under “Datasets library”.","year":2018,"link":"http://supervise.ly/","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited."},{"id":98,"name":"VOiCES","type":"audio","description":"The Voices Obscured in Complex Environmental settings (VOiCES) corpus presents audio recorded in acoustically challenging conditions. Source Material: a total of 15 hours (3,903 audio files).","year":2018,"link":"https://voices18.github.io/","license":"CC BY 4.0","licenseType":"commercial","licenseText":"Attribution 4.0 International (CC BY 4.0) - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon, even commercialy,\nUnder the following terms:\nAttribution - you must give approprate credit.","arxiv":1804.05053},{"id":97,"name":"WikiHow-Dataset","type":"nlp","description":"WikiHow is a new large-scale dataset using the online WikiHow (http://www.wikihow.com/) knowledge base. Please refer to the paper for more information regarding the dataset and its properties. Each article consists of multiple paragraphs and each paragraph starts with a sentence summarizing it. By merging the paragraphs to form the article and the paragraph outlines to form the summary, the resulting version of the dataset contains more than 200,000 long-sequence pairs.","year":2018,"link":"https://github.com/mahnazkoupaee/WikiHow-Dataset","license":"CC-BY-NC-SA","licenseType":"non-commercial","licenseText":"Attribution-NonCommercial-ShareAlike International - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon,\nUnder the following terms:\nAttribution - you must give approprate credit,\nNonCommercial - you may not use the material for commercial purposes,\nShareAlike - if you make changes, you must distribute your contributions.","arxiv":1810.09305},{"id":96,"name":"HD1K Benchmark Suite","type":"self-driving","description":"An autonomous driving dataset and benchmark for optical flow. > 1000 frames at 2560x1080 with diverse lighting and weather scenarios, reference data with error bars for optical flow, evaluation masks for dynamic objects, specific robustness evaluation on challenging scenes. The dataset includes:\n110,500 vehicles\n44,500 driven kilometers\n147 driven hours","year":2018,"link":"http://hci-benchmark.org/","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","pdf":"http://hci-benchmark.iwr.uni-heidelberg.de/media/publications//kondermann2016.pdf"},{"id":95,"name":"VQA Visual Question Answering","type":"qa","Stats":"204721 images, 1,105,904 questions, 11,059,040 ground truth answers","description":"VQA is a dataset containing open-ended questions about images. These questions require an understanding of vision and language. It contains 265,016 images (COCO and abstract scenes), at least 3 questions (5.4 questions on average) per image, 10 ground truth answers per question.","year":2018,"link":"http://www.visualqa.org/","license":"CC BY 4.0","licenseType":"commercial","licenseText":"Attribution 4.0 International (CC BY 4.0) - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon, even commercialy,\nUnder the following terms:\nAttribution - you must give approprate credit.","arxiv":1505.00468,"torrent":"https://academictorrents.com/details/f075ad12eccbbd665aec68db5d208dc68e7a384f"},{"id":94,"name":"The DriveU Traffic Light Dataset (DTLD)","type":"cv","description":"\nDTLD contains more than 230 000 annotated traffic lights in camera images with a resolution of 2 megapixels. The dataset was recorded in 11 cities in Germany with a frequency of 15 Hz. Due to additional annotation attributes such as the traffic light pictogram, orientation or relevancy 344 unique classes exist. In addition to camera images and labels we provide stereo information in form of disparity images allowing stereo-based detection and depth-dependent evaluations.","year":2018,"link":"http://www.traffic-light-data.com/","license":"Not found","licenseType":"non-commercial","licenseText":"License information not found","pdf":"https://ieeexplore.ieee.org/document/8460737"},{"id":93,"name":"BoxCars116K","type":"cv","description":"A large fine-grained vehicle data set BoxCars116k, with 116k images of vehicles from various viewpoints taken by numerous surveillance cameras.","year":2018,"link":"https://medusa.fit.vutbr.cz/traffic/research-topics/fine-grained-vehicle-recognition/boxcars-improving-vehicle-fine-grained-recognition-using-3d-bounding-boxes-in-traffic-surveillance/","license":"CC BY-NC-SA 4.0","licenseType":"non-commercial","licenseText":"Attribution-NonCommercial-ShareAlike International - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon,\nUnder the following terms:\nAttribution - you must give approprate credit,\nNonCommercial - you may not use the material for commercial purposes,\nShareAlike - if you make changes, you must distribute your contributions.","pdf":"https://ieeexplore.ieee.org/document/8307405"},{"id":92,"name":"MultiNLI","type":"nlp","description":"The Multi-Genre Natural Language Inference (MultiNLI) corpus is a crowd-sourced collection of 433k sentence pairs annotated with textual entailment information. The corpus is modeled on the SNLI corpus, but differs in that covers a range of genres of spoken and written text, and supports a distinctive cross-genre generalization evaluation.","year":2018,"link":"https://www.nyu.edu/projects/bowman/multinli/","license":"Various","licenseType":"non-commercial","licenseText":"The majority of the\ncorpus is released under the OANC’s license,\nwhich allows all content to be freely used, modified, and shared under permissive terms. The data\nin the FICTION section falls under several permissive licenses; Seven Swords is available under\na Creative Commons Share-Alike 3.0 Unported\nLicense, and with the explicit permission of the\nauthor, Living History and Password Incorrect are\navailable under Creative Commons Attribution\n3.0 Unported Licenses; the remaining works of\nfiction are in the public domain in the United\nStates (but may be licensed differently elsewhere).","arxiv":1704.05426},{"id":91,"name":"ApolloScape","type":"self-driving","description":"ApolloScape is an order of magnitude bigger and more complex than existing similar datasets such as Kitti and CityScapes. ApolloScape offers 10 times more high-resolution images with pixel-by-pixel annotations, and includes 26 different recognizable objects such as cars, bicycles, pedestrians and buildings. The dataset offers several levels of scene complexity with increasing number of pedestrians and vehicles, up to 100 vehicles in a given scene, as well as a wider set of challenging environments such as heavy weather or extreme lighting conditions.","year":2018,"link":"http://apolloscape.auto","license":"Non-commercial & commercial","licenseType":"commercial","licenseText":"Non-commercial and commercial licenses available","arxiv":1803.06184},{"id":90,"name":"DVQA","type":"qa","description":"DVQA: Understanding Data Visualizations via Question Answering, a dataset that\ntests many aspects of bar chart understanding in a question answering framework. Contains over 3\nmillion image-question pairs about bar charts. It tests\nthree forms of diagram understanding: a) structure understanding; b) data retrieval; and c) reasoning. ","year":2018,"link":"https://github.com/kushalkafle/DVQA_dataset","license":"CC BY-NC 4.0","licenseType":"non-commercial","licenseText":"Attribution-NonCommercial 4.0 International - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon,\nUnder the following terms:\nAttribution - you must give approprate credit,\nNonCommercial - you may not use the material for commercial purposes.","arxiv":1801.08163},{"id":89,"name":"nuScenes","type":"self-driving","description":"The nuScenes dataset is a large-scale autonomous driving dataset. It features:\n● Full sensor suite (1x LIDAR, 5x RADAR, 6x camera, IMU, GPS)\n● 1000 scenes of 20s each\n● 1,440,000 camera images\n● 400,000 lidar sweeps\n● Two diverse cities: Boston and Singapore","year":2018,"link":"https://www.nuscenes.org/","license":"CC BY-NC-SA 4.0 or commercial","licenseType":"commercial","licenseText":"Attribution-NonCommercial-ShareAlike International - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon,\nUnder the following terms:\nAttribution - you must give approprate credit,\nNonCommercial - you may not use the material for commercial purposes,\nShareAlike - if you make changes, you must distribute your contributions."},{"id":88,"name":"MURA","type":"medical","description":"MURA (musculoskeletal radiographs) is a large dataset of bone X-rays that can be used to train algorithms tasked with detecting abnormalities in X-rays. MURA is believed to be the world’s largest public radiographic image dataset with 40,561 labeled images.","year":2018,"link":"https://stanfordmlgroup.github.io/competitions/mura/","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Stanford University School of Medicine MURA Dataset Research Use Agreement (see website for license)","arxiv":1712.06957},{"id":87,"name":"COCO-Text","type":"cv","description":"A Large-Scale Scene Text Dataset, Based on MSCOCO. COCO-Text V2.0 contains 63,686 images with 239,506 annotated text instances. Segmentation mask is annotated for every word, allowing fine-level detection. Three attributes are labeled for every word: machine-printed vs. handwritten, legible vs. illgible, and English vs. non-English.","year":2018,"link":"https://bgshih.github.io/cocotext/","license":"CC BY 4.0","licenseType":"commercial","licenseText":"Attribution 4.0 International (CC BY 4.0) - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon, even commercialy,\nUnder the following terms:\nAttribution - you must give approprate credit."},{"id":86,"name":"Synscapes","type":"cv","description":"A photorealistic synthetic dataset for street scene parsing. The images in the dataset do not follow a driven path through a single virtual world. Instead, an entirely unique scene was procedurally generated for each of the 25,000 images. As a result, the dataset contains a wide range of variations and unique combinations of features.","year":2018,"link":"https://7dlabs.com/synscapes-overview","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","arxiv":1810.08705},{"id":85,"name":"CULane","type":"cv","description":"\nCULane is a large scale challenging dataset for academic research on traffic lane detection. It is collected by cameras mounted on six different vehicles driven by different drivers in Beijing. More than 55 hours of videos were collected and 133,235 frames were extracted. Data examples are shown above. We have divided the dataset into 88880 for training set, 9675 for validation set, and 34680 for test set. The test set is divided into normal and 8 challenging categories, which correspond to the 9 examples above.","year":2018,"link":"https://xingangpan.github.io/projects/CULane.html","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","arxiv":1712.0608},{"id":84,"name":"MultiWOZ","type":"nlp","description":"The MultiWOZ dataset is a fully-labeled collection of human-human written conversations spanning over multiple domains and topics. At a size of 10k dialogues, it is at least one order of magnitude larger than all previous annotated task-oriented corpora. The dialogue are set between a tourist and a clerk in the information. It spans over 7 domains.","year":2018,"link":"https://www.repository.cam.ac.uk/handle/1810/280608","license":"CC BY 4.0","licenseType":"commercial","licenseText":"Attribution 4.0 International (CC BY 4.0) - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon, even commercialy,\nUnder the following terms:\nAttribution - you must give approprate credit.","arxiv":1810.00278},{"id":83,"name":"CoQA","type":"qa","description":"CoQA is a large-scale dataset for building Conversational Question Answering systems. CoQA contains 127k questions with answers, obtained from 8k conversations about text passages from seven diverse domains.","year":2018,"link":"https://stanfordnlp.github.io/coqa/","license":"Various","licenseType":"non-commercial","licenseText":"CoQA contains passages from seven domains. We make five of these public under the following licenses:\n\nLiterature and Wikipedia passages are shared under CC BY-SA 4.0 license.\nChildren's stories are collected from MCTest which comes with MSR-LA license.\nMiddle/High school exam passages are collected from RACE which comes with its own license.\nNews passages are collected from the DeepMind CNN dataset which comes with Apache license.","arxiv":1808.07042},{"id":82,"name":"Spider 1.0","type":"nlp","description":"Spider is a large-scale complex and cross-domain semantic parsing and text-to-SQL dataset. Spider consists of 10,181 questions and 5,693 unique complex SQL queries on 200 databases with multiple tables covering 138 different domains.","year":2018,"link":"https://yale-lily.github.io/spider","license":"CC BY-SA 4.0","licenseType":"commercial","licenseText":"Attribution-ShareAlike 4.0 International - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon, even commercialy,\nUnder the following terms:\nAttribution - you must give approprate credit,\nShareAlike - if you make changes, you must distribute your contributions.","arxiv":1808.07042},{"id":81,"name":"Google Conceptual Captions","type":"nlp","description":"We make available Conceptual Captions, a new dataset consisting of ~3.3M images annotated with captions. In contrast with the curated style of other image caption annotations, Conceptual Caption images and their raw descriptions are harvested from the web, and therefore represent a wider variety of styles. More precisely, the raw descriptions are harvested from the Alt-text HTML attribute associated with web images. To arrive at the current version of the captions, we have developed an automatic pipeline that extracts, filters, and transforms candidate image/caption pairs, with the goal of achieving a balance of cleanliness, informativeness, fluency, and learnability of the resulting captions.","year":2018,"link":"https://ai.google.com/research/ConceptualCaptions","license":"Not found","licenseType":"non-commercial","licenseText":"License information not found"},{"id":80,"name":"DensePose","type":"cv","description":"Dense human pose estimation aims at mapping all human pixels of an RGB image to the 3D surface of the human body. \n\nWe introduce DensePose-COCO, a large-scale ground-truth dataset with image-to-surface correspondences manually annotated on 50K COCO images.","year":2018,"link":"http://densepose.org/","license":"CC BY-NC 2.0","licenseType":"non-commercial","licenseText":"Attribution-NonCommercial 2.0 International - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon,\nUnder the following terms:\nAttribution - you must give approprate credit,\nNonCommercial - you may not use the material for commercial purposes.","arxiv":1802.00434},{"id":79,"name":"Dreyeve","type":"cv","description":"Composed by 74 video sequences of 5 mins each, we have captured and annotated more than 500,000 frames. The labeling contains drivers’ gaze fixations and their temporal integration providing task-specific saliency maps. Geo-referenced locations, driving speed and course complete the set of released data.","year":2018,"link":"http://imagelab.ing.unimore.it/dreyeve","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","arxiv":1705.03854},{"id":78,"name":"HotpotQA","type":"qa","description":"HotpotQA is a question answering dataset featuring natural, multi-hop questions, with strong supervision for supporting facts to enable more explainable question answering systems. The dataset is composed of 113,000 QA pairs based on Wikipedia.","year":2018,"link":"https://hotpotqa.github.io/","license":"CC BY-SA 4.0","licenseType":"commercial","licenseText":"Attribution-ShareAlike 4.0 International - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon, even commercialy,\nUnder the following terms:\nAttribution - you must give approprate credit,\nShareAlike - if you make changes, you must distribute your contributions.","arxiv":1809.096},{"id":77,"name":"Tencent ML — Images","type":"cv","description":"Tencent ML — Images is the largest open-source multi-label image dataset, including 17,609,752 training and 88,739 validation image URLs which are annotated with up to 11,166 categories.","year":2018,"link":"https://github.com/Tencent/tencent-ml-images","license":"CC BY 4.0","licenseType":"commercial","licenseText":"Attribution 4.0 International (CC BY 4.0) - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon, even commercialy,\nUnder the following terms:\nAttribution - you must give approprate credit.","arxiv":1901.01703},{"id":76,"name":"fastMRI Dataset","type":"medical","description":"Acollaborative research project from Facebook AI Research (FAIR) and NYU Langone Health to investigate the use of AI to make MRI scans up to 10 times faster. The dataset includes more than 1.5 million anonymous MRI images of the knee, drawn from 10,000 scans, and raw measurement data from nearly 1,600 scans.","year":2018,"link":"http://fastmri.org/","license":"MIT","licenseType":"commercial","licenseText":"MIT - You are free to: use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the work. Under the following terms: the work is provided \"as is\", you must include copyright and the license in all copies or substantial uses of the work.","arxiv":1811.08839},{"id":75,"name":"Baidu DuReader 2.0","type":"qa","description":"DuReader 2.0 is a large-scale open-domain Chinese dataset for Machine Reading Comprehension (MRC) and Question Answering (QA). It contains more than 300K questions, 1.4M evident documents and corresponding human generated answers.","year":2018,"link":"https://ai.baidu.com/broad/subordinate?dataset=dureader","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","arxiv":1711.05073},{"id":74,"name":"Mapillary Vistas","type":"cv","Stats":"25,000 Images | 100 Categories | 60 Instance-wise Categories","description":"The Mapillary Vistas Dataset is the most diverse publicly available dataset of manually annotated training data for semantic segmentation of street scenes. 25,000 images pixel-accurately labeled into 152 object categories, 100 of those instance-specific. ","year":2017,"link":"https://www.mapillary.com/dataset/vistas","license":"Research or commercial","licenseType":"commercial","licenseText":"Research and commercial licenses available.","pdf":"https://research.mapillary.com/img/publications/ICCV17a.pdf"},{"id":73,"name":"WebLogo-2M\n","type":"cv","description":"The WebLogo-2M dataset is a weakly labelled (at image level rather than object bounding box level) logo detection dataset. The dataset was constructed automatically by sampling the Twitter stream data. It contains 194 unique logo classes and over 2 million logo images.","year":2017,"link":"http://www.eecs.qmul.ac.uk/~hs308/WebLogo-2M.html/","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","arxiv":1612.09322},{"id":72,"name":"FMA: A Dataset For Music Analysis","type":"audio","description":"We introduce the Free Music Archive (FMA), an open and easily accessible dataset suitable for evaluating several tasks in MIR, a field concerned with browsing, searching, and organizing large music collections. The community's growing interest in feature and end-to-end learning is however restrained by the limited availability of large audio datasets. The FMA aims to overcome this hurdle by providing 917 GiB and 343 days of Creative Commons-licensed audio from 106,574 tracks from 16,341 artists and 14,854 albums, arranged in a hierarchical taxonomy of 161 genres. It provides full-length and high-quality audio, pre-computed features, together with track- and user-level metadata, tags, and free-form text such as biographies.","year":2017,"link":"https://github.com/mdeff/fma","license":"CC BY 4.0","licenseType":"commercial","licenseText":"Attribution 4.0 International (CC BY 4.0) - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon, even commercialy,\nUnder the following terms:\nAttribution - you must give approprate credit.","arxiv":1612.0184,"torrent":"https://academictorrents.com/details/dba20c45d4d6fa6453a4e99d2f8a4817893cfb94"},{"id":71,"name":"Twitter100k","type":"nlp","description":"Twitter100k dataset is characterized by two aspects: 1) it has 100,000 image-text pairs randomly crawled from Twitter and thus has no constraint in the image categories; 2) text in Twitter100k is written in informal language by the users. ","year":2017,"link":"https://github.com/huyt16/Twitter100k","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","arxiv":1703.06618},{"id":70,"name":"CityCam","type":"cv","description":"CITYCAM aims to understand the city by analyzing the vehicles. We collected and annotated 60,000 frames\n\nwith rich information, leading to about 900,000 annotated objects.","year":2017,"link":"https://www.citycam-cmu.com/","license":"CC BY 4.0","licenseType":"commercial","licenseText":"Attribution 4.0 International (CC BY 4.0) - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon, even commercialy,\nUnder the following terms:\nAttribution - you must give approprate credit.","arxiv":1703.05868},{"id":69,"name":"The Quick, Draw! Dataset","type":"cv","description":"The Quick Draw Dataset is a collection of 50 million drawings across 345 categories, contributed by players of the game Quick, Draw!. The drawings were captured as timestamped vectors, tagged with metadata including what the player was asked to draw and in which country the player was located. You can browse the recognized drawings on quickdraw.withgoogle.com/data.","year":2017,"link":"https://github.com/googlecreativelab/quickdraw-dataset","license":"CC BY 4.0","licenseType":"commercial","licenseText":"Attribution 4.0 International (CC BY 4.0) - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon, even commercialy,\nUnder the following terms:\nAttribution - you must give approprate credit."},{"id":68,"name":"Vehicle Make and Model Recognition Dataset (VMMRdb)","type":"cv","description":"The Vehicle Make and Model Recognition dataset (VMMRdb) is large in scale and diversity, containing 9,170 classes consisting of 291,752 images, covering models manufactured between 1950 to 2016. VMMRdb dataset contains images that were taken by different users, different imaging devices, and multiple view angles, ensuring a wide range of variations to account for various scenarios that could be encountered in a real-life scenario. The cars are not well aligned, and some images contain irrelevant background. The data was gathered by crawling web pages related to vehicle sales on craigslist.com, including 712 areas covering all 412 sub-domains corresponding to US metro areas.","year":2017,"link":"http://vmmrdb.cecsresearch.org/","license":"Not found","licenseType":"non-commercial","licenseText":"License information not found","pdf":"http://vmmrdb.cecsresearch.org/papers/VMMR_TSWC.pdf"},{"id":67,"name":"Places2","type":"cv","Stats":"10 million images comprising 400+ unique scene categories","description":"Places contains more than 10 million images comprising 400+ unique scene categories. The dataset features 5000 to 30,000 training images per class, consistent with real-world frequencies of occurrence.","year":2017,"link":"http://places2.csail.mit.edu/","license":"CC BY 4.0","licenseType":"commercial","licenseText":"Attribution 4.0 International (CC BY 4.0) - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon, even commercialy,\nUnder the following terms:\nAttribution - you must give approprate credit.","arxiv":1610.02055},{"id":66,"name":"UTKFace","type":"cv","description":"UTKFace dataset is a large-scale face dataset with long age span (range from 0 to 116 years old). The dataset consists of over 20,000 face images with annotations of age, gender, and ethnicity. The images cover large variation in pose, facial expression, illumination, occlusion, resolution, etc. This dataset could be used on a variety of tasks, e.g., face detection, age estimation, age progression/regression, landmark localization, etc.","year":2017,"link":"https://susanqq.github.io/UTKFace/","license":"Non commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","arxiv":1702.08423},{"id":65,"name":"VoxCeleb","type":"audio","description":"VoxCeleb is an audio-visual dataset consisting of short clips of human speech, extracted from interview videos uploaded to YouTube.\nIt contains data from 7,000+ speakers, 1 million+ utterances, 2,000+ hours. VoxCeleb consists of both audio and video. Each segment is at least 3 seconds long.","year":2017,"link":"http://www.robots.ox.ac.uk/~vgg/data/voxceleb/","license":"CC BY-SA 4.0","licenseType":"commercial","licenseText":"\"Attribution-ShareAlike 4.0 International - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon, even commercialy,\nUnder the following terms:\nAttribution - you must give approprate credit,\nShareAlike - if you make changes, you must distribute your contributions.\"","pdf":"http://www.robots.ox.ac.uk/~vgg/publications/2017/Nagrani17/nagrani17.pdf"},{"id":64,"name":"Bosch Small Traffic Lights Dataset","type":"cv","description":"This dataset contains 13,427 camera images at a resolution of 1280x720 pixels and contains about 24,000 annotated traffic lights. The annotations include bounding boxes of traffic lights as well as the current state (active light) of each traffic light. ","year":2017,"link":"https://hci.iwr.uni-heidelberg.de/node/6132","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited."},{"id":63,"name":"Fashion MNIST","type":"cv","description":"Fashion-MNIST is a dataset of Zalando's article images—consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes.","year":2017,"link":"https://github.com/zalandoresearch/fashion-mnist","license":"MIT","licenseType":"commercial","licenseText":"MIT - You are free to: use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the work. Under the following terms: the work is provided \"as is\", you must include copyright and the license in all copies or substantial uses of the work.","arxiv":1708.07747},{"id":62,"name":"Google sentence compression","type":"nlp","description":"Large corpus of uncompressed and compressed sentences from news articles. Contains over 200,000 sentence compression pairs.","year":2017,"link":"https://github.com/google-research-datasets/sentence-compression","license":"Not found","licenseType":"non-commercial","licenseText":"License information not found"},{"id":61,"name":"Youtube-BoundingBoxes","type":"cv","description":"YouTube-BoundingBoxes is a large-scale data set of video URLs with densely-sampled high-quality single-object bounding box annotations. The data set consists of approximately 380,000 15-20s video segments extracted from 240,000 different publicly visible YouTube videos, automatically selected to feature objects in natural settings without editing or post-processing, with a recording quality often akin to that of a hand-held cell phone camera.","year":2017,"link":"https://research.google.com/youtube-bb/index.html","license":"CC BY 4.0","licenseType":"commercial","licenseText":"Attribution 4.0 International (CC BY 4.0) - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon, even commercialy,\nUnder the following terms:\nAttribution - you must give approprate credit.","arxiv":1702.00824},{"id":60,"name":"Reddit comments","type":"nlp","description":"Reddit Comments from 2005-12 to 2017-03. Downloaded from https://files.pushshift.io/comments.","year":2017,"link":"https://www.reddit.com/r/datasets/comments/65o7py/updated_reddit_comment_dataset_as_torrents/","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","torrent":"http://academictorrents.com/details/85a5bd50e4c365f8df70240ffd4ecc7dec59912b"},{"id":59,"name":"NarrativeQA","type":"qa","description":"NarrativeQA is a dataset built to encourage deeper comprehension of language. This dataset involves reasoning over reading entire books or movie scripts. This dataset contains approximately 45K question answer pairs in free form text. There are two modes of this dataset (1) reading comprehension over summaries and (2) reading comprehension over entire books/scripts.","year":2017,"link":"https://github.com/deepmind/narrativeqa","license":"Apache","licenseType":"commercial","licenseText":"Apache License 2.0 - \n\nA permissive license whose main conditions require preservation of copyright and license notices. Contributors provide an express grant of patent rights. Licensed works, modifications, and larger works may be distributed under different terms and without source code.","arxiv":1712.0704},{"id":58,"name":"ScanNet","type":"cv","description":"ScanNet is an RGB-D video dataset containing 2.5 million views in more than 1500 scans, annotated with 3D camera poses, surface reconstructions, and instance-level semantic segmentations. To collect this data, we designed an easy-to-use and scalable RGB-D capture system that includes automated surface reconstruction and crowdsourced semantic annotation. ","year":2017,"link":"http://www.scan-net.org/","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","arxiv":1702.04405},{"id":57,"name":"NSynth","type":"audio","description":"A large-scale and high-quality dataset of annotated musical notes. The NSynth Dataset is an audio dataset containing ~300k musical notes, each with a unique pitch, timbre, and envelope. Each note is annotated with three additional pieces of information based on a combination of human evaluation and heuristic algorithms: the method of sound production for the note's instrument, the high-level family of which the note's instrument is a member and sonic qualities of the note.","year":2017,"link":"https://magenta.tensorflow.org/datasets/nsynth","license":"CC BY 4.0","licenseType":"commercial","licenseText":"Attribution 4.0 International (CC BY 4.0) - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon, even commercialy,\nUnder the following terms:\nAttribution - you must give approprate credit.","arxiv":1704.01279},{"id":56,"name":"ADE20K","type":"cv","Stats":"20.210 images, All images are fully annotated with objects ","description":"A dataset for scene parsing. There are 20,210 images in the training set, 2,000 images\nin the validation set, and 3,000 images in the testing set. All\nthe images are exhaustively annotated with objects. Many\nobjects are also annotated with their parts. For each object\nthere is additional information about whether it is occluded\nor cropped, and other attributes.","year":2017,"link":"http://groups.csail.mit.edu/vision/datasets/ADE20K/","license":"Not found","licenseType":"non-commercial","licenseText":"License information not found","arxiv":1608.05442},{"id":55,"name":"Question Pairs (Quora)","type":"qa","Stats":"400000 lines","description":"A dataset of questions from Quora aimed at determining if pairs of question text actually correspond to semantically equivalent queries. Over 400,000 lines of potential question duplicate pairs. ","year":2017,"link":"https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited."},{"id":54,"name":"Yelp open dataset","type":"nlp","Stats":"5,200,000 reviews\n\n174,000 businesses\n\n200,000 pictures\n\n11 metropolitan areas\n1,100,000 tips by 1,300,000 users\nOver 1.2 million business attributes like hours, parking, availability, and ambience\nAggregated check-ins over time for each of the 174,000 businesses","description":"The Yelp dataset contains data about businesses, reviews, and user data for use in personal, educational, and academic purposes. Available in both JSON and SQL files.","year":2017,"link":"https://www.yelp.com/dataset","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","torrent":"https://academictorrents.com/details/66ab083bda0c508de6c641baabb1ec17f72dc480"},{"id":53,"name":"Google Audioset","type":"audio","Stats":"AudioSet consists of an expanding ontology of 632 audio event classes and a collection of 2,084,320 human-labeled 10-second sound clips drawn from YouTube videos. The ontology is specified as a hierarchical graph of event categories, covering a wide range of human and animal sounds, musical instruments and genres, and common everyday environmental sounds.\n\n2.1 million\nannotated videos\n5.8 thousand\nhours of audio\n527 classes\nof annotated sounds","description":"AudioSet consists of an expanding ontology of 632 audio event classes and a collection of 2,084,320 human-labeled 10-second sound clips drawn from YouTube videos. The ontology is specified as a hierarchical graph of event categories, covering a wide range of human and animal sounds, musical instruments and genres, and common everyday environmental sounds.","year":2017,"link":"https://research.google.com/audioset/","license":"CC-BY 4.0","licenseType":"commercial","licenseText":"Attribution 4.0 International (CC BY 4.0) - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon, even commercialy,\nUnder the following terms:\nAttribution - you must give approprate credit.","pdf":"https://research.google.com/pubs/pub45857.html"},{"id":52,"name":"HASY\n","type":"cv","description":"HASY is a publicly available, free of charge dataset of single symbols similar to MNIST. It contains 168233 instances of 369 classes. ","year":2017,"link":"https://zenodo.org/record/259444","license":"Not found","licenseType":"non-commercial","licenseText":"License information not found","arxiv":1701.0838},{"id":51,"name":"ETH3D","type":"cv","description":"A multi-view stereo / 3D reconstruction benchmark covering a variety of indoor and outdoor scenes.\nGround truth geometry has been obtained using a high-precision laser scanner. Contains 13 / 12 DSLR datasets for training / testing,\n5 / 5 multi-cam rig videos for training / testing,\n27 / 20 frames for two-view stereo training / testing.","year":2017,"link":"https://www.eth3d.net/","license":"CC BY-NC-SA 4.0","licenseType":"non-commercial","licenseText":"Attribution-NonCommercial-ShareAlike International - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon,\nUnder the following terms:\nAttribution - you must give approprate credit,\nNonCommercial - you may not use the material for commercial purposes,\nShareAlike - if you make changes, you must distribute your contributions.","pdf":"https://www.eth3d.net/data/schoeps2017cvpr.pdf"},{"id":50,"name":"A dataset of English plaintext jokes","type":"nlp","description":"There are about 208,000 jokes in this database scraped from three sources (reddit, stupidstuff.org, wocka.com).","year":2017,"link":"https://github.com/taivop/joke-dataset","license":"Various","licenseType":"non-commercial","licenseText":"Parts of the dataset could be under different licenses, check the dataset web page for more information"},{"id":49,"name":"WildDash","type":"cv","description":"The main focus of this dataset is testing. It contains data recorded under real world driving situations. Aims of it are:\nto compile and provide standard data which can be used for evaluation.\nto establish accepted evaluation protocols, data and measures.\nto boost the algorithm development on driving applications using computer vision techniques.\nThe WildDash dataset does not offer enough material to train algorithms by itself.","year":2017,"link":"http://www.wilddash.cc/","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","pdf":"http://openaccess.thecvf.com/content_ECCV_2018/papers/Oliver_Zendel_WildDash_-_Creating_ECCV_2018_paper.pdf"},{"id":48,"name":"Oxford RobotCar Dataset","type":"self-driving","description":"The Oxford RobotCar Dataset contains over 100 repetitions of a consistent route through Oxford, UK, captured over a period of over a year. The dataset captures many different combinations of weather, traffic and pedestrians, along with longer term changes such as construction and roadworks.","year":2017,"link":"http://robotcar-dataset.robots.ox.ac.uk/","license":"CC BY-NC-SA 4.0","licenseType":"non-commercial","licenseText":"Attribution-NonCommercial-ShareAlike International - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon,\nUnder the following terms:\nAttribution - you must give approprate credit,\nNonCommercial - you may not use the material for commercial purposes,\nShareAlike - if you make changes, you must distribute your contributions.","pdf":"https://robotcar-dataset.robots.ox.ac.uk/images/robotcar_ijrr.pdf"},{"id":47,"name":"Facebook bAbI","type":"nlp","description":"A set of datasets for automatic text understanding and reasoning.","year":2017,"link":"https://research.fb.com/downloads/babi/","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","arxiv":1502.05698},{"id":46,"name":"Recipe1M","type":"cv","description":"Recipe1M, a new large-scale, structured corpus of over one million cooking recipes and 13 million food images. As the largest publicly available collection of recipe data, Recipe1M affords the ability to train high-capacity models on aligned, multi-modal data. ","year":2017,"link":"http://pic2recipe.csail.mit.edu/","license":"Not found","licenseType":"non-commercial","licenseText":"License information not found","arxiv":1810.06553},{"id":45,"name":"Stanford Drone Dataset","type":"cv","description":"A large scale dataset that collects images and videos of various types of agents (not just pedestrians, but also bicyclists, skateboarders, cars, buses, and golf carts) that navigate in a real world outdoor environment such as a university campus. In the above images, pedestrians are labeled in pink, bicyclists in red, skateboarders in orange, and cars in green. 60 videos of 8 distinct scenes.","year":2016,"link":"http://cvgl.stanford.edu/projects/uav_data/","license":"CC BY-NC-SA 3.0","licenseType":"non-commercial","licenseText":"Attribution-NonCommercial-ShareAlike International - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon,\nUnder the following terms:\nAttribution - you must give approprate credit,\nNonCommercial - you may not use the material for commercial purposes,\nShareAlike - if you make changes, you must distribute your contributions.","pdf":"http://www.eccv2016.org/files/posters/P-3C-31.pdf","torrent":"https://academictorrents.com/details/01f95ea32e160e6c251ea55a87bd5a24b23cb03d"},{"id":44,"name":"Tsinghua-Tencent 100k (traffic signs)","type":"cv","description":"It provides 100,000 images containing 30,000 traffic-sign instances. These images cover large variations in illuminance and weather conditions. Each traffic-sign in the benchmark is annotated with a class label, its bounding box and pixel mask.","year":2016,"link":"https://cg.cs.tsinghua.edu.cn/traffic-sign/","license":"Not found","licenseType":"non-commercial","licenseText":"License information not found","pdf":"https://cg.cs.tsinghua.edu.cn/traffic-sign/0682.pdf"},{"id":43,"name":"MegaFace","type":"cv","Stats":"4.7 million faces","description":"The MF2 training dataset is the largest (in number of identities) publicly available facial recognition dataset with a 4.7 million faces, 672K identities, and their respective bounding boxes. All images obtained from Flickr (Yahoo's dataset) and licensed under Creative Commons.","year":2016,"link":"http://megaface.cs.washington.edu/","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","arxiv":1505.02108},{"id":42,"name":"GTA dataset","type":"cv","description":"The datasets consists of 24,966 densely labelled frames split into 10 parts for convenience. The class labels are compatible with the CamVid and CityScapes datasets. ","year":2016,"link":"https://download.visinf.tu-darmstadt.de/data/from_games/","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","arxiv":1608.02192},{"id":41,"name":"MIMIC","type":"medical","description":"MIMIC is an openly available dataset developed by the MIT Lab for Computational Physiology, comprising deidentified health data associated with ~40,000 critical care patients. It includes demographics, vital signs, laboratory tests, medications, and more. The latest version of MIMIC is MIMIC-III v1.4, which comprises over 58,000 hospital admissions for 38,645 adults and 7,875 neonates. The data spans June 2001 - October 2012. The database, although de-identified, still contains detailed information regarding the clinical care of patients, so must be treated with appropriate care and respect.","year":2016,"link":"https://mimic.physionet.org/","license":"Not found","licenseType":"non-commercial","licenseText":"License information not found","pdf":"https://www.nature.com/articles/sdata201635.pdf"},{"id":40,"name":"SceneNet RGB-D","type":"cv","Stats":"5 million images","description":" It provides pixel-perfect ground truth for scene understanding problems such as semantic segmentation, instance segmentation, and object detection, and also for geometric computer vision problems such as optical flow, depth estimation, camera pose estimation, and 3D reconstruction. A set of 5M rendered RGB-D images from over 15K trajectories in synthetic layouts with random but physically simulated object poses.","year":2016,"link":"https://robotvault.bitbucket.io/scenenet-rgbd.html","license":"GPL","licenseType":"commercial","licenseText":"GPL - You are free to: copy, distribute and modify the software as long as you track changes/dates in source files. Under the following terms: any modifications to or software including (via compiler) GPL-licensed code must also be made available under the GPL along with build & install instructions.\n","arxiv":1612.05079},{"id":39,"name":"MS MARCO","type":"nlp","Stats":"1,000,000 Real Bing User Queries, 100,000 Natural Language Answers","description":"Microsoft Machine Reading Comprehension (MS MARCO) is a new large scale dataset for reading comprehension and question answering. In MS MARCO, all questions are sampled from real anonymized user queries. The context passages, from which answers in the dataset are derived, are extracted from real web documents using the most advanced version of the Bing search engine. The answers to the queries are human generated if they could summarize the answer. It contains 1,010,916 user queries and 182,669 natural language answers.","year":2016,"link":"http://www.msmarco.org/","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","arxiv":1611.09268},{"id":38,"name":"SYNTHIA","type":"cv","Stats":"200,000 HD images from video streams and 20,000 HD images from independent snapshots","description":"The  SYNTHetic collection of Imagery and Annotations, is a dataset that has been generated with the purpose of aiding semantic segmentation and related scene understanding problems in the context of driving scenarios. SYNTHIA consists of a collection of photo-realistic frames rendered from a virtual city and comes with precise pixel-level semantic annotations. It contains: +200,000 HD images from video streams and +20,000 HD images from independent snapshots. Scene diversity: European style town, modern city, highway and green areas. Variety of dynamic objects: cars, pedestrians and cyclists.","year":2016,"link":"http://synthia-dataset.net/","license":"CC BY-SA 4.0","licenseType":"commercial","licenseText":"Attribution-ShareAlike 4.0 International - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon, even commercialy,\nUnder the following terms:\nAttribution - you must give approprate credit,\nShareAlike - if you make changes, you must distribute your contributions.","pdf":"https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Ros_The_SYNTHIA_Dataset_CVPR_2016_paper.pdf"},{"id":37,"name":"NewsQA","type":"qa","description":"The purpose of the NewsQA dataset is to help the research community build algorithms that are capable of answering questions requiring human-level comprehension and reasoning skills. Leveraging CNN articles from the DeepMind Q&A Dataset, we prepared a crowd-sourced machine reading comprehension dataset of 120K Q&A pairs.","year":2016,"link":"https://www.microsoft.com/en-us/research/project/newsqa-dataset","license":"Various","licenseType":"non-commercial","licenseText":"Parts of the dataset are under different licenses, check the dataset web page for more information","arxiv":1611.0983},{"id":36,"name":"UMD Faces","type":"cv","Stats":"367,888 face annotations for 8,277 subjects and Over 3.7 million annotated video frames from over 22,000 videos of 3100 subjects","description":"The dataset contains 367,888 face annotations for 8,277 subjects divided into 3 batches. Contains bounding boxes, the extimated pose (yaw, pitch, and roll), locations of twenty-one keypoints, and gender information generated by a pre-trained neural network.\nThe second part contains 3,735,476 annotated video frames extracted from a total of 22,075 for 3,107 subjects.","year":2016,"link":"http://www.umdfaces.io/","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","arxiv":"1611.01484v2"},{"id":35,"name":"comma.ai","type":"cv","description":"7 and a quarter hours of largely highway driving.","year":2016,"link":"https://github.com/commaai/research","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","arxiv":1608.0123},{"id":34,"name":"Spacenet","type":"cv","description":"SpaceNet is an online repository of freely available satellite imagery, co-registered map data to train algorithms, and a series of public challenges designed to accelerate innovation in machine learning using geospatial data. This first of its kind open innovation project for the geospatial industry is a collaboration between CosmiQ Works, DigitalGlobe and NVIDIA. In the first year, over 5,700 km2 of very high-resolution imagery and more than 520,000 vectors were released through SpaceNet on AWS.","year":2016,"link":"http://explore.digitalglobe.com/spacenet","license":"Various","licenseType":"non-commercial","licenseText":"Parts of the dataset are under different licenses, check the dataset web page for more information"},{"id":33,"name":"CompCars","type":"cv","Stats":"136,726 images","description":"The Comprehensive Cars (CompCars) dataset contains data from two scenarios, including images from web-nature and surveillance-nature. The web-nature data contains 163 car makes with 1,716 car models. There are a total of 136,726 images capturing the entire cars and 27,618 images capturing the car parts. The full car images are labeled with bounding boxes and viewpoints. Each car model is labeled with five attributes, including maximum speed, displacement, number of doors, number of seats, and type of car.","year":2015,"link":"http://mmlab.ie.cuhk.edu.hk/datasets/comp_cars/index.html","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","arxiv":1506.08959},{"id":32,"name":"ShapeNet","type":"cv","description":"ShapeNet is an ongoing effort to establish a richly-annotated, large-scale dataset of 3D shapes. ShapeNet is organized according to the WordNet hierarchy. Each meaningful concept in WordNet, possibly described by multiple words or word phrases, is called a \"synonym set\" or \"synset\". There are more than 100,000 synsets in WordNet, the majority of them being nouns (80,000+).","year":2015,"link":"https://shapenet.org/","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","arxiv":1512.03012},{"id":31,"name":"WIDER Face","type":"cv","Stats":"32,203 images and 393,703 labeled faces","description":"WIDER FACE dataset is a face detection benchmark dataset, of which images are selected from the publicly available WIDER dataset. We choose 32,203 images and label 393,703 faces with a high degree of variability in scale, pose and occlusion as depicted in the sample images. WIDER FACE dataset is organized based on 61 event classes.","year":2015,"link":"http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","arxiv":1511.06523},{"id":30,"name":"WIDER","type":"cv","Stats":"61 event categories and around 50574 images","description":"WIDER is a dataset for complex event recognition from static images. As of v0.1, it contains 61 event categories and around 50574 images annotated with event class labels. We provide a split of 50% for training and 50% for testing.","year":2015,"link":"http://yjxiong.me/event_recog/WIDER/","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","pdf":"http://yjxiong.me/papers/cvpr15event_supp.pdf"},{"id":29,"name":"LSUN","type":"cv","Stats":"around one million labeled images for each of 10 scene categories and 20 object categories","description":"LSUN contains around one million labeled images for each of 10 scene categories and 20 object categories.","year":2015,"link":"http://www.yf.io/p/lsun","license":"Not found","licenseType":"non-commercial","licenseText":"License information not found","arxiv":1506.03365,"torrent":"https://academictorrents.com/details/c53c374bd6de76da7fe76ed5c9e3c7c6c691c489"},{"id":28,"name":"CelebA","type":"cv","description":"CelebFaces Attributes Dataset (CelebA) is a large-scale face attributes dataset with more than 200K celebrity images, each with 40 attribute annotations. The images in this dataset cover large pose variations and background clutter. CelebA has large diversities, large quantities, and rich annotations.","year":2015,"link":"http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","arxiv":1411.7766},{"id":27,"name":"Visual Genome","type":"cv","Stats":"108,077 Images\n5.4 Million Region Descriptions\n1.7 Million Visual Question Answers\n3.8 Million Object Instances\n2.8 Million Attributes\n2.3 Million Relationships","description":"Visual Genome is a dataset, a knowledge base, an ongoing effort to connect structured image concepts to language. It contains: 108,077 Images\n5.4 Million Region Descriptions\n1.7 Million Visual Question Answers\n3.8 Million Object Instances","year":2015,"link":"http://visualgenome.org/","license":"CC BY 4.0","licenseType":"commercial","licenseText":"Attribution 4.0 International (CC BY 4.0) - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon, even commercialy,\nUnder the following terms:\nAttribution - you must give approprate credit.","arxiv":1602.07332},{"id":26,"name":"DBPedia, Amazon, Yelp, Yahoo!, Sogou, and AG","type":"nlp","Stats":"120K to 3.6M, ranging from binary to 14 class problems","description":"An extensive set of eight datasets for text classification. Datasets from DBPedia, Amazon, Yelp, Yahoo!, Sogou, and AG. Sample size of 120K to 3.6M, ranging from binary to 14 class problems. ","year":2015,"link":"https://drive.google.com/drive/u/0/folders/0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M","license":"Various","licenseType":"non-commercial","licenseText":"Parts of the dataset are under different licenses, check the dataset web page for more information"},{"id":25,"name":"CNN and Daily Mail summarization","type":"nlp","description":"\nTwo datasets using news articles for Q&A research. Each dataset contains many documents (90k and 197k each), and each document companies on average 4 questions approximately. Each question is a sentence with one missing word/phrase which can be found from the accompanying document/context.","year":2015,"link":"https://cs.nyu.edu/~kcho/DMQA/","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited."},{"id":24,"name":"Cityscapes","type":"cv","Stats":"street scenes from 50 different cities, with high quality pixel-level annotations of 5 000 frames in addition to a larger set of 20 000 weakly annotated frames","description":"Large-scale dataset that contains a diverse set of stereo video sequences recorded in street scenes from 50 different cities, with high quality pixel-level annotations of 5 000 frames in addition to a larger set of 20 000 weakly annotated frames.","year":2015,"link":"https://www.cityscapes-dataset.com/","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","arxiv":1604.01685,"torrent":"https://academictorrents.com/details/4f76b97fbb851fac002dcc55dcc55883e9728db7"},{"id":23,"name":"ACTIVITYNET","type":"cv","Stats":"200\nCLASSES\n100\nUNTRIMMED VIDEOS PER CLASS\n1.54\nACTIVITY INSTANCES PER VIDEO\n648\nVIDEO HOURS","description":"ActivityNet is a new large-scale video benchmark for human activity understanding. ActivityNet aims at covering a wide range of complex human activities that are of interest to people in their daily living. In its current version, ActivityNet provides samples from 203 activity classes with an average of 137 untrimmed videos per class and 1.41 activity instances per video, for a total of 849 video hours.","year":2015,"link":"http://activity-net.org/","license":"Not found","licenseType":"non-commercial","licenseText":"License information not found","pdf":"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Heilbron_ActivityNet_A_Large-Scale_2015_CVPR_paper.pdf"},{"id":22,"name":"LibriSpeech","type":"audio","description":"Large-scale (1000 hours) corpus of read English speech.","year":2015,"link":"http://www.openslr.org/12/","license":"CC BY 4.0","licenseType":"commercial","licenseText":"Attribution 4.0 International (CC BY 4.0) - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon, even commercialy,\nUnder the following terms:\nAttribution - you must give approprate credit.","pdf":"http://www.danielpovey.com/files/2015_icassp_librispeech.pdf"},{"id":21,"name":"IMDB-WIKI faces","type":"cv","description":"Faces from the list of the most popular 100,000 actors as listed on the IMDb website and (automatically) crawled from their profiles date of birth, name, gender and all images related to that person. 460,723 face images from 20,284 celebrities from IMDb and 62,328 from Wikipedia, thus 523,051 in total. ","year":2015,"link":"https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/","license":"Non-commciral","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","pdf":"https://www.vision.ee.ethz.ch/publications/papers/proceedings/eth_biwi_01229.pdf"},{"id":20,"name":"SNLI","type":"nlp","description":"The SNLI corpus (version 1.0) is a collection of 570k human-written English sentence pairs manually labeled for balanced classification with the labels entailment, contradiction, and neutral, supporting the task of natural language inference (NLI), also known as recognizing textual entailment (RTE).","year":2015,"link":"https://nlp.stanford.edu/projects/snli/","license":"CC BY-SA 4.0","licenseType":"commercial","licenseText":"Attribution-ShareAlike 4.0 International - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon, even commercialy,\nUnder the following terms:\nAttribution - you must give approprate credit,\nShareAlike - if you make changes, you must distribute your contributions.","arxiv":1508.05326},{"id":19,"name":"COCO","type":"cv","Stats":"330K images (>200K labeled)\n1.5 million object instances\n80 object categories\n91 stuff categories\n5 captions per image","description":"COCO is a large-scale object detection, segmentation, and captioning dataset. It contains: 330K images (>200K labeled), 1.5 million object instances, 80 object categories.","year":2014,"link":"http://cocodataset.org/","license":"CC BY 4.0","licenseType":"commercial","licenseText":"Attribution 4.0 International (CC BY 4.0) - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon, even commercialy,\nUnder the following terms:\nAttribution - you must give approprate credit.","arxiv":1405.0312,"torrent":"https://academictorrents.com/details/74dec1dd21ae4994dfd9069f9cb0443eb960c962"},{"id":18,"name":"Yahoo Flickr Creative Commons 100M","type":"cv","Stats":"100 million images","description":"This dataset contains a list of photos and videos. This list is compiled from data available on Yahoo! Flickr. All the photos and videos provided in the list are licensed under one of the Creative Commons copyright licenses.","year":2014,"link":"https://webscope.sandbox.yahoo.com/catalog.php?datatype=i&did=67","license":"Various","licenseType":"non-commercial","licenseText":"Parts of the dataset are under different licenses, check the dataset web page for more information","arxiv":1503.01817},{"id":17,"name":"The Cancer Imaging Archive","type":"cv","description":"TCIA is a service which de-identifies and hosts a large archive of medical images of cancer accessible for public download. The data are organized as “Collections”, typically patients related by a common disease (e.g. lung cancer), image modality (MRI, CT, etc) or research focus. DICOM is the primary file format used by TCIA for image storage.","year":2014,"link":"https://www.cancerimagingarchive.net/","license":"Various","licenseType":"non-commercial","licenseText":"Dataset are under different licenses, check the dataset web page for more information"},{"id":16,"name":"Pascal part","type":"cv","Stats":"Training and validation contains 10,103 images while testing contains 9,637 images.","description":"This dataset is a set of additional annotations for PASCAL VOC 2010. It goes beyond the original PASCAL object detection task by providing segmentation masks for each body part of the object.","year":2014,"link":"http://www.stat.ucla.edu/~xianjie.chen/pascal_part_dataset/pascal_part.html","license":"Not found","licenseType":"non-commercial","licenseText":"License information not found","arxiv":1406.2031},{"id":15,"name":"PETA","type":"cv","description":"Pedestrian Attribute Recognition At Far Distance dataset. The PETA dataset consists of 19000 images, with resolution ranging from 17-by-39 to 169-by-365 pixels. Those 19000 images include 8705 persons, each annotated with 61 binary and 4 multi-class attributes.","year":2014,"link":"http://mmlab.ie.cuhk.edu.hk/projects/PETA.html","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","pdf":"http://mmlab.ie.cuhk.edu.hk/projects/PETA_files/Pedestrian%20Attribute%20Recognition%20At%20Far%20Distance.pdf"},{"id":14,"name":"Flickr30k","type":"cv","description":"An image caption corpus consisting of 158,915 crowd-sourced captions describing 31,783 images. This is an extension of the Flickr 8k Dataset. The new images and captions focus on people involved in everyday activities and events.","year":2014,"link":"http://hockenmaier.cs.illinois.edu/DenotationGraph/","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","pdf":"http://hockenmaier.cs.illinois.edu/DenotationGraph/TACLDenotationGraph.pdf"},{"id":13,"name":"Food 101","type":"cv","description":"We introduce a challenging data set of 101 food categories, with 101'000 images. For each class, 250 manually reviewed test images are provided as well as 750 training images. On purpose, the training images were not cleaned, and thus still contain some amount of noise. This comes mostly in the form of intense colors and sometimes wrong labels. All images were rescaled to have a maximum side length of 512 pixels.","year":2014,"link":"https://www.vision.ee.ethz.ch/datasets_extra/food-101/","license":"Not found","licenseType":"non-commercial","licenseText":"License information not found","pdf":"https://www.vision.ee.ethz.ch/datasets_extra/food-101/static/bossard_eccv14_food-101.pdf","torrent":"https://academictorrents.com/details/470791483f8441764d3b01dbc4d22b3aa58ef46f"},{"id":12,"name":"KITTI","type":"self-driving","description":"A novel dataset captured from a VW station wagon for use in mobile robotics and autonomous driving\nresearch. In total, 6 hours of traffic scenarios recorded at\n10-100 Hz. The scenarios are diverse, capturing real-world traffic\nsituations and range from freeways over rural areas to innercity\nscenes with many static and dynamic objects.","year":2013,"link":"http://www.cvlibs.net/datasets/kitti/index.php","license":"CC BY-NC-SA 4.0","licenseType":"non-commercial","licenseText":"Attribution-NonCommercial-ShareAlike International - \nYou are free to:\nShare - copy and redistribute,\nAdapt - remix, transform, and build upon,\nUnder the following terms:\nAttribution - you must give approprate credit,\nNonCommercial - you may not use the material for commercial purposes,\nShareAlike - if you make changes, you must distribute your contributions.","pdf":"http://www.cvlibs.net/publications/Geiger2013IJRR.pdf"},{"id":11,"name":"Stanford cars","type":"cv","description":"Stanford Cars dataset contains 16,185 images of 196 classes of cars. The data is split into 8,144 training images and 8,041 testing images, where each class has been split roughly in a 50-50 split. Classes are typically at the level of Make, Model, Year, e.g. 2012 Tesla Model S or 2012 BMW M3 coupe.","year":2013,"link":"https://ai.stanford.edu/~jkrause/cars/car_dataset.html","license":"Not found","licenseType":"non-commercial","licenseText":"License information not found","pdf":"https://ai.stanford.edu/~jkrause/papers/3drr13.pdf","torrent":"https://academictorrents.com/details/9c90b7f6208d430bff288845d45667ab2670da56"},{"id":10,"name":"Paris500k","type":"cv","description":"The Paris500k dataset consists of 501,356 geotagged images collected from Flickr and Panoramio. The dataset was collected from a geographic bounding box rather than using keyword queries. Thus, the images have a \"natural\" distribution, as shown in the figure on the right. The dataset is very challenging due to the presence of duplicates and near-duplicates, as well as a large fraction of unrelated images, such as photos of parties, pets, etc.","year":2013,"link":"https://www.vision.rwth-aachen.de/page/paris500k","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","pdf":"https://vision.rwth-aachen.de/media/papers/weyandiccv13.pdf"},{"id":9,"name":"Billion Words","type":"nlp","description":"The purpose of the project is to make available a standard training and test setup for language modeling experiments.","year":2013,"link":"http://www.statmt.org/lm-benchmark/","license":"Not found","licenseType":"non-commercial","licenseText":"License information not found","arxiv":1312.3005},{"id":8,"name":"Stanford Sentiment Treebank","type":"nlp","description":"A dataset for sentiment analysis that includes fine grained\nsentiment labels for 215,154 phrases in the\nparse trees of 11,855 sentences and presents\nnew challenges for sentiment compositionality.","year":2013,"link":"https://nlp.stanford.edu/sentiment/code.html","license":"Not found","licenseType":"non-commercial","licenseText":"License information not found","pdf":"https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf"},{"id":7,"name":"PASCAL VOC 2012","type":"cv","description":"PASCAL VOC (2012 version) has 20 classes. The train/val data has 11,530 images containing 27,450 ROI annotated objects and 6,929 segmentations.","year":2012,"link":"http://host.robots.ox.ac.uk/pascal/VOC/","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","pdf":"http://host.robots.ox.ac.uk/pascal/VOC/pubs/everingham10.pdf","torrent":"https://academictorrents.com/details/e6d591cef9ea2840f7d8dfb6bb0e0503d5592128"},{"id":6,"name":"The German Traffic Sign Recognition Benchmark","type":"cv","description":"The German Traffic Sign Benchmark is a multi-class, single-image classification challenge held at the IJCNN 2011. The dataset contains: more than 40 classes, more than 50,000 images in total.","year":2012,"link":"http://benchmark.ini.rub.de/?section=gtsrb","license":"Not found","licenseType":"non-commercial","licenseText":"License information not found"},{"id":5,"name":"SVHN Street View House Numbers","type":"cv","Stats":"over 600,000 digit images","description":"SVHN is a real-world image dataset for developing machine learning and object recognition algorithms with minimal requirement on data preprocessing and formatting. It can be seen as similar to MNIST (e.g., the images are of small cropped digits), but incorporates an order of magnitude more labeled data (over 600,000 digit images) and comes from a significantly harder, unsolved, real world problem (recognizing digits and numbers in natural scene images). SVHN is obtained from house numbers in Google Street View images.","year":2011,"link":"http://ufldl.stanford.edu/housenumbers/","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","pdf":"http://ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf"},{"id":4,"name":"Large Movie Review Dataset","type":"nlp","description":"This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided. ","year":2011,"link":"http://ai.stanford.edu/~amaas/data/sentiment/","license":"Not found","licenseType":"non-commercial","licenseText":"License information not found","pdf":"http://www.aclweb.org/anthology/P11-1015","torrent":"https://academictorrents.com/details/fd24bc44d461b10288469e05a64a8344eb079f15"},{"id":3,"name":"Princeton WordNet","type":"NLP","description":"WordNet is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept. Synsets are interlinked by means of conceptual-semantic and lexical relations. The resulting network of meaningfully related words and concepts can be navigated with the browser. WordNet is also freely and publicly available for download.","year":2010,"link":"http://wordnet.princeton.edu/","license":"WordNet license","licenseType":"commercial","licenseText":"WordNet® is unencumbered, and may be used in commercial applications in accordance with the following license agreement. (see website for license)"},{"id":2,"name":"CIFAR-100","type":"cv","description":"This dataset is just like the CIFAR-10, except it has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. Each image comes with a “fine” label (the class to which it belongs) and a “coarse” label (the superclass to which it belongs).","year":2009,"link":"https://www.cs.toronto.edu/~kriz/cifar.html","license":"Not found","licenseType":"non-commercial","licenseText":"License information not found","pdf":"https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf","torrent":"https://academictorrents.com/details/4fb115df73d3313fae9264fd6c0bad061add2d63"},{"id":1,"name":"ImageNet","type":"cv","Stats":"Total number of non-empty synsets: 21841\nTotal number of images: 14,197,122\n","description":"ImageNet is an image database organized according to the WordNet hierarchy (currently only the nouns), in which each node of the hierarchy is depicted by hundreds and thousands of images.","year":2009,"link":"http://www.image-net.org/","license":"Non-commercial","licenseType":"non-commercial","licenseText":"Can only be used for research and educational purposes. Commercial use is prohibited.","arxiv":1409.0575,"torrent":"https://academictorrents.com/details/564a77c1e1119da199ff32622a1609431b9f1c47"}]
</script>
    <div class="bg-grey-lighter border-b border-t border-indigo-lightest p-6">
  <div class="mx-auto" style="max-width:600px">
    <div class="mb-8 text-indigo-darker leading-normal">You can subscribe to get updates when new datasets and tools are released.</div>

<form action="https://buttondown.email/api/emails/embed-subscribe/datasetlist" method="post" target="popupwindow" onsubmit="window.open(&#39;https://buttondown.email/datasetlist&#39;, &#39;popupwindow&#39;)" class="embeddable-buttondown-form block md:flex items-center rounded shadow-none md:shadow bg-tranparent md:bg-white">
    <input type="email" name="email" id="bd-email" placeholder="your@email.com" aria-label="Email address entry" class="bg-white border-none w-full md:rounded-l text-grey-darkest py-2 px-4 leading-tight md:shadow-none shadow mb-6 md:mb-0">
    <input type="hidden" value="1" name="embed">
    <input type="submit" class="w-full md:w-auto md:flex-no-shrink bg-indigo hover:bg-indigo-light border-indigo hover:border-indigo-light font-bold
                tracking-wide border-4 text-white py-2 px-4 md:rounded-r text-base md:shadow-none shadow" value="Subscribe">
</form>

  </div>
</div>

<div class="bg-grey-lightest p-6 text-indigo-darker">
  <div class="flex items-center justify-between mx-auto" style="max-width:1120px;">
  <div>
      © 2019 <a href="http://www.nikolaplesa.com/" target="_blank" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Nikola Plesa</a> | <a href="https://www.datasetlist.com/privacy" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Privacy</a> | <a href="https://www.datasetlist.com/" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Datasets</a> | <a href="https://www.datasetlist.com/tools" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">Annotation tools</a>  
    </div>
    <div><a href="mailto:hello@datasetlist.com" class="text-indigo-darkest hover:text-indigo no-underline font-semibold">hello@datasetlist.com</a></div>
    
  </div>
</div>

<script src="./Dataset list — A list of the biggest machine learning datasets_files/vue.min.js.下载"></script>
<script src="./Dataset list — A list of the biggest machine learning datasets_files/app.js.下载"></script>

  

</body></html>